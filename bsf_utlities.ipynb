{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d7e8d47-5d43-492c-bb71-b500464d2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_utilities' log_level=WARN (effective=WARN), progress=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/04 09:49:09 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/10/04 09:49:09 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/10/04 09:49:09 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from bsf_dbutilities import DBUtils\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    # SparkSession doesn't exist\n",
    "    pass\n",
    "\n",
    "spark = init_spark(\"bsf_utilities\", log_level=\"WARN\", show_progress=False, enable_ui=True,process_option='manual')\n",
    "engine = init_mariadb_engine()\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a57c1-24bd-4b24-b3de-3562ce841c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ Setup Database Communications ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "db = DBUtils(spark, ingest_ts)\n",
    "db.spark_stats()\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 200)         # Expand width to avoid line breaks\n",
    "\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "import datetime, os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# List all databases\n",
    "databases = [db.name for db in spark.catalog.listDatabases()]\n",
    "\n",
    "for database in databases:\n",
    "    print(f\"\\n=== Database: {database} ===\\n\")\n",
    "    \n",
    "    # Get all tables in this database\n",
    "    tables = spark.catalog.listTables(database)\n",
    "    if not tables:\n",
    "        print(\"No tables found.\")\n",
    "        continue\n",
    "\n",
    "    table_info = []\n",
    "    \n",
    "    for t in tables:\n",
    "        full_table_name = f\"{database}.{t.name}\"\n",
    "        row_count = spark.table(full_table_name).count()\n",
    "        table_type = t.tableType\n",
    "    \n",
    "        # Default: no max date\n",
    "        from pyspark.sql import functions as F\n",
    "        import datetime\n",
    "        import os\n",
    "        \n",
    "        max_date = None\n",
    "        try:\n",
    "            df_table = spark.table(full_table_name)\n",
    "            \n",
    "            if \"LastLoadedDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"LastLoadedDate\")).collect()[0][0]\n",
    "                field = \"LastLoadedDate\"\n",
    "            elif \"StockDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"StockDate\")).collect()[0][0]\n",
    "                field = \"StockDate\"\n",
    "            elif \"ChangeDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"ChangeDate\")).collect()[0][0]\n",
    "                field = \"ChangeDate\"\n",
    "            else:\n",
    "                try:\n",
    "                    dt = DeltaTable.forName(spark, full_table_name)\n",
    "                    history = dt.history(1)\n",
    "                    max_date = history.select(F.max(\"timestamp\")).collect()[0][0]\n",
    "                    field = \"timestamp\"\n",
    "                except:\n",
    "                    max_date = None\n",
    "        \n",
    "            # --- Normalize to datetime ---\n",
    "            if isinstance(max_date, datetime.date) and not isinstance(max_date, datetime.datetime):\n",
    "                max_date = datetime.datetime.combine(max_date, datetime.time.min)\n",
    "        \n",
    "        except:\n",
    "            # Non-Delta fallback\n",
    "            table_location = (\n",
    "                spark.sql(f\"DESCRIBE FORMATTED {full_table_name}\")\n",
    "                     .filter(\"col_name='Location'\")\n",
    "                     .select(\"data_type\")\n",
    "                     .collect()[0][0]\n",
    "            )\n",
    "            max_date = datetime.datetime.fromtimestamp(os.path.getmtime(table_location))\n",
    "\n",
    "\n",
    "    \n",
    "        table_info.append((t.name, row_count, field, max_date, table_type))\n",
    "    \n",
    "    df_table_info = spark.createDataFrame(\n",
    "        table_info,\n",
    "        schema=[\"TableName\", \"RowCount\",\"DateField\", \"MaxDate\", \"TableType\"]\n",
    "    )\n",
    "    \n",
    "    df_table_info.show(truncate=False)\n",
    "    # üö® Safety stop ‚Äî prevents accidental full execution\n",
    "    raise RuntimeError(\"‚ö†Ô∏è This notebook is blocked. Do NOT run all cells without checking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc238394-7a3e-4898-8bbd-8481ac292696",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055e7b9-bc19-4ef1-9d46-8095e5861ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "sdf.toPandas().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae836f7-8511-4cfe-8925-a004f5bfecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from bsf_config import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dc158-fce1-40af-ba66-42cc044a70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_settings(profile: str = \"default\" ):\n",
    "    \"\"\"Load settings, merging defaults with optional profile overrides.\n",
    "       For `timeframe_map`, profile overrides REPLACE the whole dict\n",
    "       instead of merging.\n",
    "    \"\"\"\n",
    "    default_settings = deepcopy(CONFIG[\"default\"])\n",
    "    settings = deepcopy(default_settings)\n",
    "\n",
    "    if profile and profile in CONFIG:\n",
    "        overrides = CONFIG[profile]\n",
    "\n",
    "        def merge(base, update, path=\"\"):\n",
    "            for k, v in update.items():\n",
    "                current_path = f\"{path}.{k}\" if path else k\n",
    "\n",
    "                # Special case: timeframe_map is replace, not merge\n",
    "                if k == \"timeframe_map\":\n",
    "                    old_value = base.get(k, \"<not in default>\")\n",
    "                    #print(f\"Override: {current_path}: default={old_value}, profile={v}\")\n",
    "                    base[k] = deepcopy(v)\n",
    "                    continue\n",
    "\n",
    "                if isinstance(v, dict) and k in base and isinstance(base[k], dict):\n",
    "                    merge(base[k], v, current_path)\n",
    "                else:\n",
    "                    old_value = base.get(k, \"<not in default>\")\n",
    "                    #print(f\"Override: {current_path}: default={old_value}, profile={v}\")\n",
    "                    base[k] = deepcopy(v)\n",
    "\n",
    "        merge(settings, overrides)\n",
    "\n",
    "    return settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096cc37-a3ca-4a10-835d-c42025bb6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "mbsetting = load_settings()\n",
    "\n",
    "config_json = json.dumps(mbsetting)\n",
    "print(config_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5de072-731e-4d7a-9ea2-2cc77e888836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "mbsetting = load_settings(\"tier1\")\n",
    "\n",
    "config_json = json.dumps(mbsetting)\n",
    "print(config_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "35991dd2-d79e-4261-92dd-5b26c67e853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CompanyId LastLoadedDate\n",
      "0         52     2025-10-03\n"
     ]
    }
   ],
   "source": [
    "wm_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM bsf.companystockhistory_watermark\n",
    "\"\"\").toPandas()\n",
    "wm_dict = dict(zip(wm_df.CompanyId, wm_df.LastLoadedDate))\n",
    "print(wm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd9deaec-ec01-4e55-8018-52a2fb1f1479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+\n",
      "|CompanyId|StockDate|row_count|\n",
      "+---------+---------+---------+\n",
      "+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming your stock history table is a Delta table\n",
    "df = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "\n",
    "# Count rows per company per stock date\n",
    "df.groupBy(\"CompanyId\", \"StockDate\") \\\n",
    "  .agg(F.count(\"*\").alias(\"row_count\")) \\\n",
    "  .filter(\"row_count > 1\") \\\n",
    "  .orderBy(\"CompanyId\", \"StockDate\") \\\n",
    "  .show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943f9ac-c778-4267-96f0-55f821ea4d9d",
   "metadata": {},
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eb13756f-d3ac-446b-a691-2ff9ed0912a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+---------+--------------+\n",
      "|CompanyId|LastLoadedDate|\n",
      "+---------+--------------+\n",
      "|       52|    2025-10-03|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4dc5ad64-9fef-4de9-9240-5773e189c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-------+-------+-------+--------+\n",
      "|CompanyId|StockDate |Open   |High   |Low    |Close  |Volume  |\n",
      "+---------+----------+-------+-------+-------+-------+--------+\n",
      "|52       |2025-10-03|0.04549|0.04549|0.041  |0.041  |108000.0|\n",
      "|52       |2025-10-02|0.044  |0.044  |0.044  |0.044  |0.0     |\n",
      "|52       |2025-10-01|0.04005|0.044  |0.04005|0.044  |50000.0 |\n",
      "|52       |2025-09-30|0.044  |0.044  |0.044  |0.044  |9900.0  |\n",
      "|52       |2025-09-29|0.0441 |0.0441 |0.037  |0.04349|156505.0|\n",
      "|52       |2025-09-26|0.054  |0.054  |0.054  |0.054  |0.0     |\n",
      "|52       |2025-09-25|0.054  |0.054  |0.054  |0.054  |0.0     |\n",
      "|52       |2025-09-24|0.0449 |0.054  |0.0449 |0.054  |10066.0 |\n",
      "|52       |2025-09-23|0.035  |0.035  |0.035  |0.035  |0.0     |\n",
      "|52       |2025-09-22|0.051  |0.055  |0.035  |0.035  |271066.0|\n",
      "+---------+----------+-------+-------+-------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"bsf.companystockhistory\") \\\n",
    "     .orderBy(F.col(\"StockDate\").desc()) \\\n",
    "     .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77fa11-d734-419e-a10a-e0991864f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"bsf.companystockhistory\") \\\n",
    "     .orderBy(F.col(\"StockDate\").desc()) \\\n",
    "     .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac94560b-3424-4939-a6d2-453fef121f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "+---------+---------------+-------+--------+--------+--------------+--------------+------------------+-----------------+------------+----------------+-------------+-------------+--------+---------------+\n",
      "|CompanyId|FundamentalDate|PeRatio|PegRatio| PbRatio|ReturnOnEquity|GrossMarginTTM|NetProfitMarginTTM|TotalDebtToEquity|CurrentRatio|InterestCoverage|EpsChangeYear|RevChangeYear|    Beta|ShortIntToFloat|\n",
      "+---------+---------------+-------+--------+--------+--------------+--------------+------------------+-----------------+------------+----------------+-------------+-------------+--------+---------------+\n",
      "|       52|     2024-12-29| -630.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2025-01-05| -800.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2025-09-07|-5200.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-9.28002|            0.0|\n",
      "|       52|     2025-09-14|-4800.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-9.28002|            0.0|\n",
      "|       52|     2024-10-20| -550.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "|       52|     2024-10-27| -500.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "|       52|     2024-12-15| -600.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2024-12-22| -650.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2024-11-24| -760.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.75422|            0.0|\n",
      "|       52|     2024-12-01| -530.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.75422|            0.0|\n",
      "|       52|     2025-01-12| -970.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.71661|            0.0|\n",
      "|       52|     2025-01-19| -790.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.71661|            0.0|\n",
      "|       52|     2025-02-09|-8150.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.66101|            0.0|\n",
      "|       52|     2025-02-16|-8320.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.66101|            0.0|\n",
      "|       52|     2024-11-03| -500.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "|       52|     2024-11-17| -600.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.75422|            0.0|\n",
      "|       52|     2025-02-23|-9000.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.66101|            0.0|\n",
      "|       52|     2025-04-06|-7000.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-5.84813|            0.0|\n",
      "|       52|     2024-12-08| -600.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2024-10-06| -550.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "+---------+---------------+-------+--------+--------+--------------+--------------+------------------+-----------------+------------+----------------+-------------+-------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"bsf.companyfundamental\")\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43d95112-a276-43a6-a659-05e6d3ecaae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "+---------+---------------+-------+--------+--------+--------------+--------------+------------------+-----------------+------------+----------------+-------------+-------------+--------+---------------+\n",
      "|CompanyId|FundamentalDate|PeRatio|PegRatio| PbRatio|ReturnOnEquity|GrossMarginTTM|NetProfitMarginTTM|TotalDebtToEquity|CurrentRatio|InterestCoverage|EpsChangeYear|RevChangeYear|    Beta|ShortIntToFloat|\n",
      "+---------+---------------+-------+--------+--------+--------------+--------------+------------------+-----------------+------------+----------------+-------------+-------------+--------+---------------+\n",
      "|       52|     2024-11-24| -760.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.75422|            0.0|\n",
      "|       52|     2024-12-01| -530.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.75422|            0.0|\n",
      "|       52|     2024-12-29| -630.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2025-01-05| -800.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2025-09-07|-5200.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-9.28002|            0.0|\n",
      "|       52|     2025-09-14|-4800.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-9.28002|            0.0|\n",
      "|       52|     2025-01-12| -970.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.71661|            0.0|\n",
      "|       52|     2025-01-19| -790.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.71661|            0.0|\n",
      "|       52|     2024-12-15| -600.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2024-12-22| -650.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "|       52|     2024-10-20| -550.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "|       52|     2024-10-27| -500.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "|       52|     2025-02-09|-8150.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.66101|            0.0|\n",
      "|       52|     2025-02-16|-8320.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.66101|            0.0|\n",
      "|       52|     2024-11-03| -500.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "|       52|     2024-11-17| -600.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.75422|            0.0|\n",
      "|       52|     2025-02-23|-9000.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.66101|            0.0|\n",
      "|       52|     2025-04-06|-7000.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-5.84813|            0.0|\n",
      "|       52|     2024-10-06| -550.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.90372|            0.0|\n",
      "|       52|     2024-12-08| -600.0|     0.0|-684.211|           0.0|           0.0|               0.0|              0.0|         0.4|             0.0|          0.0|          0.0|-3.76912|            0.0|\n",
      "+---------+---------------+-------+--------+--------+--------------+--------------+------------------+-----------------+------------+----------------+-------------+-------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"bsf.companyfundamental\")\n",
    "print(df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99b02db5-3f46-43a7-a293-8fe75f5dfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Reference the table\n",
    "table_name = \"bsf.companystockhistory_watermark\"\n",
    "dt = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Update CompanyId = 52 to new date\n",
    "dt.update(\n",
    "    condition = F.col(\"CompanyId\") == 52,\n",
    "    set = { \"LastLoadedDate\": F.lit(\"2025-10-01\") }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a425cc-a90d-49b8-b303-ac1aa67cea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "\n",
    "# Count rows per timeframe\n",
    "sdf.groupBy(\"CompanyId\") \\\n",
    "   .agg(F.max(\"LastLoadedDate\").alias(\"max_date\")) \\\n",
    "   .orderBy(\"CompanyId\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "        watermark_update_df = sdf.groupBy(\"CompanyId\") \\\n",
    "                               .agg(F.max(\"StockDate\").alias(\"LastLoadedDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468e619-d57f-4328-9f0d-e68f6f0bf834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.vacuum(retentionHours=0)  # ‚ö† deletes unreferenced files immediately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c1081-3f59-4312-b408-6aa8bb67c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.history().show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250b081-2b70-43aa-814b-d10932f61db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(\"/srv/lakehouse/tables/bsf.db/history_signals\") \\\n",
    "     .select(\"TimeFrame\").distinct().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dbef9-1dda-4972-b189-3b1d3e70072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT CompanyId) as cnt\n",
    "    FROM bsf.company\n",
    "    WHERE ListingExchange IN (1,2,3,16)\n",
    "      AND Active = 1\n",
    "      AND LastClose < 0.1\n",
    "      AND LastHistoryDate >= date_sub(current_date(), 30)\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c22f74-973e-41bc-af51-725e9efa82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "sdf.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ccf49-17f3-4642-8322-c167996d0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"CompanyId\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"CompanyId\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a41705-e767-4284-acbf-cb4fa19637d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78ed39-90b2-4a2e-aca6-c6fc63bf4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.history_signals_last\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa9aa6-d62d-4fbe-bda5-6f01e1cee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.signaldriver\")\n",
    "sdf.toPandas().describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5755c8c-0a33-495c-9336-153222484559",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718306cb-f473-455a-a85c-57485593f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.final_candidates\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccc2ff-2a09-4c60-96dc-31d4b57bc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"ListingExchange\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"ListingExchange\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy( \"TimeFrame\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy( \"TimeFrame\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffc9e5-751f-43d0-97f8-1e601221412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a Spark DataFrame\n",
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "\n",
    "# 1Ô∏è‚É£ Show the schema (data types included)\n",
    "sdf.printSchema()\n",
    "\n",
    "# 2Ô∏è‚É£ Get a list of column names\n",
    "# print(sdf.columns)\n",
    "\n",
    "# 3Ô∏è‚É£ Show first few rows with all columns (default shows truncated view)\n",
    "sdf.show(truncate=False)  \n",
    "\n",
    "# 4Ô∏è‚É£ For more detailed metadata about columns\n",
    "sdf.dtypes  # Returns a list of (column_name, data_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f906d5-5d27-4ddb-97ac-946e9442affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö® Safety stop ‚Äî prevents accidental full execution\n",
    "raise RuntimeError(\"‚ö†Ô∏è This notebook is blocked. Do NOT run all cells without checking!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
