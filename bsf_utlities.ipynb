{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7e8d47-5d43-492c-bb71-b500464d2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_utilities' log_level=WARN (effective=WARN), progress=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/03 21:10:58 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/10/03 21:10:59 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/10/03 21:10:59 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from bsf_dbutilities import DBUtils\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    # SparkSession doesn't exist\n",
    "    pass\n",
    "\n",
    "spark = init_spark(\"bsf_utilities\", log_level=\"WARN\", show_progress=False, enable_ui=True,process_option='manual')\n",
    "engine = init_mariadb_engine()\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3a57c1-24bd-4b24-b3de-3562ce841c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    🔍 === Runtime Config Check ===\n",
      "      📌 User                       : jupyter\n",
      "      📌 Python Version             : 3.9.21\n",
      "    🔍 === Spark Runtime Config Check ===\n",
      "      📌 Name                       : bsf_utilities\n",
      "      📌 Master                     : local[1]\n",
      "      📌 Version                    : Not set\n",
      "      📌 Max Cores                  : 1\n",
      "      📌 Executor Instances         : 1\n",
      "      📌 Executor Cores             : 1\n",
      "      📌 Task Cpus                  : 1\n",
      "      📌 Executor Memory            : 512m\n",
      "      📌 Driver Memory              : 512m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/03 21:11:51 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/10/03 21:11:54 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/10/03 21:11:54 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/10/03 21:11:54 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/10/03 21:12:03 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Database: bsf ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/03 21:12:24 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/10/03 21:12:27 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "|TableName                    |RowCount|DateField |MaxDate                   |TableType|\n",
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "|company                      |31051   |ChangeDate|2025-09-30 09:03:07.840831|MANAGED  |\n",
      "|companyfundamental           |283118  |timestamp |2025-10-03 09:14:28.767   |MANAGED  |\n",
      "|companystockhistory          |548957  |StockDate |2025-10-02 00:00:00       |MANAGED  |\n",
      "|companystockhistory_watermark|6171    |timestamp |2025-10-03 09:12:58.554   |MANAGED  |\n",
      "|final_candidates             |25      |StockDate |2025-10-02 00:00:00       |MANAGED  |\n",
      "|final_candidates_enriched    |110     |StockDate |2025-10-02 00:00:00       |MANAGED  |\n",
      "|history_signal_driver        |546731  |StockDate |2025-10-02 00:00:00       |MANAGED  |\n",
      "|history_signals              |2733655 |StockDate |2025-10-02 00:00:00       |MANAGED  |\n",
      "|history_signals_last         |2733655 |StockDate |2025-10-02 00:00:00       |MANAGED  |\n",
      "|signaldriver                 |9674    |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "⚠️ This notebook is blocked. Do NOT run all cells without checking!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m df_table_info\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 🚨 Safety stop — prevents accidental full execution\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ This notebook is blocked. Do NOT run all cells without checking!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ⚠️ This notebook is blocked. Do NOT run all cells without checking!"
     ]
    }
   ],
   "source": [
    "# ─── Setup Database Communications ──────────────────────────────────────\n",
    "db = DBUtils(spark, ingest_ts)\n",
    "db.spark_stats()\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 200)         # Expand width to avoid line breaks\n",
    "\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "import datetime, os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# List all databases\n",
    "databases = [db.name for db in spark.catalog.listDatabases()]\n",
    "\n",
    "for database in databases:\n",
    "    print(f\"\\n=== Database: {database} ===\\n\")\n",
    "    \n",
    "    # Get all tables in this database\n",
    "    tables = spark.catalog.listTables(database)\n",
    "    if not tables:\n",
    "        print(\"No tables found.\")\n",
    "        continue\n",
    "\n",
    "    table_info = []\n",
    "    \n",
    "    for t in tables:\n",
    "        full_table_name = f\"{database}.{t.name}\"\n",
    "        row_count = spark.table(full_table_name).count()\n",
    "        table_type = t.tableType\n",
    "    \n",
    "        # Default: no max date\n",
    "        from pyspark.sql import functions as F\n",
    "        import datetime\n",
    "        import os\n",
    "        \n",
    "        max_date = None\n",
    "        try:\n",
    "            df_table = spark.table(full_table_name)\n",
    "            \n",
    "            if \"LastLoadedDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"LastLoadedDate\")).collect()[0][0]\n",
    "                field = \"LastLoadedDate\"\n",
    "            elif \"StockDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"StockDate\")).collect()[0][0]\n",
    "                field = \"StockDate\"\n",
    "            elif \"ChangeDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"ChangeDate\")).collect()[0][0]\n",
    "                field = \"ChangeDate\"\n",
    "            else:\n",
    "                try:\n",
    "                    dt = DeltaTable.forName(spark, full_table_name)\n",
    "                    history = dt.history(1)\n",
    "                    max_date = history.select(F.max(\"timestamp\")).collect()[0][0]\n",
    "                    field = \"timestamp\"\n",
    "                except:\n",
    "                    max_date = None\n",
    "        \n",
    "            # --- Normalize to datetime ---\n",
    "            if isinstance(max_date, datetime.date) and not isinstance(max_date, datetime.datetime):\n",
    "                max_date = datetime.datetime.combine(max_date, datetime.time.min)\n",
    "        \n",
    "        except:\n",
    "            # Non-Delta fallback\n",
    "            table_location = (\n",
    "                spark.sql(f\"DESCRIBE FORMATTED {full_table_name}\")\n",
    "                     .filter(\"col_name='Location'\")\n",
    "                     .select(\"data_type\")\n",
    "                     .collect()[0][0]\n",
    "            )\n",
    "            max_date = datetime.datetime.fromtimestamp(os.path.getmtime(table_location))\n",
    "\n",
    "\n",
    "    \n",
    "        table_info.append((t.name, row_count, field, max_date, table_type))\n",
    "    \n",
    "    df_table_info = spark.createDataFrame(\n",
    "        table_info,\n",
    "        schema=[\"TableName\", \"RowCount\",\"DateField\", \"MaxDate\", \"TableType\"]\n",
    "    )\n",
    "    \n",
    "    df_table_info.show(truncate=False)\n",
    "    # 🚨 Safety stop — prevents accidental full execution\n",
    "    raise RuntimeError(\"⚠️ This notebook is blocked. Do NOT run all cells without checking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc238394-7a3e-4898-8bbd-8481ac292696",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055e7b9-bc19-4ef1-9d46-8095e5861ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "sdf.toPandas().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aae836f7-8511-4cfe-8925-a004f5bfecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from bsf_config import CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "898dc158-fce1-40af-ba66-42cc044a70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_settings(profile: str = \"default\" ):\n",
    "    \"\"\"Load settings, merging defaults with optional profile overrides.\n",
    "       For `timeframe_map`, profile overrides REPLACE the whole dict\n",
    "       instead of merging.\n",
    "    \"\"\"\n",
    "    default_settings = deepcopy(CONFIG[\"default\"])\n",
    "    settings = deepcopy(default_settings)\n",
    "\n",
    "    if profile and profile in CONFIG:\n",
    "        overrides = CONFIG[profile]\n",
    "\n",
    "        def merge(base, update, path=\"\"):\n",
    "            for k, v in update.items():\n",
    "                current_path = f\"{path}.{k}\" if path else k\n",
    "\n",
    "                # Special case: timeframe_map is replace, not merge\n",
    "                if k == \"timeframe_map\":\n",
    "                    old_value = base.get(k, \"<not in default>\")\n",
    "                    #print(f\"Override: {current_path}: default={old_value}, profile={v}\")\n",
    "                    base[k] = deepcopy(v)\n",
    "                    continue\n",
    "\n",
    "                if isinstance(v, dict) and k in base and isinstance(base[k], dict):\n",
    "                    merge(base[k], v, current_path)\n",
    "                else:\n",
    "                    old_value = base.get(k, \"<not in default>\")\n",
    "                    #print(f\"Override: {current_path}: default={old_value}, profile={v}\")\n",
    "                    base[k] = deepcopy(v)\n",
    "\n",
    "        merge(settings, overrides)\n",
    "\n",
    "    return settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4096cc37-a3ca-4a10-835d-c42025bb6d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timeframe_map\": {\"Daily\": 1, \"Short\": 3, \"Swing\": 5, \"Long\": 10}, \"phases\": {\"phase1\": {\"topN\": 50}, \"phase2\": {\"topN\": 25}, \"phase3\": {\"topN\": 5}}, \"profiles\": {\"Short\": {\"ma\": 2, \"ret\": 1, \"vol\": 3, \"roc_thresh\": 0.02, \"slope_horizon\": 1}, \"Swing\": {\"ma\": 5, \"ret\": 5, \"vol\": 5, \"roc_thresh\": 0.02, \"slope_horizon\": 5}, \"Long\": {\"ma\": 10, \"ret\": 10, \"vol\": 10, \"roc_thresh\": 0.02, \"slope_horizon\": 10}, \"Daily\": {\"ma\": 7, \"ret\": 1, \"vol\": 5, \"roc_thresh\": 0.02, \"slope_horizon\": 1}}, \"candle_params\": {\"doji_base\": 0.01, \"doji_scale\": 0.02, \"doji_min\": 0.01, \"doji_max\": 0.1, \"long_body_base\": 0.3, \"long_body_scale\": 0.3, \"long_body_min\": 0.3, \"long_body_max\": 0.6, \"small_body_base\": 0.15, \"small_body_scale\": 0.1, \"small_body_min\": 0.15, \"small_body_max\": 0.25, \"shadow_ratio_base\": 1.2, \"shadow_ratio_scale\": 0.8, \"shadow_ratio_min\": 1.2, \"shadow_ratio_max\": 2.0, \"near_edge\": 0.25, \"highvol_spike\": 1.5, \"lowvol_dip\": 0.7, \"hammer_base\": 0.15, \"hammer_scale\": 0.1, \"hammer_min\": 0.15, \"hammer_max\": 0.25, \"marubozu_base\": 0.03, \"marubozu_scale\": 0.02, \"marubozu_min\": 0.03, \"marubozu_max\": 0.05, \"rng_base\": 1e-05, \"rng_scale\": 0.0001, \"rng_min\": 1e-05, \"rng_max\": 0.0001}, \"signals\": {\"bullish_patterns\": [\"Hammer\", \"InvertedHammer\", \"BullishEngulfing\", \"BullishHarami\", \"PiercingLine\", \"MorningStar\", \"ThreeWhiteSoldiers\", \"BullishMarubozu\", \"TweezerBottom\", \"DragonflyDoji\", \"RisingThreeMethods\", \"GapUp\"], \"bearish_patterns\": [\"HangingMan\", \"ShootingStar\", \"BearishEngulfing\", \"BearishHarami\", \"DarkCloudCover\", \"EveningStar\", \"ThreeBlackCrows\", \"BearishMarubozu\", \"TweezerTop\", \"GravestoneDoji\", \"FallingThreeMethods\", \"GapDown\"], \"timeframes\": {\"Short\": {\"buy\": [\"hammer\", \"bullish\", \"piercing\", \"morning\", \"white\", \"marubozu\", \"tweezerbottom\"], \"sell\": [\"shooting\", \"bearish\", \"dark\", \"evening\", \"black\", \"marubozu\", \"tweezertop\"], \"momentum\": 0.05}, \"Swing\": {\"buy\": [\"hammer\", \"bullish\", \"piercing\", \"morning\", \"white\"], \"sell\": [\"shooting\", \"bearish\", \"dark\", \"evening\", \"black\"], \"momentum\": 0.1}, \"Long\": {\"buy\": [\"bullish\", \"morning\", \"white\", \"threewhitesoldiers\"], \"sell\": [\"bearish\", \"evening\", \"black\", \"threeblackcrows\"], \"momentum\": 0.2}, \"Daily\": {\"buy\": [\"bullish\", \"morning\", \"white\"], \"sell\": [\"bearish\", \"evening\", \"black\"], \"momentum\": 0.15}}, \"penny_stock_adjustment\": {\"threshold\": 1.0, \"factor\": 0.2, \"min_momentum\": 0.005}}, \"signal_strength\": {\"doji_thresh\": 0.1, \"hammer_thresh\": 0.25, \"marubozu_thresh\": 0.05, \"long_body\": 0.6, \"small_body\": 0.25, \"shadow_ratio\": 2.0, \"near_edge\": 0.25, \"rng_thresh\": 0.0001}, \"fundamental_weights\": {\"valuation\": 0.2, \"profitability\": 0.3, \"DebtLiquidity\": 0.2, \"Growth\": 0.2, \"Sentiment\": 0.1}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "mbsetting = load_settings()\n",
    "\n",
    "config_json = json.dumps(mbsetting)\n",
    "print(config_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e5de072-731e-4d7a-9ea2-2cc77e888836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timeframe_map\": {\"Daily\": 1, \"Short\": 3}, \"phases\": {\"phase1\": {\"topN\": 75}, \"phase2\": {\"topN\": 25}, \"phase3\": {\"topN\": 5}}, \"profiles\": {\"Short\": {\"ma\": 2, \"ret\": 1, \"vol\": 3, \"roc_thresh\": 0.02, \"slope_horizon\": 1}, \"Swing\": {\"ma\": 5, \"ret\": 5, \"vol\": 5, \"roc_thresh\": 0.02, \"slope_horizon\": 5}, \"Long\": {\"ma\": 10, \"ret\": 10, \"vol\": 10, \"roc_thresh\": 0.02, \"slope_horizon\": 10}, \"Daily\": {\"ma\": 7, \"ret\": 1, \"vol\": 5, \"roc_thresh\": 0.02, \"slope_horizon\": 1}}, \"candle_params\": {\"doji_base\": 0.025, \"doji_scale\": 0.02, \"doji_min\": 0.01, \"doji_max\": 0.09, \"long_body_base\": 0.4, \"long_body_scale\": 0.3, \"long_body_min\": 0.3, \"long_body_max\": 0.6, \"small_body_base\": 0.15, \"small_body_scale\": 0.1, \"small_body_min\": 0.15, \"small_body_max\": 0.25, \"shadow_ratio_base\": 1.2, \"shadow_ratio_scale\": 0.8, \"shadow_ratio_min\": 1.2, \"shadow_ratio_max\": 2.0, \"near_edge\": 0.25, \"highvol_spike\": 1.5, \"lowvol_dip\": 0.7, \"hammer_base\": 0.15, \"hammer_scale\": 0.1, \"hammer_min\": 0.15, \"hammer_max\": 0.25, \"marubozu_base\": 0.03, \"marubozu_scale\": 0.02, \"marubozu_min\": 0.03, \"marubozu_max\": 0.05, \"rng_base\": 1e-05, \"rng_scale\": 0.0001, \"rng_min\": 1e-05, \"rng_max\": 0.0001}, \"signals\": {\"bullish_patterns\": [\"Hammer\", \"InvertedHammer\", \"BullishEngulfing\", \"BullishHarami\", \"PiercingLine\", \"MorningStar\", \"ThreeWhiteSoldiers\", \"BullishMarubozu\", \"TweezerBottom\", \"DragonflyDoji\", \"RisingThreeMethods\", \"GapUp\"], \"bearish_patterns\": [\"HangingMan\", \"ShootingStar\", \"BearishEngulfing\", \"BearishHarami\", \"DarkCloudCover\", \"EveningStar\", \"ThreeBlackCrows\", \"BearishMarubozu\", \"TweezerTop\", \"GravestoneDoji\", \"FallingThreeMethods\", \"GapDown\"], \"timeframes\": {\"Short\": {\"buy\": [\"hammer\", \"bullish\", \"piercing\", \"morning\", \"white\", \"marubozu\", \"tweezerbottom\"], \"sell\": [\"shooting\", \"bearish\", \"dark\", \"evening\", \"black\", \"marubozu\", \"tweezertop\"], \"momentum\": 0.05}, \"Swing\": {\"buy\": [\"hammer\", \"bullish\", \"piercing\", \"morning\", \"white\"], \"sell\": [\"shooting\", \"bearish\", \"dark\", \"evening\", \"black\"], \"momentum\": 0.1}, \"Long\": {\"buy\": [\"bullish\", \"morning\", \"white\", \"threewhitesoldiers\"], \"sell\": [\"bearish\", \"evening\", \"black\", \"threeblackcrows\"], \"momentum\": 0.2}, \"Daily\": {\"buy\": [\"bullish\", \"morning\", \"white\"], \"sell\": [\"bearish\", \"evening\", \"black\"], \"momentum\": 0.15}}, \"penny_stock_adjustment\": {\"threshold\": 1.0, \"factor\": 0.2, \"min_momentum\": 0.005}}, \"signal_strength\": {\"doji_thresh\": 0.1, \"hammer_thresh\": 0.25, \"marubozu_thresh\": 0.05, \"long_body\": 0.6, \"small_body\": 0.25, \"shadow_ratio\": 2.0, \"near_edge\": 0.25, \"rng_thresh\": 0.0001}, \"fundamental_weights\": {\"valuation\": 0.2, \"profitability\": 0.3, \"DebtLiquidity\": 0.2, \"Growth\": 0.2, \"Sentiment\": 0.1}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "mbsetting = load_settings(\"tier1\")\n",
    "\n",
    "config_json = json.dumps(mbsetting)\n",
    "print(config_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8f972b5-86a2-4784-b16e-c9ec9668e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|CompanyId|LastLoadedDate|\n",
      "+---------+--------------+\n",
      "|   322964|    2025-10-02|\n",
      "|   322855|    2025-10-02|\n",
      "|   322797|    2025-10-02|\n",
      "|   322753|    2025-10-02|\n",
      "|   322622|    2025-10-02|\n",
      "|   322587|    2025-10-02|\n",
      "|   321969|    2025-10-02|\n",
      "|   321957|    2025-10-02|\n",
      "|   321955|    2025-10-02|\n",
      "|   321954|    2025-10-02|\n",
      "+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Option 1: Read directly via Spark table name\n",
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "\n",
    "# Option 2: Read via DeltaTable path\n",
    "# delta_path = \"/srv/lakehouse/tables/bsf.db/companystockhistory_watermark\"\n",
    "# sdf = DeltaTable.forPath(spark, delta_path).toDF()\n",
    "\n",
    "# Show the latest rows\n",
    "sdf.orderBy(\"CompanyId\", \"LastLoadedDate\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35991dd2-d79e-4261-92dd-5b26c67e853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CompanyId LastLoadedDate\n",
      "0         52     2025-10-01\n"
     ]
    }
   ],
   "source": [
    "wm_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM bsf.companystockhistory_watermark\n",
    "    WHERE CompanyId = 52\n",
    "\"\"\").toPandas()\n",
    "wm_dict = dict(zip(wm_df.CompanyId, wm_df.LastLoadedDate))\n",
    "print(wm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72388542-c73e-4999-ada7-464637e68008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CompanyId LastLoadedDate\n",
      "0         52     2025-10-02\n"
     ]
    }
   ],
   "source": [
    "print(wm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99b02db5-3f46-43a7-a293-8fe75f5dfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Reference the table\n",
    "table_name = \"bsf.companystockhistory_watermark\"\n",
    "dt = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Update CompanyId = 52 to new date\n",
    "dt.update(\n",
    "    condition = F.col(\"CompanyId\") == 52,\n",
    "    set = { \"LastLoadedDate\": F.lit(\"2025-10-01\") }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a425cc-a90d-49b8-b303-ac1aa67cea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|CompanyId|max_date  |\n",
      "+---------+----------+\n",
      "|52       |2025-10-02|\n",
      "|91       |2025-10-02|\n",
      "|138      |2025-10-02|\n",
      "|145      |2025-10-02|\n",
      "|151      |2025-10-02|\n",
      "|205      |2025-10-02|\n",
      "|216      |2025-10-02|\n",
      "|253      |2025-10-02|\n",
      "|320      |2025-10-02|\n",
      "|325      |2025-10-02|\n",
      "|332      |2025-10-02|\n",
      "|383      |2025-10-02|\n",
      "|397      |2025-10-02|\n",
      "|424      |2025-10-02|\n",
      "|437      |2025-10-02|\n",
      "|458      |2025-10-02|\n",
      "|471      |2025-10-02|\n",
      "|494      |2025-10-02|\n",
      "|497      |2025-10-02|\n",
      "|521      |2025-10-02|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "\n",
    "# Count rows per timeframe\n",
    "sdf.groupBy(\"CompanyId\") \\\n",
    "   .agg(F.max(\"LastLoadedDate\").alias(\"max_date\")) \\\n",
    "   .orderBy(\"CompanyId\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "        watermark_update_df = sdf.groupBy(\"CompanyId\") \\\n",
    "                               .agg(F.max(\"StockDate\").alias(\"LastLoadedDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468e619-d57f-4328-9f0d-e68f6f0bf834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.vacuum(retentionHours=0)  # ⚠ deletes unreferenced files immediately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c1081-3f59-4312-b408-6aa8bb67c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.history().show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250b081-2b70-43aa-814b-d10932f61db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(\"/srv/lakehouse/tables/bsf.db/history_signals\") \\\n",
    "     .select(\"TimeFrame\").distinct().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dbef9-1dda-4972-b189-3b1d3e70072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT CompanyId) as cnt\n",
    "    FROM bsf.company\n",
    "    WHERE ListingExchange IN (1,2,3,16)\n",
    "      AND Active = 1\n",
    "      AND LastClose < 0.1\n",
    "      AND LastHistoryDate >= date_sub(current_date(), 30)\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c22f74-973e-41bc-af51-725e9efa82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "sdf.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ccf49-17f3-4642-8322-c167996d0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"CompanyId\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"CompanyId\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a41705-e767-4284-acbf-cb4fa19637d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78ed39-90b2-4a2e-aca6-c6fc63bf4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.history_signals_last\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa9aa6-d62d-4fbe-bda5-6f01e1cee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.signaldriver\")\n",
    "sdf.toPandas().describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5755c8c-0a33-495c-9336-153222484559",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718306cb-f473-455a-a85c-57485593f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.final_candidates\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccc2ff-2a09-4c60-96dc-31d4b57bc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"ListingExchange\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"ListingExchange\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy( \"TimeFrame\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy( \"TimeFrame\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffc9e5-751f-43d0-97f8-1e601221412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a Spark DataFrame\n",
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "\n",
    "# 1️⃣ Show the schema (data types included)\n",
    "sdf.printSchema()\n",
    "\n",
    "# 2️⃣ Get a list of column names\n",
    "# print(sdf.columns)\n",
    "\n",
    "# 3️⃣ Show first few rows with all columns (default shows truncated view)\n",
    "sdf.show(truncate=False)  \n",
    "\n",
    "# 4️⃣ For more detailed metadata about columns\n",
    "sdf.dtypes  # Returns a list of (column_name, data_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f906d5-5d27-4ddb-97ac-946e9442affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚨 Safety stop — prevents accidental full execution\n",
    "raise RuntimeError(\"⚠️ This notebook is blocked. Do NOT run all cells without checking!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
