{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7e8d47-5d43-492c-bb71-b500464d2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_utilities' log_level=WARN (effective=WARN), progress=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:24:41 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/09/30 19:24:41 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/09/30 19:24:41 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from bsf_dbutilities import DBUtils\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    # SparkSession doesn't exist\n",
    "    pass\n",
    "\n",
    "spark = init_spark(\"bsf_utilities\", log_level=\"WARN\", show_progress=False, enable_ui=True,process_option='manual')\n",
    "engine = init_mariadb_engine()\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d3a57c1-24bd-4b24-b3de-3562ce841c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üîç === Runtime Config Check ===\n",
      "      üìå User                       : jupyter\n",
      "      üìå Python Version             : 3.9.21\n",
      "    üîç === Spark Runtime Config Check ===\n",
      "      üìå Name                       : bsf_utilities\n",
      "      üìå Master                     : local[1]\n",
      "      üìå Version                    : Not set\n",
      "      üìå Max Cores                  : 1\n",
      "      üìå Executor Instances         : 1\n",
      "      üìå Executor Cores             : 1\n",
      "      üìå Task Cpus                  : 1\n",
      "      üìå Executor Memory            : 512m\n",
      "      üìå Driver Memory              : 512m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:28:24 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Database: bsf ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:28:25 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "|TableName                    |RowCount|DateField |MaxDate                   |TableType|\n",
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "|company                      |31051   |ChangeDate|2025-09-30 09:03:07.840831|MANAGED  |\n",
      "|companyfundamental           |94446   |timestamp |2025-09-30 19:20:21.049   |MANAGED  |\n",
      "|companystockhistory          |644789  |StockDate |2025-09-29 00:00:00       |MANAGED  |\n",
      "|companystockhistory_watermark|2104    |timestamp |2025-09-30 19:18:53.368   |MANAGED  |\n",
      "|history_signal_driver        |644156  |StockDate |2025-09-29 00:00:00       |MANAGED  |\n",
      "|signaldriver                 |9674    |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "‚ö†Ô∏è This notebook is blocked. Do NOT run all cells without checking!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m df_table_info\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# üö® Safety stop ‚Äî prevents accidental full execution\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è This notebook is blocked. Do NOT run all cells without checking!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ‚ö†Ô∏è This notebook is blocked. Do NOT run all cells without checking!"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ Setup Database Communications ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "db = DBUtils(spark, ingest_ts)\n",
    "db.spark_stats()\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 200)         # Expand width to avoid line breaks\n",
    "\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "import datetime, os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# List all databases\n",
    "databases = [db.name for db in spark.catalog.listDatabases()]\n",
    "\n",
    "for database in databases:\n",
    "    print(f\"\\n=== Database: {database} ===\\n\")\n",
    "    \n",
    "    # Get all tables in this database\n",
    "    tables = spark.catalog.listTables(database)\n",
    "    if not tables:\n",
    "        print(\"No tables found.\")\n",
    "        continue\n",
    "\n",
    "    table_info = []\n",
    "    \n",
    "    for t in tables:\n",
    "        full_table_name = f\"{database}.{t.name}\"\n",
    "        row_count = spark.table(full_table_name).count()\n",
    "        table_type = t.tableType\n",
    "    \n",
    "        # Default: no max date\n",
    "        from pyspark.sql import functions as F\n",
    "        import datetime\n",
    "        import os\n",
    "        \n",
    "        max_date = None\n",
    "        try:\n",
    "            df_table = spark.table(full_table_name)\n",
    "            \n",
    "            if \"LastLoadDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"LastLoadDate\")).collect()[0][0]\n",
    "                field = \"LastLoadDate\"\n",
    "            elif \"StockDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"StockDate\")).collect()[0][0]\n",
    "                field = \"StockDate\"\n",
    "            elif \"ChangeDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"ChangeDate\")).collect()[0][0]\n",
    "                field = \"ChangeDate\"\n",
    "            else:\n",
    "                try:\n",
    "                    dt = DeltaTable.forName(spark, full_table_name)\n",
    "                    history = dt.history(1)\n",
    "                    max_date = history.select(F.max(\"timestamp\")).collect()[0][0]\n",
    "                    field = \"timestamp\"\n",
    "                except:\n",
    "                    max_date = None\n",
    "        \n",
    "            # --- Normalize to datetime ---\n",
    "            if isinstance(max_date, datetime.date) and not isinstance(max_date, datetime.datetime):\n",
    "                max_date = datetime.datetime.combine(max_date, datetime.time.min)\n",
    "        \n",
    "        except:\n",
    "            # Non-Delta fallback\n",
    "            table_location = (\n",
    "                spark.sql(f\"DESCRIBE FORMATTED {full_table_name}\")\n",
    "                     .filter(\"col_name='Location'\")\n",
    "                     .select(\"data_type\")\n",
    "                     .collect()[0][0]\n",
    "            )\n",
    "            max_date = datetime.datetime.fromtimestamp(os.path.getmtime(table_location))\n",
    "\n",
    "\n",
    "    \n",
    "        table_info.append((t.name, row_count, field, max_date, table_type))\n",
    "    \n",
    "    df_table_info = spark.createDataFrame(\n",
    "        table_info,\n",
    "        schema=[\"TableName\", \"RowCount\",\"DateField\", \"MaxDate\", \"TableType\"]\n",
    "    )\n",
    "    \n",
    "    df_table_info.show(truncate=False)\n",
    "    # üö® Safety stop ‚Äî prevents accidental full execution\n",
    "    raise RuntimeError(\"‚ö†Ô∏è This notebook is blocked. Do NOT run all cells without checking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc238394-7a3e-4898-8bbd-8481ac292696",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055e7b9-bc19-4ef1-9d46-8095e5861ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "sdf.toPandas().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a425cc-a90d-49b8-b303-ac1aa67cea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 19:26:47 WARN SQLConf: The SQL config 'spark.sql.adaptive.shuffle.targetPostShuffleInputSize' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.adaptive.advisoryPartitionSizeInBytes' instead of it.\n",
      "25/09/30 19:26:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/30 19:26:47 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/30 19:26:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/30 19:26:47 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|CompanyId|row_count|\n",
      "+---------+---------+\n",
      "|52       |1        |\n",
      "|91       |1        |\n",
      "|138      |1        |\n",
      "|145      |1        |\n",
      "|151      |1        |\n",
      "|205      |1        |\n",
      "|216      |1        |\n",
      "|253      |1        |\n",
      "|320      |1        |\n",
      "|325      |1        |\n",
      "|332      |1        |\n",
      "|383      |1        |\n",
      "|397      |1        |\n",
      "|424      |1        |\n",
      "|437      |1        |\n",
      "|458      |1        |\n",
      "|471      |1        |\n",
      "|494      |1        |\n",
      "|497      |1        |\n",
      "|521      |1        |\n",
      "+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "\n",
    "# Count rows per timeframe\n",
    "sdf.groupBy(\"CompanyId\") \\\n",
    "   .agg(F.count(\"*\").alias(\"row_count\")) \\\n",
    "   .orderBy(\"CompanyId\") \\\n",
    "   .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468e619-d57f-4328-9f0d-e68f6f0bf834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.vacuum(retentionHours=0)  # ‚ö† deletes unreferenced files immediately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c1081-3f59-4312-b408-6aa8bb67c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.history().show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250b081-2b70-43aa-814b-d10932f61db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(\"/srv/lakehouse/tables/bsf.db/history_signals\") \\\n",
    "     .select(\"TimeFrame\").distinct().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dbef9-1dda-4972-b189-3b1d3e70072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT CompanyId) as cnt\n",
    "    FROM bsf.company\n",
    "    WHERE ListingExchange IN (1,2,3,16)\n",
    "      AND Active = 1\n",
    "      AND LastClose < 0.1\n",
    "      AND LastHistoryDate >= date_sub(current_date(), 30)\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c22f74-973e-41bc-af51-725e9efa82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "sdf.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ccf49-17f3-4642-8322-c167996d0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"CompanyId\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"CompanyId\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a41705-e767-4284-acbf-cb4fa19637d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78ed39-90b2-4a2e-aca6-c6fc63bf4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.history_signals_last\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa9aa6-d62d-4fbe-bda5-6f01e1cee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.signaldriver\")\n",
    "sdf.toPandas().describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5755c8c-0a33-495c-9336-153222484559",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718306cb-f473-455a-a85c-57485593f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.final_candidates\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccc2ff-2a09-4c60-96dc-31d4b57bc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"ListingExchange\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"ListingExchange\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy( \"TimeFrame\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy( \"TimeFrame\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffc9e5-751f-43d0-97f8-1e601221412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a Spark DataFrame\n",
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "\n",
    "# 1Ô∏è‚É£ Show the schema (data types included)\n",
    "sdf.printSchema()\n",
    "\n",
    "# 2Ô∏è‚É£ Get a list of column names\n",
    "# print(sdf.columns)\n",
    "\n",
    "# 3Ô∏è‚É£ Show first few rows with all columns (default shows truncated view)\n",
    "sdf.show(truncate=False)  \n",
    "\n",
    "# 4Ô∏è‚É£ For more detailed metadata about columns\n",
    "sdf.dtypes  # Returns a list of (column_name, data_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f906d5-5d27-4ddb-97ac-946e9442affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö® Safety stop ‚Äî prevents accidental full execution\n",
    "raise RuntimeError(\"‚ö†Ô∏è This notebook is blocked. Do NOT run all cells without checking!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
