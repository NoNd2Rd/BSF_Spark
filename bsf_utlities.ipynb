{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d7e8d47-5d43-492c-bb71-b500464d2ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_utilities' log_level=WARN (effective=WARN), progress=False\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from bsf_dbutilities import DBUtils\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    # SparkSession doesn't exist\n",
    "    pass\n",
    "\n",
    "spark = init_spark(\"bsf_utilities\", log_level=\"WARN\", show_progress=False, enable_ui=True,process_option='manual')\n",
    "engine = init_mariadb_engine()\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d3a57c1-24bd-4b24-b3de-3562ce841c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    🔍 === Runtime Config Check ===\n",
      "      📌 User                       : jupyter\n",
      "      📌 Python Version             : 3.9.21\n",
      "    🔍 === Spark Runtime Config Check ===\n",
      "      📌 Name                       : bsf_utilities\n",
      "      📌 Master                     : spark://10.20.10.19:7077\n",
      "      📌 Version                    : Not set\n",
      "      📌 Max Cores                  : 2\n",
      "      📌 Executor Instances         : 2\n",
      "      📌 Executor Cores             : 1\n",
      "      📌 Task Cous                  : 1\n",
      "      📌 Executor Memory            : 1024m\n",
      "      📌 Driver Memory              : 2048m\n",
      "      📌 JVM Memory Overhead        : 256m\n",
      "      📌 Dynamic Allocation Enabled : true\n",
      "      📌 Default Parallelism        : 12\n",
      "      📌 SQL Shuffle Partitions     : 12\n",
      "      📌 Memory Fraction            : Not set\n",
      "      📌 Memory StorageFraction     : Not set\n",
      "      📌 SQL Adaptive               : Not set\n",
      "      📌 Shuffle Input Size         : Not set\n",
      "      📌 Delta OptimizeWrite        : Not set\n",
      "      📌 Delta AutoCompact          : Not set\n",
      "      📌 SQL Adaptive SkewJoin      : Not set\n",
      "      📌 Scheduler Pool             : default\n",
      "      📌 Sql Catalog Implementation : hive\n",
      "      📌 Catalog                    : org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "      📌 Sql Warehouse Dir          : file:/srv/lakehouse/tables\n",
      "      📌 Delta Base Path            : /srv/lakehouse/delta\n",
      "      📌 Filesource Path            : /srv/lakehouse/files\n",
      "      📌 Nond2rd Path               : /srv/lakehouse/nond2rd\n",
      "      📌 Delta Retention Check      : false\n",
      "      📌 Delta LogStore             : Not set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 11:47:29 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/29 11:47:29 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/29 11:47:29 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Database: bsf ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 11:47:30 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "|TableName                    |RowCount|DateField |MaxDate                   |TableType|\n",
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "|company                      |31051   |ChangeDate|2025-09-25 09:04:36.828567|MANAGED  |\n",
      "|companyfundamental           |46148   |timestamp |2025-09-29 08:22:33.708   |MANAGED  |\n",
      "|companystockhistory          |9674    |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "|companystockhistory_watermark|9674    |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "|final_candidates             |5       |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "|final_candidates_enriched    |15      |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "|history_signals              |38696   |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "|history_signals_last         |38696   |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "|signaldriver                 |9674    |StockDate |2025-09-24 00:00:00       |MANAGED  |\n",
      "+-----------------------------+--------+----------+--------------------------+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "⚠️ This notebook is blocked. Do NOT run all cells without checking!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m df_table_info\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 🚨 Safety stop — prevents accidental full execution\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ This notebook is blocked. Do NOT run all cells without checking!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ⚠️ This notebook is blocked. Do NOT run all cells without checking!"
     ]
    }
   ],
   "source": [
    "# ─── Setup Database Communications ──────────────────────────────────────\n",
    "db = DBUtils(spark, ingest_ts)\n",
    "db.spark_stats()\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 200)         # Expand width to avoid line breaks\n",
    "\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "import datetime, os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# List all databases\n",
    "databases = [db.name for db in spark.catalog.listDatabases()]\n",
    "\n",
    "for database in databases:\n",
    "    print(f\"\\n=== Database: {database} ===\\n\")\n",
    "    \n",
    "    # Get all tables in this database\n",
    "    tables = spark.catalog.listTables(database)\n",
    "    if not tables:\n",
    "        print(\"No tables found.\")\n",
    "        continue\n",
    "\n",
    "    table_info = []\n",
    "    \n",
    "    for t in tables:\n",
    "        full_table_name = f\"{database}.{t.name}\"\n",
    "        row_count = spark.table(full_table_name).count()\n",
    "        table_type = t.tableType\n",
    "    \n",
    "        # Default: no max date\n",
    "        from pyspark.sql import functions as F\n",
    "        import datetime\n",
    "        import os\n",
    "        \n",
    "        max_date = None\n",
    "        try:\n",
    "            df_table = spark.table(full_table_name)\n",
    "            \n",
    "            if \"LastLoadDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"LastLoadDate\")).collect()[0][0]\n",
    "                field = \"LastLoadDate\"\n",
    "            elif \"StockDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"StockDate\")).collect()[0][0]\n",
    "                field = \"StockDate\"\n",
    "            elif \"ChangeDate\" in df_table.columns:\n",
    "                max_date = df_table.agg(F.max(\"ChangeDate\")).collect()[0][0]\n",
    "                field = \"ChangeDate\"\n",
    "            else:\n",
    "                try:\n",
    "                    dt = DeltaTable.forName(spark, full_table_name)\n",
    "                    history = dt.history(1)\n",
    "                    max_date = history.select(F.max(\"timestamp\")).collect()[0][0]\n",
    "                    field = \"timestamp\"\n",
    "                except:\n",
    "                    max_date = None\n",
    "        \n",
    "            # --- Normalize to datetime ---\n",
    "            if isinstance(max_date, datetime.date) and not isinstance(max_date, datetime.datetime):\n",
    "                max_date = datetime.datetime.combine(max_date, datetime.time.min)\n",
    "        \n",
    "        except:\n",
    "            # Non-Delta fallback\n",
    "            table_location = (\n",
    "                spark.sql(f\"DESCRIBE FORMATTED {full_table_name}\")\n",
    "                     .filter(\"col_name='Location'\")\n",
    "                     .select(\"data_type\")\n",
    "                     .collect()[0][0]\n",
    "            )\n",
    "            max_date = datetime.datetime.fromtimestamp(os.path.getmtime(table_location))\n",
    "\n",
    "\n",
    "    \n",
    "        table_info.append((t.name, row_count, field, max_date, table_type))\n",
    "    \n",
    "    df_table_info = spark.createDataFrame(\n",
    "        table_info,\n",
    "        schema=[\"TableName\", \"RowCount\",\"DateField\", \"MaxDate\", \"TableType\"]\n",
    "    )\n",
    "    \n",
    "    df_table_info.show(truncate=False)\n",
    "    # 🚨 Safety stop — prevents accidental full execution\n",
    "    raise RuntimeError(\"⚠️ This notebook is blocked. Do NOT run all cells without checking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc238394-7a3e-4898-8bbd-8481ac292696",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b055e7b9-bc19-4ef1-9d46-8095e5861ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 12:15:55 WARN DeltaLog: Failed to parse file:/srv/lakehouse/tables/bsf.db/company/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:265)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:258)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:257)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:310)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:308)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:307)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:800)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:795)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:794)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:812)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:811)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:826)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:668)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:84)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:122)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:122)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:103)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:179)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:245)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:203)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:285)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1631)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)\n",
      "\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:601)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/09/29 12:15:56 WARN DeltaLog: Failed to parse file:/srv/lakehouse/tables/bsf.db/company/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:265)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:258)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:257)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:310)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:308)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:307)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:800)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:795)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:794)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:812)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:811)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:826)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:668)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:84)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:122)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:122)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:103)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:179)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:245)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:203)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:285)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1631)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)\n",
      "\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:601)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/09/29 12:15:57 WARN DeltaLog: Failed to parse file:/srv/lakehouse/tables/bsf.db/company/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:265)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:258)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:257)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:310)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:308)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:307)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:800)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:795)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:794)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:812)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:811)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:826)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:668)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:84)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:122)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:122)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:103)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:179)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:245)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:203)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:285)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1631)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)\n",
      "\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:601)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/09/29 12:15:58 WARN DeltaLog: file:/srv/lakehouse/tables/bsf.db/company/_delta_log/_last_checkpoint is corrupted. Will search the checkpoint files directly\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\n",
      "\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:116)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:265)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:258)\n",
      "\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:257)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:310)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:308)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:307)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:800)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:795)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:578)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:794)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:812)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:811)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:826)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:668)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:84)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:122)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:122)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:103)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:179)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:245)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:203)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:285)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1631)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n",
      "\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)\n",
      "\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:601)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2544.table.\n: org.sparkproject.guava.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2263)\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:245)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1631)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:275)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:601)\n\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\n\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:116)\n\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:116)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore(Checkpoints.scala:330)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore$(Checkpoints.scala:320)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpointBefore(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:287)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:258)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:257)\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:310)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:308)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:307)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:800)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:795)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:578)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:578)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:578)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:794)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:812)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:811)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:826)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:668)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:84)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:122)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:122)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:103)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:179)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:245)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:257)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\t... 75 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbsf.company\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sdf\u001b[38;5;241m.\u001b[39mtoPandas()\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/sql/session.py:1476\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2544.table.\n: org.sparkproject.guava.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2263)\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:245)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:1631)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:275)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:239)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:608)\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:601)\n\tat jdk.internal.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n\tat org.apache.spark.sql.delta.storage.DelegatingLogStore.<init>(DelegatingLogStore.scala:38)\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStoreWithClassName(LogStore.scala:288)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:385)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:380)\n\tat org.apache.spark.sql.delta.storage.LogStore$.createLogStore(LogStore.scala:266)\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:279)\n\tat org.apache.spark.sql.delta.storage.LogStore$.apply(LogStore.scala:274)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore(LogStore.scala:322)\n\tat org.apache.spark.sql.delta.storage.LogStoreProvider.createLogStore$(LogStore.scala:321)\n\tat org.apache.spark.sql.delta.DeltaLog.createLogStore(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.DeltaLog.store$lzycompute(DeltaLog.scala:116)\n\tat org.apache.spark.sql.delta.DeltaLog.store(DeltaLog.scala:116)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore(Checkpoints.scala:330)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore$(Checkpoints.scala:320)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpointBefore(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:287)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:275)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:264)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:258)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:257)\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:310)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:308)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:307)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:800)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:795)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:578)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:578)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:578)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:794)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:812)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:811)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:826)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:668)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:84)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:122)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:122)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:103)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:179)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:245)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:257)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\t... 75 more\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "sdf.toPandas().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51a425cc-a90d-49b8-b303-ac1aa67cea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|TimeFrame|row_count|\n",
      "+---------+---------+\n",
      "|Daily    |9674     |\n",
      "|Long     |9674     |\n",
      "|Short    |9674     |\n",
      "|Swing    |9674     |\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "\n",
    "# Count rows per timeframe\n",
    "sdf.groupBy(\"TimeFrame\") \\\n",
    "   .agg(F.count(\"*\").alias(\"row_count\")) \\\n",
    "   .orderBy(\"TimeFrame\") \\\n",
    "   .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a468e619-d57f-4328-9f0d-e68f6f0bf834",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[1;32m      2\u001b[0m dt \u001b[38;5;241m=\u001b[39m DeltaTable\u001b[38;5;241m.\u001b[39mforName(spark, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbsf.history_signals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvacuum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretentionHours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ⚠ deletes unreferenced files immediately\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/delta/tables.py:249\u001b[0m, in \u001b[0;36mDeltaTable.vacuum\u001b[0;34m(self, retentionHours)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\n\u001b[1;32m    244\u001b[0m         jdt\u001b[38;5;241m.\u001b[39mvacuum(),\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_wrapped\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\n\u001b[0;32m--> 249\u001b[0m         \u001b[43mjdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvacuum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretentionHours\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_wrapped\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 11:41:07 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/09/29 11:41:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.vacuum(retentionHours=0)  # ⚠ deletes unreferenced files immediately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c23c1081-3f59-4312-b408-6aa8bb67c021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------+------------+--------------------------------------+\n",
      "|version|timestamp              |userId|userName|operation                        |operationParameters                                                                              |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                |userMetadata|engineInfo                            |\n",
      "+-------+-----------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------+------------+--------------------------------------+\n",
      "|3      |2025-09-29 11:20:18.404|null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"UserId\",\"TimeFrame\"], properties -> {}}|null|null    |null     |2          |Serializable  |false        |{numFiles -> 8, numOutputRows -> 9674, numOutputBytes -> 862060}|null        |Apache-Spark/3.4.1 Delta-Lake/3.0.0rc1|\n",
      "|2      |2025-09-29 11:19:28.449|null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"UserId\",\"TimeFrame\"], properties -> {}}|null|null    |null     |1          |Serializable  |false        |{numFiles -> 8, numOutputRows -> 9674, numOutputBytes -> 866359}|null        |Apache-Spark/3.4.1 Delta-Lake/3.0.0rc1|\n",
      "|1      |2025-09-29 11:18:35.547|null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"UserId\",\"TimeFrame\"], properties -> {}}|null|null    |null     |0          |Serializable  |false        |{numFiles -> 8, numOutputRows -> 9674, numOutputBytes -> 838558}|null        |Apache-Spark/3.4.1 Delta-Lake/3.0.0rc1|\n",
      "|0      |2025-09-29 11:17:38.36 |null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"UserId\",\"TimeFrame\"], properties -> {}}|null|null    |null     |null       |Serializable  |false        |{numFiles -> 8, numOutputRows -> 9674, numOutputBytes -> 861951}|null        |Apache-Spark/3.4.1 Delta-Lake/3.0.0rc1|\n",
      "+-------+-----------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------+------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, \"bsf.history_signals\")\n",
    "dt.history().show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9250b081-2b70-43aa-814b-d10932f61db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|TimeFrame|\n",
      "+---------+\n",
      "|    Short|\n",
      "|    Swing|\n",
      "|    Daily|\n",
      "|     Long|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"/srv/lakehouse/tables/bsf.db/history_signals\") \\\n",
    "     .select(\"TimeFrame\").distinct().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc7dbef9-1dda-4972-b189-3b1d3e70072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2957\n"
     ]
    }
   ],
   "source": [
    "count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT CompanyId) as cnt\n",
    "    FROM bsf.company\n",
    "    WHERE ListingExchange IN (1,2,3,16)\n",
    "      AND Active = 1\n",
    "      AND LastClose < 0.1\n",
    "      AND LastHistoryDate >= date_sub(current_date(), 30)\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05c22f74-973e-41bc-af51-725e9efa82fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyId</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9.674000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>283131.323444</td>\n",
       "      <td>0.202219</td>\n",
       "      <td>0.214481</td>\n",
       "      <td>0.190078</td>\n",
       "      <td>0.200451</td>\n",
       "      <td>7.996915e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27379.690969</td>\n",
       "      <td>0.629919</td>\n",
       "      <td>0.658202</td>\n",
       "      <td>0.596637</td>\n",
       "      <td>0.621049</td>\n",
       "      <td>8.924417e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>246458.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250003.000000</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>289829.000000</td>\n",
       "      <td>0.037925</td>\n",
       "      <td>0.041070</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.038220</td>\n",
       "      <td>6.948000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>292453.000000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.032100e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>322964.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>9.140100</td>\n",
       "      <td>8.080000</td>\n",
       "      <td>8.680000</td>\n",
       "      <td>6.647880e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CompanyId         Open         High          Low        Close        Volume\n",
       "count    9674.000000  9674.000000  9674.000000  9674.000000  9674.000000  9.674000e+03\n",
       "mean   283131.323444     0.202219     0.214481     0.190078     0.200451  7.996915e+05\n",
       "std     27379.690969     0.629919     0.658202     0.596637     0.621049  8.924417e+06\n",
       "min    246458.000000     0.000001     0.000001     0.000001     0.000001  0.000000e+00\n",
       "25%    250003.000000     0.013200     0.014000     0.011900     0.013542  0.000000e+00\n",
       "50%    289829.000000     0.037925     0.041070     0.035000     0.038220  6.948000e+03\n",
       "75%    292453.000000     0.090000     0.100000     0.081500     0.090000  1.032100e+05\n",
       "max    322964.000000     8.500000     9.140100     8.080000     8.680000  6.647880e+08"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "sdf.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c1ccf49-17f3-4642-8322-c167996d0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|CompanyId|row_count|\n",
      "+---------+---------+\n",
      "|246458   |65       |\n",
      "|247632   |166      |\n",
      "|247699   |171      |\n",
      "|247700   |171      |\n",
      "|247709   |166      |\n",
      "|248157   |160      |\n",
      "|248489   |152      |\n",
      "|248509   |153      |\n",
      "|249416   |149      |\n",
      "|249434   |150      |\n",
      "|249824   |138      |\n",
      "|249861   |139      |\n",
      "|249888   |141      |\n",
      "|249943   |280      |\n",
      "|249991   |144      |\n",
      "|250003   |139      |\n",
      "|250006   |280      |\n",
      "|250473   |135      |\n",
      "|250567   |114      |\n",
      "|251019   |131      |\n",
      "|251108   |119      |\n",
      "|280298   |56       |\n",
      "|280304   |77       |\n",
      "|280325   |75       |\n",
      "|280339   |279      |\n",
      "|285312   |104      |\n",
      "|285313   |104      |\n",
      "|285324   |279      |\n",
      "|285342   |81       |\n",
      "|285354   |40       |\n",
      "|286522   |123      |\n",
      "|289761   |96       |\n",
      "|289801   |64       |\n",
      "|289827   |118      |\n",
      "|289828   |44       |\n",
      "|289829   |217      |\n",
      "|291151   |31       |\n",
      "|291176   |133      |\n",
      "|291186   |280      |\n",
      "|291846   |280      |\n",
      "|292065   |280      |\n",
      "|292074   |105      |\n",
      "|292076   |280      |\n",
      "|292077   |279      |\n",
      "|292095   |113      |\n",
      "|292443   |279      |\n",
      "|292449   |106      |\n",
      "|292453   |104      |\n",
      "|292456   |21       |\n",
      "|292920   |89       |\n",
      "+---------+---------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"CompanyId\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"CompanyId\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23a41705-e767-4284-acbf-cb4fa19637d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyId</th>\n",
       "      <th>StockDate</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-04</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-08</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>249824</td>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyId   StockDate      Open      High       Low     Close  Volume\n",
       "0     249824  2025-07-31  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "1     249824  2025-08-01  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "2     249824  2025-08-04  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "3     249824  2025-08-05  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "4     249824  2025-08-06  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "5     249824  2025-08-07  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "6     249824  2025-08-08  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "7     249824  2025-08-11  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "8     249824  2025-08-12  0.000001  0.000001  0.000001  0.000001     0.0\n",
       "9     249824  2025-08-13  0.000001  0.000001  0.000001  0.000001     0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.companystockhistory_watermark\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf\n",
    "\n",
    "#pdf.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c78ed39-90b2-4a2e-aca6-c6fc63bf4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyId</th>\n",
       "      <th>StockDate</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>TomorrowClose</th>\n",
       "      <th>Return</th>\n",
       "      <th>TomorrowReturn</th>\n",
       "      <th>MA</th>\n",
       "      <th>MA_slope</th>\n",
       "      <th>UpTrend_MA</th>\n",
       "      <th>DownTrend_MA</th>\n",
       "      <th>MomentumUp</th>\n",
       "      <th>MomentumDown</th>\n",
       "      <th>ConfirmedUpTrend</th>\n",
       "      <th>ConfirmedDownTrend</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>LowVolatility</th>\n",
       "      <th>HighVolatility</th>\n",
       "      <th>SignalStrength</th>\n",
       "      <th>SignalStrengthHybrid</th>\n",
       "      <th>ActionConfidence</th>\n",
       "      <th>BullishStrengthHybrid</th>\n",
       "      <th>BearishStrengthHybrid</th>\n",
       "      <th>SignalDuration</th>\n",
       "      <th>PatternAction</th>\n",
       "      <th>CandleAction</th>\n",
       "      <th>UpTrend_Return</th>\n",
       "      <th>CandidateAction</th>\n",
       "      <th>Action</th>\n",
       "      <th>TomorrowAction</th>\n",
       "      <th>TimeFrame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.05500</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.338999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.092603</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.530954e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1.465623</td>\n",
       "      <td>1.465623</td>\n",
       "      <td>0.798956</td>\n",
       "      <td>1.465623</td>\n",
       "      <td>152.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Sell</td>\n",
       "      <td>False</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>-0.045226</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.430176e-02</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.892754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892754</td>\n",
       "      <td>0.892754</td>\n",
       "      <td>86.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>641</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.287554</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2.539276e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709860</td>\n",
       "      <td>0.709860</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>972</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.02900</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.02900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1394</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.00335</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.00335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.042857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>-0.009357</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.496555e-02</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.399176</td>\n",
       "      <td>0.399176</td>\n",
       "      <td>0.399176</td>\n",
       "      <td>0.399176</td>\n",
       "      <td>175.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1493</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.635231e-02</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.634120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634120</td>\n",
       "      <td>0.634120</td>\n",
       "      <td>127.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1849</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.04300</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.04300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042418</td>\n",
       "      <td>0.020890</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5.315796e-02</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.481975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481975</td>\n",
       "      <td>0.481975</td>\n",
       "      <td>174.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1851</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.00990</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.00990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>-0.102190</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.814933e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>0.678016</td>\n",
       "      <td>0.344683</td>\n",
       "      <td>0.678016</td>\n",
       "      <td>0.344683</td>\n",
       "      <td>130.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Buy</td>\n",
       "      <td>True</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2217</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.09660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109460</td>\n",
       "      <td>0.081065</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.769237e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.378258</td>\n",
       "      <td>0.378258</td>\n",
       "      <td>0.378258</td>\n",
       "      <td>0.378258</td>\n",
       "      <td>159.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2285</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.02700</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.02700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023380</td>\n",
       "      <td>0.035430</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2.480436e-01</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414802</td>\n",
       "      <td>0.414802</td>\n",
       "      <td>0.414802</td>\n",
       "      <td>0.414802</td>\n",
       "      <td>163.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyId   StockDate    Open     High     Low    Close  TomorrowClose    Return  TomorrowReturn        MA  MA_slope  UpTrend_MA  DownTrend_MA  MomentumUp  MomentumDown  ConfirmedUpTrend  \\\n",
       "0         52  2025-09-22  0.0510  0.05500  0.0350  0.03500            NaN -0.338999             NaN  0.053590  0.092603        True         False       False          True             False   \n",
       "1        145  2025-09-22  0.0190  0.01900  0.0190  0.01900            NaN  0.000000             NaN  0.019000 -0.045226       False          True       False          True             False   \n",
       "2        641  2025-09-22  0.0135  0.01350  0.0135  0.01350            NaN  0.000000             NaN  0.012000  0.287554        True         False        True         False              True   \n",
       "3        972  2025-09-22  0.0290  0.02900  0.0290  0.02900            NaN  0.000000             NaN  0.029000  0.000000       False         False       False         False             False   \n",
       "4       1394  2025-09-22  0.0031  0.00335  0.0031  0.00335            NaN -0.042857             NaN  0.003176 -0.009357       False          True        True         False             False   \n",
       "5       1493  2025-09-22  0.0013  0.00130  0.0013  0.00130            NaN  0.000000             NaN  0.001300  0.048387        True         False       False         False             False   \n",
       "6       1849  2025-09-22  0.0410  0.04300  0.0410  0.04300            NaN  0.000000             NaN  0.042418  0.020890        True         False        True         False              True   \n",
       "7       1851  2025-09-22  0.0099  0.00990  0.0099  0.00990            NaN  0.000000             NaN  0.007380 -0.102190       False          True        True         False             False   \n",
       "8       2217  2025-09-22  0.1000  0.10950  0.0900  0.09660            NaN -0.086093             NaN  0.109460  0.081065        True         False       False          True             False   \n",
       "9       2285  2025-09-22  0.0214  0.02700  0.0202  0.02700            NaN  0.273585             NaN  0.023380  0.035430        True         False        True         False              True   \n",
       "\n",
       "   ConfirmedDownTrend    Volatility  LowVolatility  HighVolatility  SignalStrength  SignalStrengthHybrid  ActionConfidence  BullishStrengthHybrid  BearishStrengthHybrid  SignalDuration  \\\n",
       "0               False  1.530954e-01          False            True               4              1.465623          1.465623               0.798956               1.465623           152.0   \n",
       "1                True  1.430176e-02          False           False               0              0.892754          0.000000               0.892754               0.892754            86.0   \n",
       "2               False  2.539276e-01          False            True               0              0.709860          0.000000               0.709860               0.709860           193.0   \n",
       "3               False  1.000000e-08          False           False               0              0.000000          0.000000               0.000000               0.000000            61.0   \n",
       "4               False  6.496555e-02           True           False               1              0.399176          0.399176               0.399176               0.399176           175.0   \n",
       "5               False  2.635231e-02           True           False               2              0.634120          0.000000               0.634120               0.634120           127.0   \n",
       "6               False  5.315796e-02           True           False               0              0.481975          0.000000               0.481975               0.481975           174.0   \n",
       "7               False  2.814933e-01          False            True               2              0.678016          0.344683               0.678016               0.344683           130.0   \n",
       "8               False  1.769237e-01           True           False               1              0.378258          0.378258               0.378258               0.378258           159.0   \n",
       "9               False  2.480436e-01           True           False               1              0.414802          0.414802               0.414802               0.414802           163.0   \n",
       "\n",
       "  PatternAction CandleAction  UpTrend_Return CandidateAction Action TomorrowAction TimeFrame  \n",
       "0           Buy         Sell           False            Sell   Sell           Hold     Swing  \n",
       "1          Sell         Hold           False             Buy   Hold           Hold     Swing  \n",
       "2           Buy         Hold            True             Buy   Hold           Hold     Swing  \n",
       "3          Hold         Hold           False            Hold   Hold           Hold     Swing  \n",
       "4          Sell         Hold            True            Sell   Sell           Hold     Swing  \n",
       "5           Buy         Hold           False             Buy   Hold           Hold     Swing  \n",
       "6           Buy         Hold            True             Buy   Hold           Hold     Swing  \n",
       "7          Sell          Buy            True            Sell   Sell           Hold     Swing  \n",
       "8           Buy         Hold           False             Buy    Buy           Hold     Swing  \n",
       "9           Buy         Hold            True             Buy    Buy           Hold     Swing  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.history_signals_last\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00fa9aa6-d62d-4fbe-bda5-6f01e1cee2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyId</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PeRatio</th>\n",
       "      <th>PegRatio</th>\n",
       "      <th>PbRatio</th>\n",
       "      <th>ReturnOnEquity</th>\n",
       "      <th>GrossMarginTTM</th>\n",
       "      <th>NetProfitMarginTTM</th>\n",
       "      <th>TotalDebtToEquity</th>\n",
       "      <th>CurrentRatio</th>\n",
       "      <th>InterestCoverage</th>\n",
       "      <th>EpsChangeYear</th>\n",
       "      <th>RevChangeYear</th>\n",
       "      <th>Beta</th>\n",
       "      <th>ShortIntToFloat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9674.000000</td>\n",
       "      <td>9.674000e+03</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.0</td>\n",
       "      <td>3276.0</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>283131.323444</td>\n",
       "      <td>0.202219</td>\n",
       "      <td>0.214481</td>\n",
       "      <td>0.190078</td>\n",
       "      <td>0.200451</td>\n",
       "      <td>7.996915e+05</td>\n",
       "      <td>-1.144439</td>\n",
       "      <td>1.239174</td>\n",
       "      <td>-9.447311</td>\n",
       "      <td>346.364976</td>\n",
       "      <td>-674.503793</td>\n",
       "      <td>-7628.716447</td>\n",
       "      <td>-39.978170</td>\n",
       "      <td>1.334098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.468582</td>\n",
       "      <td>1.020204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27379.690969</td>\n",
       "      <td>0.629919</td>\n",
       "      <td>0.658202</td>\n",
       "      <td>0.596637</td>\n",
       "      <td>0.621049</td>\n",
       "      <td>8.924417e+06</td>\n",
       "      <td>4.105461</td>\n",
       "      <td>20.249965</td>\n",
       "      <td>49.670092</td>\n",
       "      <td>1143.552225</td>\n",
       "      <td>2987.555880</td>\n",
       "      <td>14981.101512</td>\n",
       "      <td>582.658094</td>\n",
       "      <td>2.312820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.455414</td>\n",
       "      <td>5.562771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>246458.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-32.894700</td>\n",
       "      <td>-7.452120</td>\n",
       "      <td>-214.492000</td>\n",
       "      <td>-1684.130000</td>\n",
       "      <td>-15489.800000</td>\n",
       "      <td>-66019.900000</td>\n",
       "      <td>-9110.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-77.956700</td>\n",
       "      <td>-5.535060</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250003.000000</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.046680</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-73.596500</td>\n",
       "      <td>-87.017100</td>\n",
       "      <td>-7555.470000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>289829.000000</td>\n",
       "      <td>0.037925</td>\n",
       "      <td>0.041070</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.038220</td>\n",
       "      <td>6.948000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.043000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-22.427900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505670</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>292453.000000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.032100e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.892500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.492800</td>\n",
       "      <td>1.166000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.441880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>322964.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>9.140100</td>\n",
       "      <td>8.080000</td>\n",
       "      <td>8.680000</td>\n",
       "      <td>6.647880e+08</td>\n",
       "      <td>1.486670</td>\n",
       "      <td>321.708000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>3739.110000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>356.913000</td>\n",
       "      <td>457.778000</td>\n",
       "      <td>13.995200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1295.330000</td>\n",
       "      <td>85.782400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CompanyId         Open         High          Low        Close        Volume      PeRatio     PegRatio      PbRatio  ReturnOnEquity  GrossMarginTTM  NetProfitMarginTTM  TotalDebtToEquity  \\\n",
       "count    9674.000000  9674.000000  9674.000000  9674.000000  9674.000000  9.674000e+03  3276.000000  3276.000000  3276.000000     3276.000000     3276.000000         3276.000000        3276.000000   \n",
       "mean   283131.323444     0.202219     0.214481     0.190078     0.200451  7.996915e+05    -1.144439     1.239174    -9.447311      346.364976     -674.503793        -7628.716447         -39.978170   \n",
       "std     27379.690969     0.629919     0.658202     0.596637     0.621049  8.924417e+06     4.105461    20.249965    49.670092     1143.552225     2987.555880        14981.101512         582.658094   \n",
       "min    246458.000000     0.000001     0.000001     0.000001     0.000001  0.000000e+00   -32.894700    -7.452120  -214.492000    -1684.130000   -15489.800000       -66019.900000       -9110.730000   \n",
       "25%    250003.000000     0.013200     0.014000     0.011900     0.013542  0.000000e+00    -0.046680    -0.001370     0.000000      -73.596500      -87.017100        -7555.470000           0.000000   \n",
       "50%    289829.000000     0.037925     0.041070     0.035000     0.038220  6.948000e+03     0.000000     0.000000     0.000000      -10.043000        0.000000          -22.427900           0.000000   \n",
       "75%    292453.000000     0.090000     0.100000     0.081500     0.090000  1.032100e+05     0.000000     0.003530     0.601000        0.000000       12.892500            0.000000          13.492800   \n",
       "max    322964.000000     8.500000     9.140100     8.080000     8.680000  6.647880e+08     1.486670   321.708000   250.000000     3739.110000      100.000000          356.913000         457.778000   \n",
       "\n",
       "       CurrentRatio  InterestCoverage  EpsChangeYear  RevChangeYear         Beta  ShortIntToFloat  \n",
       "count   3276.000000            3276.0         3276.0    3276.000000  3276.000000           3276.0  \n",
       "mean       1.334098               0.0            0.0      -0.468582     1.020204              0.0  \n",
       "std        2.312820               0.0            0.0      85.455414     5.562771              0.0  \n",
       "min        0.000000               0.0            0.0     -77.956700    -5.535060              0.0  \n",
       "25%        0.025240               0.0            0.0       0.000000     0.000000              0.0  \n",
       "50%        0.740420               0.0            0.0       0.000000     0.505670              0.0  \n",
       "75%        1.166000               0.0            0.0       0.000000     1.441880              0.0  \n",
       "max       13.995200               0.0            0.0    1295.330000    85.782400              0.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.signaldriver\")\n",
    "sdf.toPandas().describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5755c8c-0a33-495c-9336-153222484559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>CompanyId</th>\n",
       "      <th>StockDate</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>TomorrowClose</th>\n",
       "      <th>Return</th>\n",
       "      <th>TomorrowReturn</th>\n",
       "      <th>MA</th>\n",
       "      <th>MA_slope</th>\n",
       "      <th>UpTrend_MA</th>\n",
       "      <th>DownTrend_MA</th>\n",
       "      <th>MomentumUp</th>\n",
       "      <th>MomentumDown</th>\n",
       "      <th>ConfirmedUpTrend</th>\n",
       "      <th>ConfirmedDownTrend</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>LowVolatility</th>\n",
       "      <th>HighVolatility</th>\n",
       "      <th>SignalStrength</th>\n",
       "      <th>SignalStrengthHybrid</th>\n",
       "      <th>ActionConfidence</th>\n",
       "      <th>BullishStrengthHybrid</th>\n",
       "      <th>BearishStrengthHybrid</th>\n",
       "      <th>SignalDuration</th>\n",
       "      <th>PatternAction</th>\n",
       "      <th>CandleAction</th>\n",
       "      <th>UpTrend_Return</th>\n",
       "      <th>CandidateAction</th>\n",
       "      <th>Action</th>\n",
       "      <th>TomorrowAction</th>\n",
       "      <th>TimeFrame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>0.072635</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>-0.270733</td>\n",
       "      <td>0.078122</td>\n",
       "      <td>0.075762</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.139085</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.072635</td>\n",
       "      <td>0.072635</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.270733</td>\n",
       "      <td>0.376747</td>\n",
       "      <td>0.077385</td>\n",
       "      <td>0.050717</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.162748</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>0.780777</td>\n",
       "      <td>0.475007</td>\n",
       "      <td>0.975007</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-04</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.376747</td>\n",
       "      <td>-0.101000</td>\n",
       "      <td>0.079715</td>\n",
       "      <td>0.072596</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.199934</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.829140</td>\n",
       "      <td>0.693257</td>\n",
       "      <td>0.829140</td>\n",
       "      <td>0.829140</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-0.101000</td>\n",
       "      <td>-0.221357</td>\n",
       "      <td>0.080604</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.204937</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.644575</td>\n",
       "      <td>0.582518</td>\n",
       "      <td>0.644575</td>\n",
       "      <td>0.644575</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>0.0808</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>-0.221357</td>\n",
       "      <td>0.112857</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.053309</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.218150</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.368132</td>\n",
       "      <td>0.314192</td>\n",
       "      <td>0.368132</td>\n",
       "      <td>0.368132</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-09</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.112857</td>\n",
       "      <td>0.129653</td>\n",
       "      <td>0.079974</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.220316</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.753102</td>\n",
       "      <td>0.545175</td>\n",
       "      <td>0.253102</td>\n",
       "      <td>0.753102</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.075010</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.129653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081524</td>\n",
       "      <td>0.054344</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.219794</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605236</td>\n",
       "      <td>0.456455</td>\n",
       "      <td>0.605236</td>\n",
       "      <td>0.605236</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.062308</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.219822</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.565408</td>\n",
       "      <td>0.432558</td>\n",
       "      <td>0.565408</td>\n",
       "      <td>0.565408</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>-0.235294</td>\n",
       "      <td>0.084304</td>\n",
       "      <td>0.067786</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.220776</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>0.447287</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>249991</td>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>-0.235294</td>\n",
       "      <td>0.224615</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>0.083625</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.236992</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.864551</td>\n",
       "      <td>0.612064</td>\n",
       "      <td>0.364551</td>\n",
       "      <td>0.864551</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  CompanyId   StockDate    Open    High       Low     Close  TomorrowClose    Return  TomorrowReturn        MA  MA_slope  UpTrend_MA  DownTrend_MA  MomentumUp  MomentumDown  \\\n",
       "0       1     249991  2025-09-02  0.0800  0.1094  0.070000  0.099600       0.072635  0.383333       -0.270733  0.078122  0.075762        True         False        True         False   \n",
       "1       1     249991  2025-09-03  0.0900  0.1000  0.072635  0.072635       0.100000 -0.270733        0.376747  0.077385  0.050717        True         False       False          True   \n",
       "2       1     249991  2025-09-04  0.0700  0.1000  0.050000  0.100000       0.089900  0.376747       -0.101000  0.079715  0.072596        True         False        True         False   \n",
       "3       1     249991  2025-09-05  0.0990  0.0990  0.089900  0.089900       0.070000 -0.101000       -0.221357  0.080604  0.083069        True         False        True         False   \n",
       "4       1     249991  2025-09-08  0.0808  0.0950  0.070000  0.070000       0.077900 -0.221357        0.112857  0.080064  0.053309        True         False       False          True   \n",
       "5       1     249991  2025-09-09  0.0900  0.0948  0.052700  0.077900       0.088000  0.112857        0.129653  0.079974  0.044567        True         False       False         False   \n",
       "6       1     249991  2025-09-10  0.0780  0.0900  0.075010  0.088000       0.088000  0.129653        0.000000  0.081524  0.054344        True         False        True         False   \n",
       "7       1     249991  2025-09-11  0.0650  0.0880  0.065000  0.088000       0.085000  0.000000       -0.034091  0.083064  0.062308        True         False        True         False   \n",
       "8       1     249991  2025-09-12  0.0850  0.0850  0.085000  0.085000       0.065000 -0.034091       -0.235294  0.084304  0.067786        True         False        True         False   \n",
       "9       1     249991  2025-09-15  0.0605  0.0650  0.060500  0.065000       0.079600 -0.235294        0.224615  0.083603  0.083625        True         False       False          True   \n",
       "\n",
       "   ConfirmedUpTrend  ConfirmedDownTrend  Volatility  LowVolatility  HighVolatility  SignalStrength  SignalStrengthHybrid  ActionConfidence  BullishStrengthHybrid  BearishStrengthHybrid  \\\n",
       "0              True               False    0.139085           True           False             0.0              1.000000          0.795773               1.000000               1.000000   \n",
       "1             False                True    0.162748          False            True             2.4              0.975007          0.780777               0.475007               0.975007   \n",
       "2              True               False    0.199934          False            True             1.2              0.829140          0.693257               0.829140               0.829140   \n",
       "3              True               False    0.204937          False            True             1.2              0.644575          0.582518               0.644575               0.644575   \n",
       "4             False                True    0.218150          False            True             1.2              0.368132          0.314192               0.368132               0.368132   \n",
       "5             False               False    0.220316          False            True             3.0              0.753102          0.545175               0.253102               0.753102   \n",
       "6              True               False    0.219794          False            True             0.0              0.605236          0.456455               0.605236               0.605236   \n",
       "7              True               False    0.219822          False            True             0.0              0.565408          0.432558               0.565408               0.565408   \n",
       "8              True               False    0.220776          False            True             2.4              0.589957          0.447287               0.589957               0.589957   \n",
       "9             False                True    0.236992          False            True             1.2              0.864551          0.612064               0.364551               0.864551   \n",
       "\n",
       "   SignalDuration PatternAction CandleAction  UpTrend_Return CandidateAction Action TomorrowAction TimeFrame  \n",
       "0            18.0          Hold          Buy            True             Buy   Hold           Sell      Long  \n",
       "1            18.0          Hold         Hold           False            Sell   Sell            Buy      Long  \n",
       "2            18.0          Hold         Hold            True             Buy    Buy           Sell      Long  \n",
       "3            19.0          Hold         Hold            True            Sell   Sell           Hold      Long  \n",
       "4            19.0          Hold         Hold           False            Sell   Hold            Buy      Long  \n",
       "5            19.0          Hold         Hold           False             Buy    Buy           Hold      Long  \n",
       "6            19.0          Hold          Buy            True             Buy   Hold           Hold      Long  \n",
       "7            18.0          Hold         Hold            True            Hold   Hold           Sell      Long  \n",
       "8            17.0          Hold         Hold            True            Sell   Sell           Hold      Long  \n",
       "9            17.0          Hold         Hold           False            Sell   Hold            Buy      Long  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718306cb-f473-455a-a85c-57485593f9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyId</th>\n",
       "      <th>TimeFrame</th>\n",
       "      <th>StockDate</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>TomorrowClose</th>\n",
       "      <th>Return</th>\n",
       "      <th>TomorrowReturn</th>\n",
       "      <th>MA</th>\n",
       "      <th>MA_slope</th>\n",
       "      <th>UpTrend_MA</th>\n",
       "      <th>DownTrend_MA</th>\n",
       "      <th>MomentumUp</th>\n",
       "      <th>MomentumDown</th>\n",
       "      <th>ConfirmedUpTrend</th>\n",
       "      <th>ConfirmedDownTrend</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>LowVolatility</th>\n",
       "      <th>HighVolatility</th>\n",
       "      <th>SignalStrength</th>\n",
       "      <th>SignalStrengthHybrid</th>\n",
       "      <th>ActionConfidence</th>\n",
       "      <th>BullishStrengthHybrid</th>\n",
       "      <th>BearishStrengthHybrid</th>\n",
       "      <th>SignalDuration</th>\n",
       "      <th>PatternAction</th>\n",
       "      <th>CandleAction</th>\n",
       "      <th>UpTrend_Return</th>\n",
       "      <th>CandidateAction</th>\n",
       "      <th>Action</th>\n",
       "      <th>TomorrowAction</th>\n",
       "      <th>BuyRank</th>\n",
       "      <th>BestModel</th>\n",
       "      <th>BestModel_RMSE</th>\n",
       "      <th>BestModel_MAE</th>\n",
       "      <th>BestModel_MAPE</th>\n",
       "      <th>BestModel_DirectionAcc</th>\n",
       "      <th>BestModel_R2</th>\n",
       "      <th>BestModel_AdjR2</th>\n",
       "      <th>Pred_Sklearn</th>\n",
       "      <th>PredictedReturn_Sklearn</th>\n",
       "      <th>Pred_Linear</th>\n",
       "      <th>PredictedReturn_Linear</th>\n",
       "      <th>Linear_RMSE</th>\n",
       "      <th>Linear_MAE</th>\n",
       "      <th>Linear_MAPE</th>\n",
       "      <th>Linear_DirectionAcc</th>\n",
       "      <th>Linear_R2</th>\n",
       "      <th>Linear_AdjR2</th>\n",
       "      <th>Pred_Lasso</th>\n",
       "      <th>PredictedReturn_Lasso</th>\n",
       "      <th>Lasso_RMSE</th>\n",
       "      <th>Lasso_MAE</th>\n",
       "      <th>Lasso_MAPE</th>\n",
       "      <th>Lasso_DirectionAcc</th>\n",
       "      <th>Lasso_R2</th>\n",
       "      <th>Lasso_AdjR2</th>\n",
       "      <th>Pred_Ridge</th>\n",
       "      <th>PredictedReturn_Ridge</th>\n",
       "      <th>Ridge_RMSE</th>\n",
       "      <th>Ridge_MAE</th>\n",
       "      <th>Ridge_MAPE</th>\n",
       "      <th>Ridge_DirectionAcc</th>\n",
       "      <th>Ridge_R2</th>\n",
       "      <th>Ridge_AdjR2</th>\n",
       "      <th>Pred_XGBoost</th>\n",
       "      <th>PredictedReturn_XGBoost</th>\n",
       "      <th>XGBoost_RMSE</th>\n",
       "      <th>XGBoost_MAE</th>\n",
       "      <th>XGBoost_MAPE</th>\n",
       "      <th>XGBoost_DirectionAcc</th>\n",
       "      <th>XGBoost_R2</th>\n",
       "      <th>XGBoost_AdjR2</th>\n",
       "      <th>MaxPredictedReturn</th>\n",
       "      <th>Phase2_Rank</th>\n",
       "      <th>SARIMAX_PredictedClose</th>\n",
       "      <th>SARIMAX_PredictedReturn</th>\n",
       "      <th>WeightedScore</th>\n",
       "      <th>AIC</th>\n",
       "      <th>MlType</th>\n",
       "      <th>TradingSymbol</th>\n",
       "      <th>Name</th>\n",
       "      <th>Phase3_Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100167</td>\n",
       "      <td>Daily</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.03000</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.165837</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>20.801939</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1.999948</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>1.999948</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>14</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>23.965739</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.998176</td>\n",
       "      <td>0.997998</td>\n",
       "      <td>0.059578</td>\n",
       "      <td>1.978905</td>\n",
       "      <td>0.600873</td>\n",
       "      <td>29.043656</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>246.314138</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.779743</td>\n",
       "      <td>0.758329</td>\n",
       "      <td>0.003931</td>\n",
       "      <td>-0.803441</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1267.855456</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.136029</td>\n",
       "      <td>0.052032</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>-0.467759</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>923.894960</td>\n",
       "      <td>0.611814</td>\n",
       "      <td>0.393814</td>\n",
       "      <td>0.334879</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>-0.222219</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>23.965739</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.998176</td>\n",
       "      <td>0.997998</td>\n",
       "      <td>29.043656</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016226</td>\n",
       "      <td>-0.188692</td>\n",
       "      <td>17.350717</td>\n",
       "      <td>-2072.654464</td>\n",
       "      <td>automl</td>\n",
       "      <td>WRCDF</td>\n",
       "      <td>WIRECARD AG, BERLIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92779</td>\n",
       "      <td>Daily</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.193396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012204</td>\n",
       "      <td>-0.021196</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.207573</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.002751</td>\n",
       "      <td>1.002751</td>\n",
       "      <td>1.002751</td>\n",
       "      <td>0.502751</td>\n",
       "      <td>166.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Buy</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>28</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.049286</td>\n",
       "      <td>0.753571</td>\n",
       "      <td>0.997741</td>\n",
       "      <td>0.997548</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.162609</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.066329</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.136591</td>\n",
       "      <td>0.532143</td>\n",
       "      <td>0.981719</td>\n",
       "      <td>0.980160</td>\n",
       "      <td>0.032810</td>\n",
       "      <td>1.593639</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>1.174648</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.103985</td>\n",
       "      <td>0.027581</td>\n",
       "      <td>0.037355</td>\n",
       "      <td>1.952980</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.008866</td>\n",
       "      <td>0.751232</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.562974</td>\n",
       "      <td>0.525708</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>-0.004140</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.049286</td>\n",
       "      <td>0.753571</td>\n",
       "      <td>0.997741</td>\n",
       "      <td>0.997548</td>\n",
       "      <td>1.952980</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.171788</td>\n",
       "      <td>-2287.052933</td>\n",
       "      <td>automl</td>\n",
       "      <td>STGZ</td>\n",
       "      <td>STARGAZE ENTMT GROUP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59260</td>\n",
       "      <td>Daily</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.00165</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.00160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>-0.024691</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.050199</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.067693</td>\n",
       "      <td>1.067693</td>\n",
       "      <td>1.067693</td>\n",
       "      <td>0.567693</td>\n",
       "      <td>182.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>22</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.023138</td>\n",
       "      <td>0.818505</td>\n",
       "      <td>0.992884</td>\n",
       "      <td>0.992280</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.221996</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.023138</td>\n",
       "      <td>0.818505</td>\n",
       "      <td>0.992884</td>\n",
       "      <td>0.992280</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.897631</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.264696</td>\n",
       "      <td>0.362989</td>\n",
       "      <td>0.263236</td>\n",
       "      <td>0.200653</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.592897</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.218106</td>\n",
       "      <td>0.626335</td>\n",
       "      <td>0.437481</td>\n",
       "      <td>0.389699</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.316706</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.800712</td>\n",
       "      <td>0.988075</td>\n",
       "      <td>0.987062</td>\n",
       "      <td>0.897631</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538579</td>\n",
       "      <td>-3603.627631</td>\n",
       "      <td>automl</td>\n",
       "      <td>IGPK</td>\n",
       "      <td>INTEGRATED CANNABIS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67984</td>\n",
       "      <td>Daily</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.05670</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.05670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058197</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080576</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1.491882</td>\n",
       "      <td>1.491882</td>\n",
       "      <td>1.491882</td>\n",
       "      <td>0.491882</td>\n",
       "      <td>165.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Buy</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>6</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.996861</td>\n",
       "      <td>0.057045</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.032772</td>\n",
       "      <td>0.764479</td>\n",
       "      <td>0.989717</td>\n",
       "      <td>0.988762</td>\n",
       "      <td>0.083729</td>\n",
       "      <td>0.476702</td>\n",
       "      <td>0.051672</td>\n",
       "      <td>0.035789</td>\n",
       "      <td>0.629236</td>\n",
       "      <td>0.478764</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>-0.090701</td>\n",
       "      <td>0.064654</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.125533</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.922731</td>\n",
       "      <td>0.915559</td>\n",
       "      <td>0.052742</td>\n",
       "      <td>-0.069798</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>0.996861</td>\n",
       "      <td>0.476702</td>\n",
       "      <td>7</td>\n",
       "      <td>0.056432</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>0.284134</td>\n",
       "      <td>-1555.210275</td>\n",
       "      <td>automl</td>\n",
       "      <td>MATEF</td>\n",
       "      <td>BLOCKMATE VENTURES I</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98577</td>\n",
       "      <td>Daily</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.00270</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.00270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>123.0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>13</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>13.051323</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.989193</td>\n",
       "      <td>0.988122</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.074258</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>-0.036688</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>113.438423</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.525668</td>\n",
       "      <td>0.478662</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.266543</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>300.929127</td>\n",
       "      <td>0.352459</td>\n",
       "      <td>0.056072</td>\n",
       "      <td>-0.037470</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.453637</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>263.323714</td>\n",
       "      <td>0.209016</td>\n",
       "      <td>0.234838</td>\n",
       "      <td>0.159011</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>13.051323</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.989193</td>\n",
       "      <td>0.988122</td>\n",
       "      <td>0.453637</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>-0.009282</td>\n",
       "      <td>0.268469</td>\n",
       "      <td>-2302.084419</td>\n",
       "      <td>automl</td>\n",
       "      <td>VLDXW</td>\n",
       "      <td>VELO3D INC Warrant 09/29/2026</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30518</td>\n",
       "      <td>Long</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>0.09200</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>0.09200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.088110</td>\n",
       "      <td>-0.107747</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.082950</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.383120</td>\n",
       "      <td>1.383120</td>\n",
       "      <td>1.383120</td>\n",
       "      <td>0.883120</td>\n",
       "      <td>128.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>25</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.018605</td>\n",
       "      <td>0.758123</td>\n",
       "      <td>0.995496</td>\n",
       "      <td>0.995069</td>\n",
       "      <td>0.096912</td>\n",
       "      <td>0.053389</td>\n",
       "      <td>0.082807</td>\n",
       "      <td>-0.099920</td>\n",
       "      <td>0.022296</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.058546</td>\n",
       "      <td>0.299639</td>\n",
       "      <td>0.914019</td>\n",
       "      <td>0.905862</td>\n",
       "      <td>0.166288</td>\n",
       "      <td>0.807478</td>\n",
       "      <td>0.067495</td>\n",
       "      <td>0.053532</td>\n",
       "      <td>0.356575</td>\n",
       "      <td>0.425993</td>\n",
       "      <td>0.212070</td>\n",
       "      <td>0.137325</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>0.025406</td>\n",
       "      <td>0.026544</td>\n",
       "      <td>0.018189</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.288809</td>\n",
       "      <td>0.878133</td>\n",
       "      <td>0.866572</td>\n",
       "      <td>0.095389</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.018605</td>\n",
       "      <td>0.758123</td>\n",
       "      <td>0.995496</td>\n",
       "      <td>0.995069</td>\n",
       "      <td>0.807478</td>\n",
       "      <td>4</td>\n",
       "      <td>0.092776</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>0.487858</td>\n",
       "      <td>-1033.987435</td>\n",
       "      <td>automl</td>\n",
       "      <td>BXPHF</td>\n",
       "      <td>BOTANIX PHARMACEUTIC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>89267</td>\n",
       "      <td>Long</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013548</td>\n",
       "      <td>-0.171913</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.113184</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1.809491</td>\n",
       "      <td>1.809491</td>\n",
       "      <td>1.809491</td>\n",
       "      <td>0.809491</td>\n",
       "      <td>164.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Buy</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>2</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.825623</td>\n",
       "      <td>0.995822</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>0.015102</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>0.825623</td>\n",
       "      <td>0.992291</td>\n",
       "      <td>0.991571</td>\n",
       "      <td>0.022729</td>\n",
       "      <td>0.515267</td>\n",
       "      <td>0.007265</td>\n",
       "      <td>0.005928</td>\n",
       "      <td>0.198650</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.450761</td>\n",
       "      <td>0.399470</td>\n",
       "      <td>0.025536</td>\n",
       "      <td>0.702405</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.113349</td>\n",
       "      <td>0.640569</td>\n",
       "      <td>0.778354</td>\n",
       "      <td>0.757655</td>\n",
       "      <td>0.014550</td>\n",
       "      <td>-0.029991</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.825623</td>\n",
       "      <td>0.995822</td>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.702405</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421443</td>\n",
       "      <td>-2454.425826</td>\n",
       "      <td>automl</td>\n",
       "      <td>STEK</td>\n",
       "      <td>STEMTECH CORP</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>246380</td>\n",
       "      <td>Long</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.00770</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.00770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>-0.195496</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.098804</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.422227</td>\n",
       "      <td>1.422227</td>\n",
       "      <td>1.422227</td>\n",
       "      <td>0.922227</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>False</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>21</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.026787</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.994122</td>\n",
       "      <td>0.993114</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.101565</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>0.117465</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.052770</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.961367</td>\n",
       "      <td>0.954744</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.537905</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.344840</td>\n",
       "      <td>0.469512</td>\n",
       "      <td>0.109062</td>\n",
       "      <td>-0.043670</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.200514</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.162691</td>\n",
       "      <td>0.310976</td>\n",
       "      <td>0.789575</td>\n",
       "      <td>0.753503</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>0.043354</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.026787</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.994122</td>\n",
       "      <td>0.993114</td>\n",
       "      <td>0.537905</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.039777</td>\n",
       "      <td>0.338654</td>\n",
       "      <td>-1355.605064</td>\n",
       "      <td>automl</td>\n",
       "      <td>EEEP</td>\n",
       "      <td>EP3OIL INC</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89124</td>\n",
       "      <td>Long</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.01438</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.01330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011315</td>\n",
       "      <td>-0.100270</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.127735</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1.441590</td>\n",
       "      <td>1.441590</td>\n",
       "      <td>1.441590</td>\n",
       "      <td>0.774923</td>\n",
       "      <td>132.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Buy</td>\n",
       "      <td>True</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>12</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.027635</td>\n",
       "      <td>0.770609</td>\n",
       "      <td>0.997056</td>\n",
       "      <td>0.996792</td>\n",
       "      <td>0.013320</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>-0.021669</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.059028</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.982274</td>\n",
       "      <td>0.980682</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.506449</td>\n",
       "      <td>0.007263</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>0.643098</td>\n",
       "      <td>0.347670</td>\n",
       "      <td>0.145434</td>\n",
       "      <td>0.068656</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.547076</td>\n",
       "      <td>0.006141</td>\n",
       "      <td>0.005057</td>\n",
       "      <td>0.505384</td>\n",
       "      <td>0.501792</td>\n",
       "      <td>0.389065</td>\n",
       "      <td>0.334176</td>\n",
       "      <td>0.012547</td>\n",
       "      <td>-0.056616</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.027635</td>\n",
       "      <td>0.770609</td>\n",
       "      <td>0.997056</td>\n",
       "      <td>0.996792</td>\n",
       "      <td>0.547076</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012683</td>\n",
       "      <td>-0.046420</td>\n",
       "      <td>0.309678</td>\n",
       "      <td>-2684.379776</td>\n",
       "      <td>automl</td>\n",
       "      <td>STRH</td>\n",
       "      <td>STAR8 CORP</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>77593</td>\n",
       "      <td>Long</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.05400</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.05400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.313869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047860</td>\n",
       "      <td>-0.179215</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.224107</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1.936992</td>\n",
       "      <td>1.936992</td>\n",
       "      <td>1.936992</td>\n",
       "      <td>0.936992</td>\n",
       "      <td>158.0</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Buy</td>\n",
       "      <td>False</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>4</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.722022</td>\n",
       "      <td>0.979096</td>\n",
       "      <td>0.977469</td>\n",
       "      <td>0.056285</td>\n",
       "      <td>0.042319</td>\n",
       "      <td>0.054709</td>\n",
       "      <td>0.013125</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.038012</td>\n",
       "      <td>0.649819</td>\n",
       "      <td>0.931183</td>\n",
       "      <td>0.925827</td>\n",
       "      <td>0.077266</td>\n",
       "      <td>0.430855</td>\n",
       "      <td>0.017697</td>\n",
       "      <td>0.013972</td>\n",
       "      <td>0.217328</td>\n",
       "      <td>0.350181</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.312957</td>\n",
       "      <td>0.011484</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.125892</td>\n",
       "      <td>0.592058</td>\n",
       "      <td>0.611192</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.050608</td>\n",
       "      <td>-0.062810</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.722022</td>\n",
       "      <td>0.979096</td>\n",
       "      <td>0.977469</td>\n",
       "      <td>0.430855</td>\n",
       "      <td>6</td>\n",
       "      <td>0.050863</td>\n",
       "      <td>-0.058084</td>\n",
       "      <td>0.235279</td>\n",
       "      <td>-1643.086230</td>\n",
       "      <td>automl</td>\n",
       "      <td>PUBC</td>\n",
       "      <td>PUREBASE CORP</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyId TimeFrame   StockDate    Open     High     Low    Close  TomorrowClose     Return  TomorrowReturn        MA  MA_slope  UpTrend_MA  DownTrend_MA  MomentumUp  MomentumDown  \\\n",
       "0     100167     Daily  2025-09-22  0.0200  0.03000  0.0200  0.02000            NaN  65.666667             NaN  0.010043  0.165837        True         False        True         False   \n",
       "1      92779     Daily  2025-09-22  0.0115  0.01265  0.0115  0.01265            NaN   0.193396             NaN  0.012204 -0.021196       False          True       False          True   \n",
       "2      59260     Daily  2025-09-22  0.0015  0.00165  0.0015  0.00160            NaN   0.032258             NaN  0.001693 -0.024691       False          True       False          True   \n",
       "3      67984     Daily  2025-09-22  0.0539  0.05670  0.0539  0.05670            NaN   0.051948             NaN  0.058197 -0.009964       False          True       False          True   \n",
       "4      98577     Daily  2025-09-22  0.0027  0.00270  0.0027  0.00270            NaN   0.038462             NaN  0.002614  0.005495        True         False        True         False   \n",
       "5      30518      Long  2025-09-22  0.0920  0.09200  0.0920  0.09200            NaN   0.000000             NaN  0.088110 -0.107747       False          True       False          True   \n",
       "6      89267      Long  2025-09-22  0.0124  0.01500  0.0124  0.01500            NaN   0.271186             NaN  0.013548 -0.171913       False          True        True         False   \n",
       "7     246380      Long  2025-09-22  0.0077  0.00770  0.0077  0.00770            NaN   0.000000             NaN  0.007860 -0.195496       False          True       False          True   \n",
       "8      89124      Long  2025-09-22  0.0108  0.01438  0.0101  0.01330            NaN   0.330000             NaN  0.011315 -0.100270       False          True        True         False   \n",
       "9      77593      Long  2025-09-22  0.0540  0.05400  0.0485  0.05400            NaN   0.313869             NaN  0.047860 -0.179215       False          True       False          True   \n",
       "\n",
       "   ConfirmedUpTrend  ConfirmedDownTrend  Volatility  LowVolatility  HighVolatility  SignalStrength  SignalStrengthHybrid  ActionConfidence  BullishStrengthHybrid  BearishStrengthHybrid  \\\n",
       "0              True               False   20.801939          False            True               4              1.999948          0.999948               0.999948               1.999948   \n",
       "1             False               False    0.207573           True           False               2              1.002751          1.002751               1.002751               0.502751   \n",
       "2             False               False    0.050199           True           False               2              1.067693          1.067693               1.067693               0.567693   \n",
       "3             False               False    0.080576          False            True               4              1.491882          1.491882               1.491882               0.491882   \n",
       "4              True               False    0.012163           True           False               3              1.000000          1.000000               1.000000               1.000000   \n",
       "5             False                True    0.082950           True           False               2              1.383120          1.383120               1.383120               0.883120   \n",
       "6             False               False    0.113184          False            True               4              1.809491          1.809491               1.809491               0.809491   \n",
       "7             False                True    0.098804           True           False               2              1.422227          1.422227               1.422227               0.922227   \n",
       "8             False               False    0.127735          False            True               3              1.441590          1.441590               1.441590               0.774923   \n",
       "9             False                True    0.224107          False            True               3              1.936992          1.936992               1.936992               0.936992   \n",
       "\n",
       "   SignalDuration PatternAction CandleAction  UpTrend_Return CandidateAction Action TomorrowAction  BuyRank BestModel  BestModel_RMSE  BestModel_MAE  BestModel_MAPE  BestModel_DirectionAcc  \\\n",
       "0            41.0           Buy         Hold            True             Buy    Buy           Hold       14   XGBoost        0.000222       0.000117       23.965739                0.924051   \n",
       "1           166.0          Sell          Buy            True             Buy    Buy           Hold       28   XGBoost        0.000830       0.000615        0.049286                0.753571   \n",
       "2           182.0          Sell         Hold            True             Buy    Buy           Hold       22    Linear        0.000130       0.000083        0.023138                0.818505   \n",
       "3           165.0          Sell          Buy            True             Buy    Buy           Hold        6   XGBoost        0.002772       0.001885        0.024518                0.857143   \n",
       "4           123.0           Buy         Hold            True             Buy    Buy           Hold       13   XGBoost        0.000310       0.000190       13.051323                0.770492   \n",
       "5           128.0          Sell         Hold           False             Buy    Buy           Hold       25   XGBoost        0.005103       0.003319        0.018605                0.758123   \n",
       "6           164.0          Sell          Buy            True             Buy    Buy           Hold        2   XGBoost        0.000634       0.000458        0.015104                0.825623   \n",
       "7            68.0          Sell         Hold           False             Buy    Buy           Hold       21   XGBoost        0.000473       0.000338        0.026787                0.634146   \n",
       "8           132.0          Sell          Buy            True             Buy    Buy           Hold       12   XGBoost        0.000426       0.000303        0.027635                0.770609   \n",
       "9           158.0          Sell          Buy           False             Buy    Buy           Hold        4   XGBoost        0.002663       0.001877        0.029197                0.722022   \n",
       "\n",
       "   BestModel_R2  BestModel_AdjR2  Pred_Sklearn  PredictedReturn_Sklearn  Pred_Linear  PredictedReturn_Linear  Linear_RMSE  Linear_MAE  Linear_MAPE  Linear_DirectionAcc  Linear_R2  Linear_AdjR2  \\\n",
       "0      0.998176         0.997998      0.059578                 1.978905     0.600873               29.043656     0.002445    0.001216   246.314138             0.620253   0.779743      0.758329   \n",
       "1      0.997741         0.997548      0.014707                 0.162609     0.013489                0.066329     0.002362    0.001615     0.136591             0.532143   0.981719      0.980160   \n",
       "2      0.992884         0.992280      0.001955                 0.221996     0.001665                0.040569     0.000130    0.000083     0.023138             0.818505   0.992884      0.992280   \n",
       "3      0.997127         0.996861      0.057045                 0.006088     0.059701                0.052933     0.005245    0.002749     0.032772             0.764479   0.989717      0.988762   \n",
       "4      0.989193         0.988122      0.002900                 0.074258     0.002601               -0.036688     0.002057    0.001217   113.438423             0.213115   0.525668      0.478662   \n",
       "5      0.995496         0.995069      0.096912                 0.053389     0.082807               -0.099920     0.022296    0.011417     0.058546             0.299639   0.914019      0.905862   \n",
       "6      0.995822         0.995432      0.015890                 0.059365     0.015102                0.006772     0.000861    0.000524     0.016273             0.825623   0.992291      0.991571   \n",
       "7      0.994122         0.993114      0.008482                 0.101565     0.008604                0.117465     0.001213    0.000745     0.052770             0.341463   0.961367      0.954744   \n",
       "8      0.997056         0.996792      0.013320                 0.001486     0.013012               -0.021669     0.001046    0.000654     0.059028             0.591398   0.982274      0.980682   \n",
       "9      0.979096         0.977469      0.056285                 0.042319     0.054709                0.013125     0.004831    0.002594     0.038012             0.649819   0.931183      0.925827   \n",
       "\n",
       "   Pred_Lasso  PredictedReturn_Lasso  Lasso_RMSE  Lasso_MAE   Lasso_MAPE  Lasso_DirectionAcc  Lasso_R2  Lasso_AdjR2  Pred_Ridge  PredictedReturn_Ridge  Ridge_RMSE  Ridge_MAE  Ridge_MAPE  \\\n",
       "0    0.003931              -0.803441    0.004842   0.004620  1267.855456            0.645570  0.136029     0.052032    0.010645              -0.467759    0.004055   0.003307  923.894960   \n",
       "1    0.032810               1.593639    0.016537   0.013329     1.174648            0.392857  0.103985     0.027581    0.037355               1.952980    0.011549   0.008866    0.751232   \n",
       "2    0.003036               0.897631    0.001319   0.001031     0.264696            0.362989  0.263236     0.200653    0.002549               0.592897    0.001153   0.000893    0.218106   \n",
       "3    0.083729               0.476702    0.051672   0.035789     0.629236            0.478764  0.001945    -0.090701    0.064654               0.140284    0.014377   0.009303    0.125533   \n",
       "4    0.003420               0.266543    0.002902   0.002071   300.929127            0.352459  0.056072    -0.037470    0.003925               0.453637    0.002612   0.001865  263.323714   \n",
       "5    0.166288               0.807478    0.067495   0.053532     0.356575            0.425993  0.212070     0.137325    0.094337               0.025406    0.026544   0.018189    0.100009   \n",
       "6    0.022729               0.515267    0.007265   0.005928     0.198650            0.298932  0.450761     0.399470    0.025536               0.702405    0.004615   0.003581    0.113349   \n",
       "7    0.011842               0.537905    0.005825   0.004602     0.344840            0.469512  0.109062    -0.043670    0.009244               0.200514    0.002831   0.002210    0.162691   \n",
       "8    0.020036               0.506449    0.007263   0.006008     0.643098            0.347670  0.145434     0.068656    0.020576               0.547076    0.006141   0.005057    0.505384   \n",
       "9    0.077266               0.430855    0.017697   0.013972     0.217328            0.350181  0.076667     0.004813    0.070900               0.312957    0.011484   0.008408    0.125892   \n",
       "\n",
       "   Ridge_DirectionAcc  Ridge_R2  Ridge_AdjR2  Pred_XGBoost  PredictedReturn_XGBoost  XGBoost_RMSE  XGBoost_MAE  XGBoost_MAPE  XGBoost_DirectionAcc  XGBoost_R2  XGBoost_AdjR2  MaxPredictedReturn  \\\n",
       "0            0.611814  0.393814     0.334879      0.015556                -0.222219      0.000222     0.000117     23.965739              0.924051    0.998176       0.997998           29.043656   \n",
       "1            0.428571  0.562974     0.525708      0.012598                -0.004140      0.000830     0.000615      0.049286              0.753571    0.997741       0.997548            1.952980   \n",
       "2            0.626335  0.437481     0.389699      0.002107                 0.316706      0.000168     0.000109      0.030664              0.800712    0.988075       0.987062            0.897631   \n",
       "3            0.621622  0.922731     0.915559      0.052742                -0.069798      0.002772     0.001885      0.024518              0.857143    0.997127       0.996861            0.476702   \n",
       "4            0.209016  0.234838     0.159011      0.002768                 0.025307      0.000310     0.000190     13.051323              0.770492    0.989193       0.988122            0.453637   \n",
       "5            0.288809  0.878133     0.866572      0.095389                 0.036842      0.005103     0.003319      0.018605              0.758123    0.995496       0.995069            0.807478   \n",
       "6            0.640569  0.778354     0.757655      0.014550                -0.029991      0.000634     0.000458      0.015104              0.825623    0.995822       0.995432            0.702405   \n",
       "7            0.310976  0.789575     0.753503      0.008034                 0.043354      0.000473     0.000338      0.026787              0.634146    0.994122       0.993114            0.537905   \n",
       "8            0.501792  0.389065     0.334176      0.012547                -0.056616      0.000426     0.000303      0.027635              0.770609    0.997056       0.996792            0.547076   \n",
       "9            0.592058  0.611192     0.580934      0.050608                -0.062810      0.002663     0.001877      0.029197              0.722022    0.979096       0.977469            0.430855   \n",
       "\n",
       "   Phase2_Rank  SARIMAX_PredictedClose  SARIMAX_PredictedReturn  WeightedScore          AIC  MlType TradingSymbol                           Name  Phase3_Rank  \n",
       "0            1                0.016226                -0.188692      17.350717 -2072.654464  automl         WRCDF            WIRECARD AG, BERLIN            1  \n",
       "1            3                0.012650                 0.000000       1.171788 -2287.052933  automl          STGZ           STARGAZE ENTMT GROUP            2  \n",
       "2            2                0.001600                 0.000000       0.538579 -3603.627631  automl          IGPK            INTEGRATED CANNABIS            3  \n",
       "3            7                0.056432                -0.004718       0.284134 -1555.210275  automl         MATEF           BLOCKMATE VENTURES I            4  \n",
       "4            5                0.002675                -0.009282       0.268469 -2302.084419  automl         VLDXW  VELO3D INC Warrant 09/29/2026            5  \n",
       "5            4                0.092776                 0.008429       0.487858 -1033.987435  automl         BXPHF           BOTANIX PHARMACEUTIC            1  \n",
       "6            3                0.015000                 0.000000       0.421443 -2454.425826  automl          STEK                  STEMTECH CORP            2  \n",
       "7            2                0.008006                 0.039777       0.338654 -1355.605064  automl          EEEP                     EP3OIL INC            3  \n",
       "8            9                0.012683                -0.046420       0.309678 -2684.379776  automl          STRH                     STAR8 CORP            4  \n",
       "9            6                0.050863                -0.058084       0.235279 -1643.086230  automl          PUBC                  PUREBASE CORP            5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.final_candidates\")\n",
    "pdf = sdf.limit(10).toPandas()\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6ccc2ff-2a09-4c60-96dc-31d4b57bc705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/29 11:26:57 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/29 11:26:57 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/29 11:26:57 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/29 11:26:57 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|ListingExchange|row_count|\n",
      "+---------------+---------+\n",
      "|1              |6442     |\n",
      "|2              |3405     |\n",
      "|16             |21204    |\n",
      "+---------------+---------+\n",
      "\n",
      "+---------+---------+\n",
      "|TimeFrame|row_count|\n",
      "+---------+---------+\n",
      "|Long     |9674     |\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.table(\"bsf.company\")\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy(\"ListingExchange\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy(\"ListingExchange\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "sdf = spark.table(\"bsf.history_signals\")\n",
    "\n",
    "# Group by CompanyId, TimeFrame, StockDate\n",
    "counts_df = (\n",
    "    sdf.groupBy( \"TimeFrame\")\n",
    "      .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "      .orderBy( \"TimeFrame\")\n",
    ")\n",
    "\n",
    "counts_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfffc9e5-751f-43d0-97f8-1e601221412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CompanyId: long (nullable = true)\n",
      " |-- StockDate: date (nullable = true)\n",
      " |-- OpenPrice: double (nullable = true)\n",
      " |-- HighPrice: double (nullable = true)\n",
      " |-- LowPrice: double (nullable = true)\n",
      " |-- ClosePrice: double (nullable = true)\n",
      " |-- StockVolume: double (nullable = true)\n",
      "\n",
      "['CompanyId', 'StockDate', 'OpenPrice', 'HighPrice', 'LowPrice', 'ClosePrice', 'StockVolume']\n",
      "+---------+----------+---------+---------+--------+----------+-----------+\n",
      "|CompanyId|StockDate |OpenPrice|HighPrice|LowPrice|ClosePrice|StockVolume|\n",
      "+---------+----------+---------+---------+--------+----------+-----------+\n",
      "|64490    |2024-08-28|0.054    |0.054    |0.0535  |0.0535    |3200.0     |\n",
      "|64490    |2024-08-29|0.0533   |0.0533   |0.0533  |0.0533    |2499.0     |\n",
      "|64490    |2024-08-30|0.053    |0.053    |0.053   |0.053     |1334.0     |\n",
      "|64490    |2024-09-03|0.041    |0.053    |0.041   |0.0416    |5275.0     |\n",
      "|64490    |2024-09-04|0.0416   |0.0416   |0.0416  |0.0416    |0.0        |\n",
      "|64490    |2024-09-05|0.041    |0.041    |0.041   |0.041     |100.0      |\n",
      "|64490    |2024-09-06|0.0449   |0.0449   |0.0416  |0.0416    |2124.0     |\n",
      "|64490    |2024-09-09|0.041    |0.0462   |0.041   |0.0462    |1316.0     |\n",
      "|64490    |2024-09-10|0.0462   |0.0462   |0.0462  |0.0462    |0.0        |\n",
      "|64490    |2024-09-11|0.0462   |0.0462   |0.0462  |0.0462    |0.0        |\n",
      "|64490    |2024-09-12|0.0462   |0.0462   |0.0462  |0.0462    |0.0        |\n",
      "|64490    |2024-09-13|0.041    |0.044965 |0.0401  |0.044965  |10705.0    |\n",
      "|64490    |2024-09-16|0.0401   |0.054    |0.0401  |0.054     |2568.0     |\n",
      "|64490    |2024-09-17|0.054    |0.054    |0.054   |0.054     |2000.0     |\n",
      "|64490    |2024-09-18|0.054    |0.054    |0.054   |0.054     |0.0        |\n",
      "|64490    |2024-09-19|0.0401   |0.054    |0.0401  |0.04427   |2756.0     |\n",
      "|64490    |2024-09-20|0.04427  |0.04427  |0.04427 |0.04427   |0.0        |\n",
      "|64490    |2024-09-23|0.0401   |0.0401   |0.0401  |0.0401    |635.0      |\n",
      "|64490    |2024-09-24|0.0401   |0.0401   |0.0401  |0.0401    |0.0        |\n",
      "|64490    |2024-09-25|0.049135 |0.0498   |0.0401  |0.04704   |6605.0     |\n",
      "+---------+----------+---------+---------+--------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('CompanyId', 'bigint'),\n",
       " ('StockDate', 'date'),\n",
       " ('OpenPrice', 'double'),\n",
       " ('HighPrice', 'double'),\n",
       " ('LowPrice', 'double'),\n",
       " ('ClosePrice', 'double'),\n",
       " ('StockVolume', 'double')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume you have a Spark DataFrame\n",
    "sdf = spark.table(\"bsf.companystockhistory\")\n",
    "\n",
    "# 1️⃣ Show the schema (data types included)\n",
    "sdf.printSchema()\n",
    "\n",
    "# 2️⃣ Get a list of column names\n",
    "# print(sdf.columns)\n",
    "\n",
    "# 3️⃣ Show first few rows with all columns (default shows truncated view)\n",
    "sdf.show(truncate=False)  \n",
    "\n",
    "# 4️⃣ For more detailed metadata about columns\n",
    "sdf.dtypes  # Returns a list of (column_name, data_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f906d5-5d27-4ddb-97ac-946e9442affa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "⚠️ This notebook is blocked. Do NOT run all cells without checking!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 🚨 Safety stop — prevents accidental full execution\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ This notebook is blocked. Do NOT run all cells without checking!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ⚠️ This notebook is blocked. Do NOT run all cells without checking!"
     ]
    }
   ],
   "source": [
    "# 🚨 Safety stop — prevents accidental full execution\n",
    "raise RuntimeError(\"⚠️ This notebook is blocked. Do NOT run all cells without checking!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
