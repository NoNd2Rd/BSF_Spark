{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b906b38d-f9ff-445b-b0ad-0105d57a153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/jupyter/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyter/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyter/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e15ec5bf-ead3-4adb-8535-2325f0d948db;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0rc1 in spark-list\n",
      "\tfound io.delta#delta-storage;3.0.0rc1 in spark-list\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in spark-list\n",
      ":: resolution report :: resolve 394ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0rc1 from spark-list in [default]\n",
      "\tio.delta#delta-storage;3.0.0rc1 from spark-list in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from spark-list in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e15ec5bf-ead3-4adb-8535-2325f0d948db\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/12ms)\n",
      "25/09/21 22:09:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_candidates_analysis' log_level=WARN (effective=WARN), progress=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/21 22:09:34 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/21 22:09:34 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/21 22:09:34 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/21 22:09:34 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/21 22:09:34 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/21 22:09:34 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/21 22:09:37 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/21 22:09:37 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/21 22:09:37 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/21 22:09:40 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "spark = init_spark(\"bsf_candidates_analysis\", log_level=\"WARN\", show_progress=False, enable_ui=True, process_option=\"wide\")\n",
    "engine = init_mariadb_engine()\n",
    "\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", 200)         # Adjust width for readability\n",
    "pd.set_option(\"display.max_rows\", 20)       # Show only top 20 rows by default\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# -----------------------------\n",
    "# Load tables\n",
    "# -----------------------------\n",
    "df_last = spark.table(\"bsf.history_signals_last\")\n",
    "df_all = spark.table(\"bsf.history_signals\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b93743-3435-4b94-a889-c087e2541230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage 1 completed: Top 100 candidates selected per timeframe\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Filter only Buy actions from last-row DF\n",
    "# -----------------------------\n",
    "# Assign a weighted score\n",
    "df_last_buys = df_last.filter(F.col(\"Action\") == \"Buy\").cache()\n",
    "\n",
    "df_last_buys = df_last_buys.withColumn(\n",
    "    \"BuyScore\",\n",
    "    # Weighted combination of key signals\n",
    "    F.col(\"ActionConfidence\") * 0.3 +\n",
    "    F.col(\"SignalStrengthHybrid\") * 0.2 +\n",
    "    F.col(\"BullishStrengthHybrid\") * 0.2 +\n",
    "    F.col(\"UpTrend_Return\").cast(\"integer\") * 0.1\n",
    ")\n",
    "'''\n",
    "+\n",
    "    F.col(\"UpTrend_Return\").cast(\"integer\") * 0.1\n",
    "    '''\n",
    "df_last_buys = df_last_buys.withColumn(\n",
    "    \"BuyScore\",\n",
    "    F.col(\"BuyScore\") * F.when(F.col(\"Volatility\") > 0.05, 0.8).otherwise(1.0)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Define ranking window per timeframe\n",
    "# -----------------------------\n",
    "'''\n",
    "w_tf = Window.partitionBy(\"TimeFrame\").orderBy(\n",
    "    F.desc(\"ActionConfidence\"),\n",
    "    F.desc(\"Return\")\n",
    ")\n",
    "'''\n",
    "w_tf = Window.partitionBy(\"TimeFrame\").orderBy(\n",
    "    F.desc(\"BuyScore\")\n",
    ")\n",
    "top_n = 100\n",
    "\n",
    "# -----------------------------\n",
    "# Rank and select top N Buy companies per timeframe\n",
    "# -----------------------------\n",
    "df_ranked_last_top = (\n",
    "    df_last_buys\n",
    "    .withColumn(\"BuyRank\", F.row_number().over(w_tf))  # use rank() if you want ties\n",
    "    .filter(F.col(\"BuyRank\") <= top_n)\n",
    "    .orderBy(F.col(\"ActionConfidence\").desc(), F.col(\"BuyRank\").asc())\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Extract top companies per timeframe\n",
    "# -----------------------------\n",
    "top_companies = df_ranked_last_top.select(\"CompanyId\", \"TimeFrame\").distinct().cache()\n",
    "# -----------------------------\n",
    "# Filter original last-row DF and full historical DF to include only top Buy companies\n",
    "# Cache large DataFrames once\n",
    "# -----------------------------\n",
    "df_ranked_last_topN = df_last.join(\n",
    "    broadcast(df_ranked_last_top.select(\"CompanyId\",\"TimeFrame\",\"BuyRank\")),\n",
    "    on=[\"CompanyId\",\"TimeFrame\"],\n",
    "    how=\"inner\"\n",
    ").cache()\n",
    "\n",
    "df_ranked_all_topN = df_all.join(\n",
    "    broadcast(df_ranked_last_top.select(\"CompanyId\",\"TimeFrame\",\"BuyRank\")),\n",
    "    on=[\"CompanyId\",\"TimeFrame\"],\n",
    "    how=\"inner\"\n",
    ").cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# -----------------------------\n",
    "# List of timeframes\n",
    "# -----------------------------\n",
    "timeframes = [\"Short\", \"Swing\", \"Long\", \"Daily\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Dictionaries to store per-timeframe DataFrames\n",
    "# -----------------------------\n",
    "timeframe_dfs = {}\n",
    "timeframe_dfs_all = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Efficient per-timeframe splitting\n",
    "# -----------------------------\n",
    "for tf in timeframes:\n",
    "    # Last-row top N for this timeframe\n",
    "    timeframe_dfs[tf] = df_ranked_last_topN.filter(F.col(\"TimeFrame\") == tf)\n",
    "    \n",
    "    # Full historical top N for this timeframe\n",
    "    timeframe_dfs_all[tf] = df_ranked_all_topN.filter(F.col(\"TimeFrame\") == tf)\n",
    "\n",
    "\n",
    "print(f\"✅ Stage 1 completed: Top {top_n} candidates selected per timeframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce54f984-8682-433a-a57d-536a3000b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def debug_spark_df(df: DataFrame, col_check=None, n=5, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Helper to inspect a PySpark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "        col_check: optional column name to count nulls\n",
    "        n: number of rows to show\n",
    "        name: optional label for printing\n",
    "    \"\"\"\n",
    "    n_rows = df.count()\n",
    "    n_cols = len(df.columns)\n",
    "    print(f\"--- Debug {name} ---\")\n",
    "    print(f\"Shape: ({n_rows}, {n_cols})\")\n",
    "    \n",
    "    if col_check and col_check in df.columns:\n",
    "        null_count = df.filter(F.col(col_check).isNull()).count()\n",
    "        print(f\"Column '{col_check}' nulls: {null_count}\")\n",
    "    \n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    print(f\"Sample rows:\")\n",
    "    df.show(n)\n",
    "    print(\"--- End Debug ---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e327ac3-3134-41e5-9b64-9a014125a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Hybrid scoring function\n",
    "# -----------------------------\n",
    "def hybrid_score(metrics, w_dir=1.0, w_rmse=0.5, w_mape=0.5):\n",
    "    \"\"\"\n",
    "    Combine multiple metrics: higher DirectionAcc, lower RMSE & MAPE\n",
    "    \"\"\"\n",
    "    return w_dir*metrics[\"DirectionAcc\"] - w_rmse*metrics[\"RMSE\"] - w_mape*metrics[\"MAPE\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Pick best model using hybrid score\n",
    "# -----------------------------\n",
    "def select_best_model(metrics_dict):\n",
    "    scores = {name: hybrid_score(metrics_dict[name]) for name in metrics_dict}\n",
    "    best_name = max(scores, key=scores.get)\n",
    "    return best_name, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad50ebd6-14de-44df-9010-c0eb0de6b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 2 - Timeframe: Short ===\n",
      "=== Phase 2 - Timeframe: Swing ===\n",
      "=== Phase 2 - Timeframe: Long ===\n",
      "=== Phase 2 - Timeframe: Daily ===\n",
      "✅ Stage 2 completed: Top 25 candidates selected per timeframe\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "target_stage2 = \"TomorrowClose\"\n",
    "epsilon = 1e-8\n",
    "all_stage2_predictions = []\n",
    "\n",
    "# -----------------------------\n",
    "# Loop over timeframes (Pandas)\n",
    "# -----------------------------\n",
    "for tf, sdf_tf in timeframe_dfs_all.items():  \n",
    "    # Convert Spark DF to Pandas once\n",
    "    df_tf = sdf_tf.toPandas()\n",
    "    \n",
    "    companies = df_tf[\"CompanyId\"].unique()\n",
    "    print(f\"=== Phase 2 - Timeframe: {tf} ===\")\n",
    "    \n",
    "    for cid in companies:\n",
    "        # Filter by company\n",
    "        df_c = df_tf[df_tf[\"CompanyId\"] == cid].copy()\n",
    "        if df_c.empty:\n",
    "            continue\n",
    "        \n",
    "        # 1️⃣ Identify training rows and future rows\n",
    "        train_df = df_c[df_c[target_stage2].notna()].copy()\n",
    "        future_df = df_c[df_c[target_stage2].isna()].copy()\n",
    "        if train_df.empty or future_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # 2️⃣ Log-transform OHLC columns to stabilize variance\n",
    "        for col in [\"Open\", \"High\", \"Low\", \"Close\"]:\n",
    "            df_c[f\"log_{col}\"] = np.log(df_c[col].replace(0, epsilon))\n",
    "        \n",
    "        # 3️⃣ Select numeric & boolean features (excluding target)\n",
    "        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.difference([target_stage2]).tolist()\n",
    "        bool_cols = train_df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "        all_features = numeric_cols + bool_cols\n",
    "        \n",
    "        # 4️⃣ Feature correlation with target\n",
    "        corr = train_df[all_features + [target_stage2]].corr()[target_stage2].abs()\n",
    "        threshold = 0.03\n",
    "        good_features = corr[corr >= threshold].drop(target_stage2).index.tolist()\n",
    "        if not good_features:\n",
    "            continue\n",
    "        \n",
    "        # 5️⃣ Prepare train dataset\n",
    "        X_train = train_df[good_features].fillna(0)\n",
    "        y_train = train_df[target_stage2]\n",
    "        X_future = future_df[good_features].fillna(0)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # 6️⃣ Train models\n",
    "        # -----------------------------\n",
    "        models = {\n",
    "            \"Linear\": LinearRegression(),\n",
    "            \"Lasso\": Lasso(alpha=0.01),\n",
    "            \"Ridge\": Ridge(alpha=1.0, solver=\"svd\"),\n",
    "            \"XGBoost\": XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, verbosity=0)\n",
    "        }\n",
    "        metrics_dict = {}\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            pred_train = model.predict(X_train)\n",
    "            rmse = mean_squared_error(y_train, pred_train, squared=False)\n",
    "            mae = mean_absolute_error(y_train, pred_train)\n",
    "            mape = np.mean(np.abs((y_train - pred_train) / (y_train + epsilon)))\n",
    "            direction = np.mean(np.sign(pred_train[1:] - pred_train[:-1]) == np.sign(y_train.values[1:] - y_train.values[:-1]))\n",
    "            r2 = r2_score(y_train, pred_train)\n",
    "            k = X_train.shape[1]\n",
    "            n = len(y_train)\n",
    "            adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1)) if n - k - 1 != 0 else 0\n",
    "            metrics_dict[name] = {\"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"DirectionAcc\": direction, \"R2\": r2, \"AdjR2\": adj_r2}\n",
    "        \n",
    "        # 7️⃣ Pick best model\n",
    "        #best_name = max(metrics_dict, key=lambda m: hybrid_score(metrics_dict[m]))\n",
    "        #best_model = models[best_name]\n",
    "        \n",
    "        # Pick best model with hybrid score\n",
    "        best_name, scores = select_best_model(metrics_dict)\n",
    "        best_model = models[best_name]\n",
    "\n",
    "        \n",
    "        # 8️⃣ Predict future rows\n",
    "        for name, model in models.items():\n",
    "            future_df[f\"Pred_{name}\"] = model.predict(X_future)\n",
    "        \n",
    "        # Weighted ensemble (inverse RMSE)\n",
    "        #total_inv = sum(1 / metrics_dict[m][\"RMSE\"] for m in metrics_dict)\n",
    "        total_inv = sum(1 / (metrics_dict[m][\"RMSE\"] + epsilon) for m in metrics_dict)\n",
    "        weights = {m: (1 / metrics_dict[m][\"RMSE\"]) / total_inv for m in metrics_dict}\n",
    "        future_df[\"Pred_Sklearn\"] = sum(future_df[f\"Pred_{m}\"] * w for m, w in weights.items())\n",
    "        \n",
    "        # Predicted return\n",
    "        if \"Close\" in future_df.columns:\n",
    "            future_df[\"PredictedReturn_Sklearn\"] = (future_df[\"Pred_Sklearn\"] - future_df[\"Close\"]) / future_df[\"Close\"]\n",
    "             # Predicted return for each individual model\n",
    "            for name in models.keys():\n",
    "                future_df[f\"PredictedReturn_{name}\"] = (future_df[f\"Pred_{name}\"] - future_df[\"Close\"]) / future_df[\"Close\"]\n",
    "\n",
    "            # Maximum predicted return across all models\n",
    "            return_cols = [f\"PredictedReturn_{name}\" for name in models.keys()]\n",
    "            future_df[\"MaxPredictedReturn\"] = future_df[return_cols].max(axis=1)       \n",
    "       \n",
    "        \n",
    "        # Add identifiers & best model info\n",
    "        future_df[\"TimeFrame\"] = tf\n",
    "        future_df[\"CompanyId\"] = cid\n",
    "        future_df[\"BestModel\"] = best_name\n",
    "        #future_df[\"BestModel_RMSE\"] = metrics_dict[best_name][\"RMSE\"]\n",
    "        #future_df[\"BestModel_MAPE\"] = metrics_dict[best_name][\"MAPE\"]\n",
    "        #future_df[\"BestModel_DirAcc\"] = metrics_dict[best_name][\"DirectionAcc\"]\n",
    "        # Save all metrics for the best model\n",
    "        for metric_name, value in metrics_dict[best_name].items():\n",
    "            col_name = f\"BestModel_{metric_name}\"   # e.g., BestModel_RMSE, BestModel_MAPE\n",
    "            future_df[col_name] = value\n",
    "        # save all scores:\n",
    "        for model_name, model_metrics in metrics_dict.items():\n",
    "            for metric_name, value in model_metrics.items():\n",
    "                # Create column names like \"XGBoost_RMSE\", \"Linear_MAPE\"\n",
    "                col_name = f\"{model_name}_{metric_name}\"\n",
    "                future_df[col_name] = value\n",
    "        '''\n",
    "        # Raise error if best_name or metrics are missing/null/blank\n",
    "        if (not best_name) or future_df[[\"BestModel_RMSE\",\"BestModel_MAPE\",\"BestModel_DirAcc\"]].isnull().any().any():\n",
    "            raise ValueError(\n",
    "                f\"Missing or null metrics for company {cid}, timeframe {tf}, best_name={best_name} | \"\n",
    "                f\"Metrics: {metrics}\"\n",
    "            )\n",
    "        if future_df.empty or len(good_features) == 0:\n",
    "            print(f\"Skipping {cid}-{tf} | future rows: {len(future_df)}, good features: {len(good_features)}\")\n",
    "            breakpoint()  # or raise ValueError to stop\n",
    "        '''\n",
    "        all_stage2_predictions.append(future_df)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "top_n_phase2 = 25  # number of top candidates per timeframe\n",
    "# -----------------------------\n",
    "# Combine all Stage 2 predictions into a single Pandas DF\n",
    "# -----------------------------\n",
    "if all_stage2_predictions:\n",
    "    stage2_df = pd.concat(all_stage2_predictions, ignore_index=True)\n",
    "else:\n",
    "    raise ValueError(\"No Stage 2 predictions generated!\")\n",
    "# -----------------------------\n",
    "# Convert Stage 2 Pandas DF → Spark DF\n",
    "# -----------------------------\n",
    "spark_stage2_all = (\n",
    "    spark.createDataFrame(stage2_df)\n",
    "    .withColumn(\"CompanyId\", F.col(\"CompanyId\").cast(\"bigint\"))\n",
    "    .withColumn(\"TimeFrame\", F.trim(F.col(\"TimeFrame\")))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Pick best prediction per CompanyId + TimeFrame\n",
    "# -----------------------------\n",
    "window_comp = Window.partitionBy(\"CompanyId\", \"TimeFrame\").orderBy(F.desc(\"PredictedReturn_Sklearn\"))\n",
    "\n",
    "spark_stage2_best = (\n",
    "    spark_stage2_all\n",
    "    .withColumn(\"row_num\", F.row_number().over(window_comp))\n",
    "    .filter(F.col(\"row_num\") == 1)\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Assign Phase2 rank per timeframe for all rows\n",
    "# -----------------------------\n",
    "window_tf = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"PredictedReturn_Sklearn\"))\n",
    "\n",
    "spark_stage2_ranked = (\n",
    "    spark_stage2_best\n",
    "    .withColumn(\"Phase2_Rank\", F.row_number().over(window_tf))\n",
    ")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "phase2_top_dfs = {}\n",
    "\n",
    "# Columns from Stage2 DF we care about\n",
    "'''\n",
    "pred_cols = [\n",
    "    \"Pred_Linear\", \"Pred_Lasso\", \"Pred_Ridge\", \"Pred_XGBoost\",\n",
    "    \"PredictedReturn_Linear\", \"PredictedReturn_Lasso\", \"PredictedReturn_Ridge\", \"PredictedReturn_XGBoost\",\n",
    "    \"Pred_Sklearn\", \"PredictedReturn_Sklearn\",\n",
    "    \"BestModel\", \"BestModel_RMSE\", \"BestModel_MAPE\",\n",
    "    \"BestModel_DirAcc\", \"Phase2_Rank\"\n",
    "]\n",
    "'''\n",
    "\n",
    "# Start with columns that are fixed\n",
    "pred_cols = [\"BestModel\"]\n",
    "# Optionally, also add BestModel metrics and all scoring columns as before\n",
    "best_model_metrics = [\"RMSE\", \"MAE\", \"MAPE\", \"DirectionAcc\", \"R2\", \"AdjR2\"]\n",
    "for metric in best_model_metrics:\n",
    "    pred_cols.append(f\"BestModel_{metric}\")\n",
    "\n",
    "    \n",
    "pred_cols.extend([\"Pred_Sklearn\", \"PredictedReturn_Sklearn\"])\n",
    "\n",
    "\n",
    "# Add all metrics for every model dynamically from metrics_dict\n",
    "for model_name, model_metrics in metrics_dict.items():\n",
    "    # Optional: Pred_ columns for each model\n",
    "    pred_cols.append(f\"Pred_{model_name}\")\n",
    "    pred_cols.append(f\"PredictedReturn_{model_name}\")\n",
    "    \n",
    "    # Add all scoring metrics\n",
    "    for metric_name in model_metrics.keys():\n",
    "        pred_cols.append(f\"{model_name}_{metric_name}\")\n",
    "    \n",
    "    \n",
    "pred_cols.extend([\"MaxPredictedReturn\",\"Phase2_Rank\"])\n",
    "\n",
    "\n",
    "\n",
    "for tf, sdf_tf in timeframe_dfs_all.items():\n",
    "    # Clean historical DF keys\n",
    "    sdf_tf_clean = (\n",
    "        sdf_tf\n",
    "        .withColumn(\"CompanyId\", F.col(\"CompanyId\").cast(\"bigint\"))\n",
    "        .withColumn(\"TimeFrame\", F.trim(F.col(\"TimeFrame\")))\n",
    "    )\n",
    "\n",
    "    # Select only the columns from Stage2 we want\n",
    "    sdf_stage2 = spark_stage2_ranked.select(\n",
    "        [\"CompanyId\", \"TimeFrame\"] + pred_cols\n",
    "    )\n",
    "\n",
    "    # Left join\n",
    "    sdf_enriched = sdf_tf_clean.join(\n",
    "        F.broadcast(sdf_stage2),\n",
    "        on=[\"CompanyId\", \"TimeFrame\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    phase2_top_dfs[tf] = sdf_enriched\n",
    "\n",
    "# Top-N filter per timeframe\n",
    "phase2_topN_dfs = {}\n",
    "for tf, sdf in phase2_top_dfs.items():\n",
    "    sdf_topN = sdf.filter(F.col(\"Phase2_Rank\") <= top_n_phase2)\n",
    "    phase2_topN_dfs[tf] = sdf_topN\n",
    "\n",
    "\n",
    "print(f\"✅ Stage 2 completed: Top {top_n_phase2} candidates selected per timeframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3025a84-f362-4bf9-9e02-b6c039a57cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def infer_season_length(ts, max_lag=30, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Infer seasonal period `m` from autocorrelation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts : pandas.Series\n",
    "        Time series values\n",
    "    max_lag : int\n",
    "        Maximum lag to inspect for autocorrelation\n",
    "    threshold : float\n",
    "        Minimum autocorrelation to consider a peak as seasonal\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    m : int\n",
    "        Estimated seasonal period\n",
    "    \"\"\"\n",
    "    ts = ts.dropna()\n",
    "    if len(ts) < 2:\n",
    "        return 1  # not enough data to infer\n",
    "    \n",
    "    acf_vals = acf(ts, nlags=max_lag, fft=True)\n",
    "    \n",
    "    # Ignore lag 0\n",
    "    acf_vals[0] = 0\n",
    "    \n",
    "    # Find first lag where autocorrelation exceeds threshold\n",
    "    peaks = np.where(acf_vals > threshold)[0]\n",
    "    \n",
    "    if len(peaks) == 0:\n",
    "        return 1  # no strong seasonality detected\n",
    "    \n",
    "    # Choose the first peak as seasonal period\n",
    "    m = int(peaks[0])\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de10e1eb-f3e9-4bc3-8fb4-2a0e38da348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 3 - Timeframe: Short ===\n",
      "=== Phase 3 - Timeframe: Swing ===\n",
      "=== Phase 3 - Timeframe: Long ===\n",
      "=== Phase 3 - Timeframe: Daily ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/21 22:22:17 WARN TaskSetManager: Stage 1058 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/21 22:22:27 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`bsf`.`final_candidates_enriched` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/09/21 22:22:27 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/09/21 22:22:28 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/09/21 22:22:28 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/21 22:22:28 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/21 22:22:28 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/21 22:22:33 WARN TaskSetManager: Stage 1071 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/21 22:22:42 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`bsf`.`final_candidates` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/09/21 22:22:43 WARN TaskSetManager: Stage 1087 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/21 22:22:45 WARN TaskSetManager: Stage 1093 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/21 22:22:48 WARN TaskSetManager: Stage 1099 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short: Final top N = 5266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/21 22:22:51 WARN TaskSetManager: Stage 1105 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swing: Final top N = 6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/21 22:22:53 WARN TaskSetManager: Stage 1111 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long: Final top N = 6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/21 22:22:55 WARN TaskSetManager: Stage 1117 contains a task of very large size (1245 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily: Final top N = 6299\n",
      "✅ Stage 3 completed: Latest rows per company + top 10 candidates selected per timeframe\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "top_n_final = 10\n",
    "sarimax_order = (1,0,0)\n",
    "sarimax_seasonal_order = (0,0,0,0)\n",
    "epsilon = 1e-6\n",
    "ml_weight = 0.6\n",
    "sarimax_weight = 0.4\n",
    "\n",
    "forecast_steps_map = {\n",
    "    \"Daily\": 1,\n",
    "    \"Short\": 3,\n",
    "    \"Swing\": 5,\n",
    "    \"Long\": 10\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "import pmdarima as pm\n",
    "\n",
    "# -----------------------------\n",
    "# Phase 3: Loop over companies per timeframe\n",
    "# -----------------------------\n",
    "phase3_results = []\n",
    "\n",
    "for tf, sdf_tf in phase2_topN_dfs.items():\n",
    "    print(f\"=== Phase 3 - Timeframe: {tf} ===\")\n",
    "\n",
    "    # Collect companies\n",
    "    companies = sdf_tf.select(\"CompanyId\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    forecast_horizon = forecast_steps_map.get(tf, 1)\n",
    "    for cid in companies:\n",
    "        # Filter Spark DF once, convert to Pandas\n",
    "        df_c = sdf_tf.filter(F.col(\"CompanyId\") == cid).orderBy(\"StockDate\").toPandas()\n",
    "        if df_c.empty:\n",
    "            continue\n",
    "        \n",
    "        # -----------------------------\n",
    "        # SARIMAX Forecast\n",
    "        # -----------------------------\n",
    "        y = df_c[\"Close\"].replace(0, epsilon)\n",
    "\n",
    "        last_close = y.iloc[-1]\n",
    "\n",
    "        try:\n",
    "            #auto_model = fit_auto_arima(y, seasonal=True, m=7)  # m=7 for weekly seasonality on daily data\n",
    "            m = infer_season_length(y, max_lag=30, threshold=0.3)\n",
    "            '''\n",
    "            auto_model = pm.auto_arima(\n",
    "                y,\n",
    "                start_p=0, start_q=0,  \n",
    "                max_p=2, max_q=2,      # keep small since your system has 6GB RAM (3,3)\n",
    "                d=None,                # let auto_arima decide\n",
    "                start_P=0, start_Q=0,  \n",
    "                max_P=1, max_Q=1,      # was (2,2)\n",
    "                D=None,\n",
    "                m=m,                   # season length (e.g. 7 = weekly seasonality for daily data)\n",
    "                seasonal=True,\n",
    "                stepwise=True,         # faster stepwise search\n",
    "                suppress_warnings=True,\n",
    "                error_action=\"ignore\", # continue even if a model fails\n",
    "                trace=False             # show models it tries (True)\n",
    "            )\n",
    "            '''\n",
    "\n",
    "\n",
    "            \n",
    "            auto_model = pm.auto_arima(\n",
    "                y,\n",
    "                start_p=0, start_q=0,\n",
    "                max_p=2, max_q=2,      # smaller max orders → fewer models\n",
    "                d=None,                # let auto_arima decide\n",
    "                start_P=0, start_Q=0,\n",
    "                max_P=1, max_Q=1,      # smaller seasonal orders\n",
    "                D=None,\n",
    "                m=m,                   # seasonal length\n",
    "                seasonal=True,\n",
    "                stepwise=True,         # enable stepwise search (faster than full search)\n",
    "                max_order=3,           # sum of p+q+P+Q ≤ 3 → reduces combinations\n",
    "                max_d=2,               # restrict differencing search\n",
    "                max_D=1,               # restrict seasonal differencing\n",
    "                n_jobs=1,              # parallel jobs if supported\n",
    "                suppress_warnings=True,\n",
    "                error_action=\"ignore\",\n",
    "                trace=False            # disable verbose output\n",
    "            )\n",
    "          \n",
    "            if auto_model is not None:\n",
    "                # Extract best orders found by auto_arima\n",
    "                sarimax_order = auto_model.order\n",
    "                sarimax_seasonal_order = auto_model.seasonal_order\n",
    "                # Log the AIC\n",
    "                #print(f\"Best auto_arima model: {auto_model.summary()}\")\n",
    "                #print(f\"AIC: {auto_model.aic()}\")\n",
    "                aic=auto_model.aic()\n",
    "                mltype=\"automl\"\n",
    "            else:\n",
    "                sarimax_order = (1,0,0)\n",
    "                sarimax_seasonal_order = (0,0,0,0)\n",
    "                aic=0\n",
    "                mltype=\"sarimax\"\n",
    "\n",
    "                \n",
    "            model = SARIMAX(y,\n",
    "                            order=sarimax_order,\n",
    "                            seasonal_order=sarimax_seasonal_order,\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False)\n",
    "            sarimax_res = model.fit(disp=False)\n",
    "            forecast = sarimax_res.get_forecast(steps=forecast_horizon)\n",
    "            pred_price = forecast.predicted_mean.iloc[-1] # iloc[-1] gets the last Value in the prediction results\n",
    "            last_close = y.iloc[-1]\n",
    "            sarimax_return = (pred_price - last_close) / last_close\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ SARIMAX failed for {cid}-{tf}: {e}\")\n",
    "            pred_price = last_close\n",
    "            sarimax_return = 0.0\n",
    "        \n",
    "        # -----------------------------\n",
    "        # ML Prediction from existing Phase 2 columns\n",
    "        # -----------------------------\n",
    "        ml_return = df_c[\"MaxPredictedReturn\"].iloc[0] if \"MaxPredictedReturn\" in df_c.columns else 0.0\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Weighted score\n",
    "        # -----------------------------\n",
    "        weighted_score = ml_weight * ml_return + sarimax_weight * sarimax_return\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Store enriched data\n",
    "        # -----------------------------\n",
    "        df_c[\"SARIMAX_PredictedClose\"] = pred_price\n",
    "        df_c[\"SARIMAX_PredictedReturn\"] = sarimax_return\n",
    "        df_c[\"WeightedScore\"] = weighted_score\n",
    "        df_c[\"AIC\"] = aic\n",
    "        df_c[\"MlType\"] = mltype\n",
    "        phase3_results.append(df_c)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Combine all companies/timeframes\n",
    "# -----------------------------\n",
    "df_phase3_full = pd.concat(phase3_results, ignore_index=True)\n",
    "df_phase3_spark = spark.createDataFrame(df_phase3_full)\n",
    "\n",
    "# -----------------------------\n",
    "# Enrich with company info\n",
    "# -----------------------------\n",
    "sdf_company = spark.table(\"bsf.company\").select(\"CompanyId\",\"TradingSymbol\",\"Name\")\n",
    "df_phase3_enriched = df_phase3_spark.join(F.broadcast(sdf_company), on=\"CompanyId\", how=\"left\")\n",
    "\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Generate a timestamp string\n",
    "# -----------------------------\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "df_phase3_enriched = df_phase3_enriched.withColumn(\"RunTimestamp\", F.lit(timestamp))\n",
    "cols = [\"RunTimestamp\"] + [c for c in df_phase3_enriched.columns if c != \"RunTimestamp\"]\n",
    "df_phase3_enriched = df_phase3_enriched.select(cols)\n",
    "\n",
    "\n",
    "# 3️⃣ Save to Delta\n",
    "db='bsf'\n",
    "table='final_candidates_enriched'\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {db}.{table}\")\n",
    "# Correct save\n",
    "df_phase3_enriched.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{db}.{table}\")\n",
    "\n",
    "# 1️⃣ Keep only the latest row per company + timeframe\n",
    "window_last = Window.partitionBy(\"CompanyId\", \"TimeFrame\").orderBy(F.desc(\"StockDate\"))\n",
    "df_latest_per_company = (\n",
    "    df_phase3_enriched\n",
    "    .withColumn(\"rn\", F.row_number().over(window_last))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# 2️⃣ Rank companies by WeightedScore per timeframe\n",
    "window_tf = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"WeightedScore\"))\n",
    "df_topN_companies = (\n",
    "    df_latest_per_company\n",
    "    .withColumn(\"Phase3_Rank\", F.row_number().over(window_tf))\n",
    "    .filter(F.col(\"Phase3_Rank\") <= top_n_final)\n",
    "    .drop(\"Phase3_Rank\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3️⃣ Save to Delta\n",
    "db='bsf'\n",
    "table='final_candidates'\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {db}.{table}\")\n",
    "\n",
    "df_topN_companies.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{db}.{table}\")\n",
    "\n",
    "# csv for review\n",
    "output_path = spark.conf.get(\"spark.sql.filesource.path\", \"/srv/lakehouse/nond2rd\")\n",
    "df_topN_companies.toPandas().to_csv(\n",
    "    f\"{output_path}/{table}.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Create dict by timeframe for plotting\n",
    "# -----------------------------\n",
    "phase3_top_dfs = {\n",
    "    tf: df_phase3_enriched.filter(F.col(\"TimeFrame\") == tf)\n",
    "    for tf in df_phase3_enriched.select(\"TimeFrame\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "}\n",
    "# -----------------------------\n",
    "# Optional: show counts\n",
    "# -----------------------------\n",
    "for tf in timeframes:\n",
    "    print(f\"{tf}: Final top N = {phase3_top_dfs[tf].count()}\")\n",
    "\n",
    "print(f\"✅ Stage 3 completed: Latest rows per company + top {top_n_final} candidates selected per timeframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc18ae7-0c4f-4cce-8242-62ef11b89196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage 4 completed: Confirm write to delta table!!\n"
     ]
    }
   ],
   "source": [
    "# Read Delta table\n",
    "df_latest_spark = spark.table(f\"{db}.{table}\")\n",
    "\n",
    "# Convert to Pandas\n",
    "df_latest_pdf = df_latest_spark.toPandas()\n",
    "\n",
    "# Option 1: Show all records (careful if large)\n",
    "#print(df_latest_pdf)\n",
    "\n",
    "# Option 2: Show grouped by timeframe\n",
    "#for tf, group in df_latest_pdf.groupby(\"TimeFrame\"):\n",
    "#    print(f\"\\n=== TimeFrame: {tf} ===\")\n",
    "#    display(group.sort_values(\"WeightedScore\", ascending=False))\n",
    "print(f\"✅ Stage 4 completed: Confirm write to delta table!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce68a564-548a-40fd-aca3-e51e2333777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9c85a-d5e4-4dc3-a0be-cccf40607ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
