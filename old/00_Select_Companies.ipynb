{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade0a488-76fc-4a4e-a87c-2c333d1f6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_candidates_analysis' log_level=WARN (effective=WARN), progress=False\n",
      "+---------+-------------------------+-----------+\n",
      "|namespace|tableName                |isTemporary|\n",
      "+---------+-------------------------+-----------+\n",
      "|bsf      |company                  |false      |\n",
      "|bsf      |companystockhistory      |false      |\n",
      "|bsf      |daily_signals            |false      |\n",
      "|bsf      |daily_signals_allcol     |false      |\n",
      "|bsf      |daily_signals_last       |false      |\n",
      "|bsf      |daily_signals_last_allcol|false      |\n",
      "+---------+-------------------------+-----------+\n",
      "\n",
      "Table: bsf.company | Rows: 30949\n",
      "Table: bsf.companystockhistory | Rows: 458254\n",
      "Table: bsf.daily_signals | Rows: 3713208\n",
      "Table: bsf.daily_signals_allcol | Rows: 3713208\n",
      "Table: bsf.daily_signals_last | Rows: 14784\n",
      "Table: bsf.daily_signals_last_allcol | Rows: 14784\n",
      "+------+-------+\n",
      "|Action|count  |\n",
      "+------+-------+\n",
      "|Hold  |2638632|\n",
      "|Buy   |596065 |\n",
      "|Sell  |478511 |\n",
      "+------+-------+\n",
      "\n",
      "+---------+------+------+\n",
      "|TimeFrame|Action|count |\n",
      "+---------+------+------+\n",
      "|Daily    |Hold  |652299|\n",
      "|Daily    |Buy   |155435|\n",
      "|Daily    |Sell  |120568|\n",
      "|Long     |Hold  |679956|\n",
      "|Long     |Buy   |137572|\n",
      "|Long     |Sell  |110774|\n",
      "|Short    |Hold  |654379|\n",
      "|Short    |Buy   |150710|\n",
      "|Short    |Sell  |123213|\n",
      "|Swing    |Hold  |651998|\n",
      "|Swing    |Buy   |152348|\n",
      "|Swing    |Sell  |123956|\n",
      "+---------+------+------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/13 09:12:01 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyId</th>\n",
       "      <th>TimeFrame</th>\n",
       "      <th>StockDate</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Doji</th>\n",
       "      <th>Hammer</th>\n",
       "      <th>HangingMan</th>\n",
       "      <th>InvertedHammer</th>\n",
       "      <th>ShootingStar</th>\n",
       "      <th>BullishMarubozu</th>\n",
       "      <th>BearishMarubozu</th>\n",
       "      <th>SuspiciousCandle</th>\n",
       "      <th>BullishEngulfing</th>\n",
       "      <th>BearishEngulfing</th>\n",
       "      <th>BullishHarami</th>\n",
       "      <th>BearishHarami</th>\n",
       "      <th>HaramiCross</th>\n",
       "      <th>PiercingLine</th>\n",
       "      <th>DarkCloudCover</th>\n",
       "      <th>MorningStar</th>\n",
       "      <th>EveningStar</th>\n",
       "      <th>ThreeWhiteSoldiers</th>\n",
       "      <th>ThreeBlackCrows</th>\n",
       "      <th>TweezerTop</th>\n",
       "      <th>TweezerBottom</th>\n",
       "      <th>InsideBar</th>\n",
       "      <th>OutsideBar</th>\n",
       "      <th>NearHigh</th>\n",
       "      <th>NearLow</th>\n",
       "      <th>PatternCount</th>\n",
       "      <th>PatternType</th>\n",
       "      <th>MA</th>\n",
       "      <th>MA_slope</th>\n",
       "      <th>UpTrend_MA</th>\n",
       "      <th>DownTrend_MA</th>\n",
       "      <th>RecentReturn</th>\n",
       "      <th>UpTrend_Return</th>\n",
       "      <th>DownTrend_Return</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>LowVolatility</th>\n",
       "      <th>HighVolatility</th>\n",
       "      <th>ROC</th>\n",
       "      <th>MomentumUp</th>\n",
       "      <th>MomentumDown</th>\n",
       "      <th>ConfirmedUpTrend</th>\n",
       "      <th>ConfirmedDownTrend</th>\n",
       "      <th>ValidHammer</th>\n",
       "      <th>ValidBullishEngulfing</th>\n",
       "      <th>ValidPiercingLine</th>\n",
       "      <th>ValidMorningStar</th>\n",
       "      <th>ValidThreeWhiteSoldiers</th>\n",
       "      <th>ValidBullishMarubozu</th>\n",
       "      <th>ValidTweezerBottom</th>\n",
       "      <th>ValidShootingStar</th>\n",
       "      <th>ValidBearishEngulfing</th>\n",
       "      <th>ValidDarkCloud</th>\n",
       "      <th>ValidEveningStar</th>\n",
       "      <th>ValidThreeBlackCrows</th>\n",
       "      <th>ValidBearishMarubozu</th>\n",
       "      <th>ValidTweezerTop</th>\n",
       "      <th>ValidHaramiCross</th>\n",
       "      <th>ValidBullishHarami</th>\n",
       "      <th>ValidBearishHarami</th>\n",
       "      <th>ValidInsideBar</th>\n",
       "      <th>ValidOutsideBar</th>\n",
       "      <th>TomorrowClose</th>\n",
       "      <th>TomorrowReturn</th>\n",
       "      <th>Return</th>\n",
       "      <th>AvgReturn</th>\n",
       "      <th>MomentumZ</th>\n",
       "      <th>BuyThresh</th>\n",
       "      <th>SellThresh</th>\n",
       "      <th>MomentumAction</th>\n",
       "      <th>BullScore</th>\n",
       "      <th>BearScore</th>\n",
       "      <th>PatternScore</th>\n",
       "      <th>PatternScoreNorm</th>\n",
       "      <th>PatternAction</th>\n",
       "      <th>CandleAction</th>\n",
       "      <th>CandidateAction</th>\n",
       "      <th>Action</th>\n",
       "      <th>TomorrowAction</th>\n",
       "      <th>TomorrowActionSource</th>\n",
       "      <th>SignalStrengthHybrid</th>\n",
       "      <th>ActionConfidence</th>\n",
       "      <th>BullishStrengthHybrid</th>\n",
       "      <th>BearishStrengthHybrid</th>\n",
       "      <th>SignalDuration</th>\n",
       "      <th>ValidAction</th>\n",
       "      <th>HasValidSignal</th>\n",
       "      <th>SignalStrength</th>\n",
       "      <th>BatchId</th>\n",
       "      <th>IngestedAt</th>\n",
       "      <th>BuyCount</th>\n",
       "      <th>SellCount</th>\n",
       "      <th>HoldCount</th>\n",
       "      <th>Return</th>\n",
       "      <th>BuyRank</th>\n",
       "      <th>SellRank</th>\n",
       "      <th>HoldRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100325</td>\n",
       "      <td>Daily</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>-0.036035</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.185284</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.020218</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>0.031380</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>-0.033779</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>162.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>100325_20250912_202542</td>\n",
       "      <td>20250912_202542</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>462</td>\n",
       "      <td>1853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100325</td>\n",
       "      <td>Daily</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>-0.036035</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.185284</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.020218</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>0.031380</td>\n",
       "      <td>0.019497</td>\n",
       "      <td>-0.036794</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>0.348937</td>\n",
       "      <td>162.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>100325_20250911_183349</td>\n",
       "      <td>20250911_183349</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>462</td>\n",
       "      <td>1853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34813</td>\n",
       "      <td>Long</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>BullishMarubozu</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.189737</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>-0.316228</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>-0.070165</td>\n",
       "      <td>Sell</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>65.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>34813_20250911_174811</td>\n",
       "      <td>20250911_174811</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>403</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34813</td>\n",
       "      <td>Long</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>BullishMarubozu</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.189737</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>-0.316228</td>\n",
       "      <td>-0.001160</td>\n",
       "      <td>-0.070403</td>\n",
       "      <td>Sell</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>0.199524</td>\n",
       "      <td>65.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>34813_20250912_182720</td>\n",
       "      <td>20250912_182720</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>403</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67893</td>\n",
       "      <td>Short</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>BullishEngulfing</td>\n",
       "      <td>0.071250</td>\n",
       "      <td>0.054774</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.035101</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.011260</td>\n",
       "      <td>2.844670</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>-0.004445</td>\n",
       "      <td>Buy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>33.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>67893_20250911_164057</td>\n",
       "      <td>20250911_164057</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1</td>\n",
       "      <td>461</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67893</td>\n",
       "      <td>Short</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>0.07500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>BullishEngulfing</td>\n",
       "      <td>0.071250</td>\n",
       "      <td>0.054774</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.035101</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.011260</td>\n",
       "      <td>2.844670</td>\n",
       "      <td>0.010770</td>\n",
       "      <td>-0.004450</td>\n",
       "      <td>Buy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>0.470283</td>\n",
       "      <td>33.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>67893_20250912_164049</td>\n",
       "      <td>20250912_164049</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1</td>\n",
       "      <td>461</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52938</td>\n",
       "      <td>Swing</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>BullishMarubozu</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.064679</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.145943</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.050551</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.145943</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>-0.187815</td>\n",
       "      <td>0.066529</td>\n",
       "      <td>0.034920</td>\n",
       "      <td>Sell</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>76.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>52938_20250911_171534</td>\n",
       "      <td>20250911_171534</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>470</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52938</td>\n",
       "      <td>Swing</td>\n",
       "      <td>2025-08-29</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>BullishMarubozu</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.064679</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.145943</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.050551</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.145943</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>-0.187815</td>\n",
       "      <td>0.069717</td>\n",
       "      <td>0.038114</td>\n",
       "      <td>Sell</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Hold</td>\n",
       "      <td>LastRowHold</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>0.180148</td>\n",
       "      <td>74.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>52938_20250912_172154</td>\n",
       "      <td>20250912_172154</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>470</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyId TimeFrame   StockDate     Open     High      Low    Close   Doji  Hammer  HangingMan  InvertedHammer  ShootingStar  BullishMarubozu  BearishMarubozu  SuspiciousCandle  BullishEngulfing  \\\n",
       "0     100325     Daily  2025-09-10  0.01260  0.01260  0.01260  0.01260   True   False       False           False         False            False            False              True             False   \n",
       "1     100325     Daily  2025-09-10  0.01260  0.01260  0.01260  0.01260   True   False       False           False         False            False            False              True             False   \n",
       "2      34813      Long  2025-08-29  0.00800  0.00800  0.00800  0.00800  False   False       False           False         False             True            False             False             False   \n",
       "3      34813      Long  2025-08-29  0.00800  0.00800  0.00800  0.00800  False   False       False           False         False             True            False             False             False   \n",
       "4      67893     Short  2025-08-29  0.07500  0.07500  0.07500  0.07500  False   False       False           False         False             True            False             False              True   \n",
       "5      67893     Short  2025-08-29  0.07500  0.07500  0.07500  0.07500  False   False       False           False         False             True            False             False              True   \n",
       "6      52938     Swing  2025-08-29  0.02175  0.02175  0.02175  0.02175  False   False       False           False         False             True            False             False             False   \n",
       "7      52938     Swing  2025-08-29  0.02175  0.02175  0.02175  0.02175  False   False       False           False         False             True            False             False             False   \n",
       "\n",
       "   BearishEngulfing  BullishHarami  BearishHarami  HaramiCross  PiercingLine  DarkCloudCover  MorningStar  EveningStar  ThreeWhiteSoldiers  ThreeBlackCrows  TweezerTop  TweezerBottom  InsideBar  \\\n",
       "0             False          False          False         True         False           False        False        False               False            False       False          False      False   \n",
       "1             False          False          False         True         False           False        False        False               False            False       False          False      False   \n",
       "2             False          False          False        False         False           False        False        False               False            False       False          False      False   \n",
       "3             False          False          False        False         False           False        False        False               False            False       False          False      False   \n",
       "4             False          False          False        False         False           False        False        False               False            False       False           True      False   \n",
       "5             False          False          False        False         False           False        False        False               False            False       False           True      False   \n",
       "6             False          False          False        False         False           False        False        False               False            False       False          False      False   \n",
       "7             False          False          False        False         False           False        False        False               False            False       False          False      False   \n",
       "\n",
       "   OutsideBar  NearHigh  NearLow  PatternCount       PatternType        MA  MA_slope  UpTrend_MA  DownTrend_MA  RecentReturn  UpTrend_Return  DownTrend_Return  Volatility  LowVolatility  \\\n",
       "0       False      True     True             5              None  0.012573 -0.036035       False          True      0.000000           False             False    0.185284           True   \n",
       "1       False      True     True             5              None  0.012573 -0.036035       False          True      0.000000           False             False    0.185284           True   \n",
       "2       False      True     True             3   BullishMarubozu  0.006800  0.307692        True         False      0.600000            True             False    0.189737          False   \n",
       "3       False      True     True             3   BullishMarubozu  0.006800  0.307692        True         False      0.600000            True             False    0.189737          False   \n",
       "4       False      True     True             5  BullishEngulfing  0.071250  0.054774        True         False      0.111111            True             False    0.035101          False   \n",
       "5       False      True     True             5  BullishEngulfing  0.071250  0.054774        True         False      0.111111            True             False    0.035101          False   \n",
       "6       False      True     True             3   BullishMarubozu  0.020642  0.064679        True         False      0.145943            True             False    0.050551          False   \n",
       "7       False      True     True             3   BullishMarubozu  0.020642  0.064679        True         False      0.145943            True             False    0.050551          False   \n",
       "\n",
       "   HighVolatility       ROC  MomentumUp  MomentumDown  ConfirmedUpTrend  ConfirmedDownTrend  ValidHammer  ValidBullishEngulfing  ValidPiercingLine  ValidMorningStar  ValidThreeWhiteSoldiers  \\\n",
       "0           False -0.020218       False          True             False               False        False                  False              False             False                    False   \n",
       "1           False -0.020218       False          True             False               False        False                  False              False             False                    False   \n",
       "2            True  0.600000        True         False              True               False        False                  False              False             False                    False   \n",
       "3            True  0.600000        True         False              True               False        False                  False              False             False                    False   \n",
       "4            True  0.111111        True         False              True               False        False                  False              False             False                    False   \n",
       "5            True  0.111111        True         False              True               False        False                  False              False             False                    False   \n",
       "6            True  0.145943        True         False              True               False        False                  False              False             False                    False   \n",
       "7            True  0.145943        True         False              True               False        False                  False              False             False                    False   \n",
       "\n",
       "   ValidBullishMarubozu  ValidTweezerBottom  ValidShootingStar  ValidBearishEngulfing  ValidDarkCloud  ValidEveningStar  ValidThreeBlackCrows  ValidBearishMarubozu  ValidTweezerTop  \\\n",
       "0                 False               False              False                  False           False             False                 False                 False            False   \n",
       "1                 False               False              False                  False           False             False                 False                 False            False   \n",
       "2                 False               False              False                  False           False             False                 False                 False            False   \n",
       "3                 False               False              False                  False           False             False                 False                 False            False   \n",
       "4                 False               False              False                  False           False             False                 False                 False            False   \n",
       "5                 False               False              False                  False           False             False                 False                 False            False   \n",
       "6                 False               False              False                  False           False             False                 False                 False            False   \n",
       "7                 False               False              False                  False           False             False                 False                 False            False   \n",
       "\n",
       "   ValidHaramiCross  ValidBullishHarami  ValidBearishHarami  ValidInsideBar  ValidOutsideBar  TomorrowClose  TomorrowReturn    Return  AvgReturn  MomentumZ  BuyThresh  SellThresh MomentumAction  \\\n",
       "0             False               False               False           False            False            NaN             NaN  0.000000  -0.005814   0.031380   0.022422   -0.033779            Buy   \n",
       "1             False               False               False           False            False            NaN             NaN  0.000000  -0.005814   0.031380   0.019497   -0.036794            Buy   \n",
       "2             False               False               False           False            False            NaN             NaN  0.000000   0.060000  -0.316228  -0.001078   -0.070165           Sell   \n",
       "3             False               False               False           False            False            NaN             NaN  0.000000   0.060000  -0.316228  -0.001160   -0.070403           Sell   \n",
       "4             False               False               False           False            False            NaN             NaN  0.111111   0.011260   2.844670   0.010723   -0.004445            Buy   \n",
       "5             False               False               False           False            False            NaN             NaN  0.111111   0.011260   2.844670   0.010770   -0.004450            Buy   \n",
       "6             False               False               False           False            False            NaN             NaN  0.000000   0.009494  -0.187815   0.066529    0.034920           Sell   \n",
       "7             False               False               False           False            False            NaN             NaN  0.000000   0.009494  -0.187815   0.069717    0.038114           Sell   \n",
       "\n",
       "   BullScore  BearScore  PatternScore  PatternScoreNorm PatternAction CandleAction CandidateAction Action TomorrowAction TomorrowActionSource  SignalStrengthHybrid  ActionConfidence  \\\n",
       "0        0.0        2.0          -2.0         -2.000000          Sell         Hold             Buy    Buy           Hold          LastRowHold              0.348937          0.348937   \n",
       "1        0.0        2.0          -2.0         -2.000000          Sell         Hold             Buy    Buy           Hold          LastRowHold              0.348937          0.348937   \n",
       "2       14.0        8.0           6.0          0.600000           Buy         Hold             Buy    Buy           Hold          LastRowHold              0.199524          0.199524   \n",
       "3       14.0        8.0           6.0          0.600000           Buy         Hold             Buy    Buy           Hold          LastRowHold              0.199524          0.199524   \n",
       "4        3.0        2.0           1.0          0.333333           Buy         Hold             Buy    Buy           Hold          LastRowHold              0.470283          0.470283   \n",
       "5        3.0        2.0           1.0          0.333333           Buy         Hold             Buy    Buy           Hold          LastRowHold              0.470283          0.470283   \n",
       "6        8.0        5.0           3.0          0.600000           Buy         Hold             Buy    Buy           Hold          LastRowHold              0.180148          0.180148   \n",
       "7        8.0        5.0           3.0          0.600000           Buy         Hold             Buy    Buy           Hold          LastRowHold              0.180148          0.180148   \n",
       "\n",
       "   BullishStrengthHybrid  BearishStrengthHybrid  SignalDuration  ValidAction  HasValidSignal  SignalStrength                 BatchId       IngestedAt  BuyCount  SellCount  HoldCount    Return  \\\n",
       "0               0.348937               0.348937           162.0         True            True               1  100325_20250912_202542  20250912_202542         2          0          0  0.000000   \n",
       "1               0.348937               0.348937           162.0         True            True               1  100325_20250911_183349  20250911_183349         2          0          0  0.000000   \n",
       "2               0.199524               0.199524            65.0         True            True               1   34813_20250911_174811  20250911_174811         2          0          0  0.000000   \n",
       "3               0.199524               0.199524            65.0         True            True               1   34813_20250912_182720  20250912_182720         2          0          0  0.000000   \n",
       "4               0.470283               0.470283            33.0         True            True               1   67893_20250911_164057  20250911_164057         2          0          0  0.222222   \n",
       "5               0.470283               0.470283            33.0         True            True               1   67893_20250912_164049  20250912_164049         2          0          0  0.222222   \n",
       "6               0.180148               0.180148            76.0         True            True               1   52938_20250911_171534  20250911_171534         2          0          0  0.000000   \n",
       "7               0.180148               0.180148            74.0         True            True               1   52938_20250912_172154  20250912_172154         2          0          0  0.000000   \n",
       "\n",
       "   BuyRank  SellRank  HoldRank  \n",
       "0        1       462      1853  \n",
       "1        1       462      1853  \n",
       "2        1       403      1845  \n",
       "3        1       403      1845  \n",
       "4        1       461      1855  \n",
       "5        1       461      1855  \n",
       "6        1       470      1851  \n",
       "7        1       470      1851  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/13 09:35:06 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/09/13 09:35:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/09/13 10:39:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2670.collectToPython.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2559)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:157)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:156)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:539)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:500)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:496)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:496)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:241)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n\tat jdk.internal.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m     df_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# e.g., \"pdf_short\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     df_name_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_all\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# e.g., \"pdf_short_all\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[df_name] \u001b[38;5;241m=\u001b[39m \u001b[43mranked_rows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTimeFrame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[df_name_all] \u001b[38;5;241m=\u001b[39m df_all\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m tf)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m#ranked_rows.filter(F.col(\"TimeFrame\") == tf).toPandas().to_csv(f\"cvs/{tf.lower()}_output.csv\", index=False)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# pdf_short = ranked_rows.filter(F.col(\"TimeFrame\") == \"Short\").toPandas()\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# pdf_short.to_csv(f\"cvs/short_output.csv\", index=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.venv/python3.9_bsf/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2670.collectToPython.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2559)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:157)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:156)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:539)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:500)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:496)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:496)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:530)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:530)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:241)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n\tat jdk.internal.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "spark = init_spark(\"bsf_candidates_analysis\", log_level=\"WARN\", show_progress=False, enable_ui=True, priority=False)\n",
    "engine = init_mariadb_engine()\n",
    "\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", 200)         # Adjust width for readability\n",
    "pd.set_option(\"display.max_rows\", 20)       # Show only top 20 rows by default\n",
    "\n",
    "# Show tables\n",
    "tables_df = spark.sql(\"SHOW TABLES IN bsf\")\n",
    "tables_df.show(truncate=False)\n",
    "\n",
    "# Add row count for each table\n",
    "for row in tables_df.collect():\n",
    "    table_name = row['tableName']\n",
    "    full_name = f\"bsf.{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        count = spark.table(full_name).count()\n",
    "    except Exception as e:\n",
    "        count = f\"Error: {e}\"\n",
    "    \n",
    "    print(f\"Table: {full_name} | Rows: {count}\")\n",
    "\n",
    "\n",
    "df_last = spark.table(\"bsf.daily_signals_last_allcol \")\n",
    "df_all = spark.table(\"bsf.daily_signals\")\n",
    "\n",
    "df_all.groupBy(\"Action\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "print(df_all.groupBy(\"TimeFrame\", \"Action\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"TimeFrame\", F.desc(\"count\")) \\\n",
    "  .show(truncate=False))\n",
    "\n",
    "df = df_last.cache()\n",
    "# -----------------------------\n",
    "# Aggregate Buy/Sell/Hold counts per company per timeframe\n",
    "# -----------------------------\n",
    "df_counts = df.groupBy(\"CompanyId\", \"TimeFrame\").agg(\n",
    "    F.sum(F.when(F.col(\"Action\") == \"Buy\", 1).otherwise(0)).alias(\"BuyCount\"),\n",
    "    F.sum(F.when(F.col(\"Action\") == \"Sell\", 1).otherwise(0)).alias(\"SellCount\"),\n",
    "    F.sum(F.when(F.col(\"Action\") == \"Hold\", 1).otherwise(0)).alias(\"HoldCount\"),\n",
    "    F.sum(\"Return\").alias(\"Return\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Define window partitioned by timeframe\n",
    "# -----------------------------\n",
    "w_buy = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"BuyCount\"))\n",
    "w_sell = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"SellCount\"))\n",
    "w_hold = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"HoldCount\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Add separate rank columns\n",
    "# -----------------------------\n",
    "df_ranked = (\n",
    "    df_counts\n",
    "    .withColumn(\"BuyRank\", F.row_number().over(w_buy))\n",
    "    .withColumn(\"SellRank\", F.row_number().over(w_sell))\n",
    "    .withColumn(\"HoldRank\", F.row_number().over(w_hold))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Select what you want\n",
    "# -----------------------------\n",
    "ranked_companies = df_ranked.select(\n",
    "    \"CompanyId\", \"TimeFrame\", \"BuyCount\", \"SellCount\", \"HoldCount\", \n",
    "    \"Return\", \"BuyRank\", \"SellRank\", \"HoldRank\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Join back to the original df to get full rows with rank by last return\n",
    "# -----------------------------\n",
    "ranked_rows = df.join(ranked_companies, on=[\"CompanyId\", \"TimeFrame\"], how=\"inner\")\n",
    "\n",
    "display(ranked_rows.filter(F.col(\"BuyRank\") <= 1).orderBy(\"TimeFrame\", \"BuyRank\").toPandas())\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# Convert to Pandas and save as csv\n",
    "# -----------------------------\n",
    "#List of timeframes\n",
    "timeframes = [\"Short\", \"Swing\", \"Long\", \"Daily\"]\n",
    "\n",
    "for tf in timeframes:\n",
    "    df_name = f\"pdf_{tf.lower()}\"  # e.g., \"pdf_short\"\n",
    "    df_name_all = f\"pdf_{tf.lower()}_all\"  # e.g., \"pdf_short_all\"\n",
    "    globals()[df_name] = ranked_rows.filter(F.col(\"TimeFrame\") == tf).toPandas()\n",
    "    globals()[df_name_all] = df_all.filter(F.col(\"TimeFrame\") == tf).toPandas()\n",
    "    #ranked_rows.filter(F.col(\"TimeFrame\") == tf).toPandas().to_csv(f\"cvs/{tf.lower()}_output.csv\", index=False)\n",
    "    \n",
    "# pdf_short = ranked_rows.filter(F.col(\"TimeFrame\") == \"Short\").toPandas()\n",
    "# pdf_short.to_csv(f\"cvs/short_output.csv\", index=False)\n",
    "\n",
    "timeframe_dfs = {\n",
    "    \"Short\": pdf_short,\n",
    "    \"Swing\": pdf_swing,\n",
    "    \"Long\": pdf_long,\n",
    "    \"Daily\": pdf_daily\n",
    "}\n",
    "timeframe_dfs_all = {\n",
    "    \"Short\": pdf_short_all,\n",
    "    \"Swing\": pdf_swing_all,\n",
    "    \"Long\": pdf_long_all,\n",
    "    \"Daily\": pdf_daily_all\n",
    "}\n",
    "\n",
    "timeframes = [\"Short\", \"Swing\", \"Long\", \"Daily\"]\n",
    "\n",
    "# Store Spark DFs only (no toPandas here)\n",
    "timeframe_dfs = {tf: ranked_rows.filter(F.col(\"TimeFrame\") == tf) for tf in timeframes}\n",
    "timeframe_dfs_all = {tf: df_all.filter(F.col(\"TimeFrame\") == tf) for tf in timeframes}\n",
    "\n",
    "\n",
    "#full\n",
    "# -------------------------\n",
    "# Full Stage 1 → Stage 2 Pipeline\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pycaret.regression import setup, compare_models, predict_model, finalize_model\n",
    "\n",
    "# -------------------------\n",
    "# Stage 1: Predict TomorrowClose\n",
    "# -------------------------\n",
    "target_stage1 = \"TomorrowClose\"\n",
    "epsilon = 1e-6\n",
    "all_stage1_predictions = []\n",
    "top_n=5\n",
    "# forecast steps per timeframe for Stage 2\n",
    "forecast_steps_map = {\n",
    "    \"Daily\": 1,\n",
    "    \"Short\": 3,\n",
    "    \"Swing\": 5,\n",
    "    \"Long\": 10\n",
    "}\n",
    "\n",
    "# Loop over timeframes\n",
    "for tf, sdf_tf in timeframe_dfs_all.items():\n",
    "    pdf_tf = sdf_tf.toPandas()\n",
    "    companies = pdf_tf['CompanyId'].unique()\n",
    "    print(f\"\\n=== Phase 1 - Processing timeframe: {tf} ===\")\n",
    "    # Loop over companies\n",
    "    for cid in companies:\n",
    "        df_c = pdf_tf[pdf_tf['CompanyId'] == cid].copy()\n",
    "\n",
    "        # -------------------------------\n",
    "        # Log-transform OHLC to normalize scale\n",
    "        # -------------------------------\n",
    "        for col in [\"Open\",\"High\",\"Low\",\"Close\"]:\n",
    "            df_c[f\"log_{col}\"] = np.log(df_c[col].replace(0, epsilon))\n",
    "\n",
    "        # -------------------------------\n",
    "        # Training data: rows where target is known\n",
    "        # -------------------------------\n",
    "        train_df = df_c[df_c[target_stage1].notna()].copy()\n",
    "        if train_df.empty:\n",
    "            continue\n",
    "\n",
    "        # -------------------------------\n",
    "        # Feature selection: numeric columns correlated with target\n",
    "        # -------------------------------\n",
    "        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if target_stage1 in numeric_cols:\n",
    "            numeric_cols.remove(target_stage1)\n",
    "\n",
    "        corr = train_df[numeric_cols + [target_stage1]].corr()[target_stage1].abs()\n",
    "        threshold = 0.03  # minimal correlation\n",
    "        good_features = corr[corr >= threshold].index.tolist()\n",
    "\n",
    "        X_train = train_df[good_features].fillna(0)\n",
    "        y_train = train_df[target_stage1]\n",
    "\n",
    "        # -------------------------------\n",
    "        # Stage 1 models\n",
    "        # -------------------------------\n",
    "        lr_model = LinearRegression().fit(X_train, y_train)\n",
    "        lasso_model = Lasso(alpha=0.01).fit(X_train, y_train)\n",
    "        ridge_model = Ridge(alpha=1.0, solver=\"svd\").fit(X_train, y_train)\n",
    "\n",
    "        # -------------------------------\n",
    "        # Predict future rows (target is NaN)\n",
    "        # -------------------------------\n",
    "        future_df = df_c[df_c[target_stage1].isna()].copy()\n",
    "        if not future_df.empty:\n",
    "            X_future = future_df[good_features].fillna(0)\n",
    "            future_df[\"Pred_Linear\"] = lr_model.predict(X_future)\n",
    "            future_df[\"Pred_Lasso\"] = lasso_model.predict(X_future)\n",
    "            future_df[\"Pred_Ridge\"] = ridge_model.predict(X_future)\n",
    "            future_df[\"TimeFrame\"] = tf\n",
    "            future_df[\"CompanyId\"] = cid\n",
    "            all_stage1_predictions.append(future_df)\n",
    "\n",
    "# -------------------------------\n",
    "# Combine Stage 1 predictions\n",
    "# -------------------------------\n",
    "if all_stage1_predictions:\n",
    "    stage1_df = pd.concat(all_stage1_predictions, ignore_index=True)\n",
    "else:\n",
    "    stage1_df = pd.DataFrame()\n",
    "    print(\"No predictions generated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Top-N selection per timeframe (using average of Linear/Lasso/Ridge)\n",
    "# -------------------------------\n",
    "if not stage1_df.empty:\n",
    "    stage1_df[\"PredictedTomorrowClose\"] = stage1_df[[\"Pred_Linear\",\"Pred_Lasso\",\"Pred_Ridge\"]].mean(axis=1)\n",
    "\n",
    "    top_list = []\n",
    "    for tf in stage1_df[\"TimeFrame\"].unique():\n",
    "        tf_df = stage1_df[stage1_df[\"TimeFrame\"] == tf].copy()\n",
    "        tf_df = tf_df.sort_values(\"PredictedTomorrowClose\", ascending=False)\n",
    "        top_list.append(tf_df.head(top_n))\n",
    "\n",
    "    stage1_top_df = pd.concat(top_list, ignore_index=True)\n",
    "\n",
    "    print(\"\\n=== Stage 1 Top Predictions per Timeframe ===\")\n",
    "    print(stage1_top_df[[\"TimeFrame\", \"CompanyId\", \"PredictedTomorrowClose\"]])\n",
    "else:\n",
    "    stage1_top_df = pd.DataFrame()\n",
    "\n",
    "# -------------------------\n",
    "# Phase 2: SARIMAX + PyCaret (optimized)\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pycaret.regression import setup, create_model, tune_model, finalize_model, predict_model\n",
    "\n",
    "# Stage 2 targets\n",
    "target_stage1 = \"PredictedTomorrowClose\"\n",
    "target_stage2 = \"TomorrowReturn\"\n",
    "\n",
    "# Forecast steps per timeframe\n",
    "forecast_steps_map = {\n",
    "    \"Daily\": 1,\n",
    "    \"Short\": 3,\n",
    "    \"Swing\": 5,\n",
    "    \"Long\": 10\n",
    "}\n",
    "\n",
    "# Option 1: Use all Stage 1 predictions\n",
    "combined_top_df_clean = stage1_df.fillna(0)\n",
    "\n",
    "# Option 2: Use only the top-N per timeframe\n",
    "combined_top_df_clean = stage1_top_df.fillna(0)\n",
    "\n",
    "# Numeric features (exclude targets)\n",
    "stage2_features = [c for c in combined_top_df_clean.select_dtypes(include=[np.number]).columns \n",
    "                   if c not in [target_stage1, target_stage2]]\n",
    "\n",
    "# -------------------------\n",
    "# Phase 2: SARIMAX + PyCaret (optimized)\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pycaret.regression import setup, create_model, tune_model, finalize_model, predict_model\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8001\")\n",
    "mlflow.set_experiment(\"Stage2_SARIMAX_PyCaret\")\n",
    "\n",
    "sarimax_results = []\n",
    "pycaret_results = []\n",
    "\n",
    "# with memory issues may want to limit\n",
    "pycaret_models = [\"lr\", \"lasso\", \"ridge\", \"en\"]\n",
    "pycaret_models = [\"lasso\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for tf, steps in forecast_steps_map.items():\n",
    "    df_tf = timeframe_dfs_all[tf].toPandas().copy()\n",
    "\n",
    "    top_companies = combined_top_df_clean.loc[\n",
    "        combined_top_df_clean[\"TimeFrame\"] == tf, \"CompanyId\"\n",
    "    ].unique()\n",
    "\n",
    "    for cid in top_companies:\n",
    "        df_c = df_tf[df_tf[\"CompanyId\"] == cid].copy().dropna(subset=[target_stage2])\n",
    "\n",
    "        if df_c.empty or len(df_c) < 120:\n",
    "            print(f\"⏭️ Skipping {cid}-{tf} (not enough data)\")\n",
    "            continue\n",
    "\n",
    "        ts = df_c[target_stage2]\n",
    "\n",
    "        # -------------------------\n",
    "        # SARIMAX\n",
    "        # -------------------------\n",
    "        try:\n",
    "            sarimax_entry = {\"CompanyId\": cid, \"TimeFrame\": tf}\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"SARIMAX_{cid}_{tf}\"):\n",
    "                # Example fixed order (replace with auto_arima search results if you have them)\n",
    "                order = (1, 1, 1)\n",
    "                seasonal_order = (0, 1, 1, 7)\n",
    "\n",
    "                sarimax_model = SARIMAX(\n",
    "                    ts, order=order, seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False\n",
    "                )\n",
    "                sarimax_fit = sarimax_model.fit(disp=False)\n",
    "\n",
    "                forecast = sarimax_fit.get_forecast(steps=steps)\n",
    "                mean_pred = forecast.predicted_mean.mean()\n",
    "                '''\n",
    "                sarimax_entry[\"Pred_SARIMAX\"] = mean_pred\n",
    "                mlflow.log_metric(\"MeanPred_SARIMAX\", mean_pred)\n",
    "\n",
    "                # Log params\n",
    "                mlflow.log_params({\"order\": order, \"seasonal_order\": seasonal_order})\n",
    "\n",
    "            sarimax_results.append(sarimax_entry)\n",
    "            '''\n",
    "                sarimax_entry = {\n",
    "                    \"CompanyId\": cid,\n",
    "                    \"TimeFrame\": tf,\n",
    "                    \"Pred_SARIMAX\": float(mean_pred),  # ensure scalar\n",
    "                    \"order\": str(order),\n",
    "                    \"seasonal_order\": str(seasonal_order)\n",
    "                }\n",
    "                mlflow.log_metric(\"MeanPred_SARIMAX\", mean_pred)\n",
    "                mlflow.log_params({\"order\": order, \"seasonal_order\": seasonal_order})\n",
    "            \n",
    "                # save the model object as an artifact\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    model_path = os.path.join(tmpdir, \"sarimax_model.pkl\")\n",
    "                    joblib.dump(sarimax_fit, model_path)\n",
    "                    mlflow.log_artifact(model_path, name=\"SARIMAX_model\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ SARIMAX failed for {cid}-{tf}: {e}\")\n",
    "\"\"\"\n",
    "for tf, steps in forecast_steps_map.items():\n",
    "    df_tf = timeframe_dfs_all[tf].toPandas().copy()\n",
    "\n",
    "    top_companies = combined_top_df_clean.loc[\n",
    "        combined_top_df_clean[\"TimeFrame\"] == tf, \"CompanyId\"\n",
    "    ].unique()\n",
    "\n",
    "    for cid in top_companies:\n",
    "        df_c = df_tf[df_tf[\"CompanyId\"] == cid].copy().dropna(subset=[target_stage2])\n",
    "\n",
    "        if df_c.empty or len(df_c) < 120:\n",
    "            print(f\"⏭️ Skipping {cid}-{tf} (not enough data)\")\n",
    "            continue\n",
    "\n",
    "        ts = df_c[target_stage2]\n",
    "\n",
    "        # -------------------------\n",
    "        # SARIMAX\n",
    "        # -------------------------\n",
    "        try:\n",
    "            sarimax_entry = {\"CompanyId\": cid, \"TimeFrame\": tf}\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"SARIMAX_{cid}_{tf}\"):\n",
    "                # Example fixed order (replace with auto_arima search results if you have them)\n",
    "                order = (1, 1, 1)\n",
    "                seasonal_order = (0, 1, 1, 7)\n",
    "\n",
    "                sarimax_model = SARIMAX(\n",
    "                    ts, order=order, seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False\n",
    "                )\n",
    "                sarimax_fit = sarimax_model.fit(disp=False)\n",
    "\n",
    "                forecast = sarimax_fit.get_forecast(steps=steps)\n",
    "                mean_pred = forecast.predicted_mean.mean()\n",
    "\n",
    "                # Store results\n",
    "                sarimax_entry.update({\n",
    "                    \"Pred_SARIMAX\": float(mean_pred),\n",
    "                    \"order\": str(order),\n",
    "                    \"seasonal_order\": str(seasonal_order)\n",
    "                })\n",
    "\n",
    "                # Log metric and params\n",
    "                mlflow.log_metric(\"MeanPred_SARIMAX\", mean_pred)\n",
    "                mlflow.log_params({\"order\": order, \"seasonal_order\": seasonal_order})\n",
    "\n",
    "                # save model to temp file\n",
    "                model_path = f\"sarimax_{cid}_{tf}.pkl\"\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    pickle.dump(sarimax_fit, f)\n",
    "                mlflow.log_artifact(model_path)\n",
    "                os.remove(model_path)\n",
    "                \n",
    "                sarimax_results.append(sarimax_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ SARIMAX failed for {cid}-{tf}: {e}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # PyCaret\n",
    "        # -------------------------\n",
    "        try:\n",
    "            stage2_features_c = [\n",
    "                c for c in df_c.select_dtypes(include=[np.number]).columns\n",
    "                if c not in [target_stage1, target_stage2]\n",
    "            ]\n",
    "\n",
    "            pycaret_entry = {\"CompanyId\": cid, \"TimeFrame\": tf}\n",
    "\n",
    "            s = setup(\n",
    "                data=df_c,\n",
    "                target=target_stage2,\n",
    "                numeric_features=stage2_features_c,\n",
    "                session_id=42,\n",
    "                log_experiment=False,   # ✅ manual MLflow control\n",
    "                html=False\n",
    "            )\n",
    "\n",
    "            for model_name in pycaret_models:\n",
    "                with mlflow.start_run(run_name=f\"PyCaret_{model_name}_{cid}_{tf}\"):\n",
    "                    '''\n",
    "                    Memory issues: \n",
    "                    Model list is above\n",
    "                    options\n",
    "                    ------------\n",
    "                    df_c_small = df_c.sample(frac=0.3, random_state=42)\n",
    "                    tuned = tune_model(model, fold=2, optimize=\"MAE\", n_iter=10)\n",
    "                    \n",
    "                    model = create_model(model_name, fold=2)  # or even 1\n",
    "                    tuned = tune_model(model, fold=2, optimize=\"MAE\")\n",
    "\n",
    "                    suggested:\n",
    "                    model = create_modelmodel_name, fold=2)\n",
    "                    final = finalize_model(model)\n",
    "                    preds = predict_model(final, data=df_c)\n",
    "                    ------------\n",
    "                    '''\n",
    "                    try:\n",
    "                        model = create_model(model_name, fold=3)\n",
    "                        tuned = tune_model(model, fold=3, optimize=\"MAE\")\n",
    "                        final = finalize_model(tuned)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Tuning failed for {model_name} {cid}-{tf}: {e}\")\n",
    "                        # fallback to untuned\n",
    "                        final = finalize_model(model)\n",
    "\n",
    "                    preds = predict_model(final, data=df_c)\n",
    "                    pred_col = next(\n",
    "                        (c for c in [\"Label\", \"prediction_label\", \"prediction\"] if c in preds.columns),\n",
    "                        None\n",
    "                    )\n",
    "                    mean_pred = preds[pred_col].mean()\n",
    "                    pycaret_entry[f\"Pred_{model_name}_PyCaret\"] = mean_pred\n",
    "\n",
    "                    mlflow.log_metric(f\"MeanPred_{model_name}\", mean_pred)\n",
    "\n",
    "                    # Log sklearn model directly\n",
    "                    mlflow.pycaret.log_model(\n",
    "                        model=final_model,\n",
    "                        name=f\"{model_name}_model\",\n",
    "                        input_example=df_c.head(1)  # just one row is enough\n",
    "                    )\n",
    "\n",
    "            pycaret_results.append(pycaret_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ PyCaret failed for {cid}-{tf}: {e}\")\n",
    "\n",
    "\n",
    "for entry in sarimax_results:\n",
    "    print(entry.keys())\n",
    "\n",
    "\n",
    "# Convert results\n",
    "sarimax_df = pd.DataFrame(sarimax_results)\n",
    "pycaret_df = pd.DataFrame(pycaret_results)\n",
    "\n",
    "# Merge SARIMAX + PyCaret outputs\n",
    "final_df = combined_top_df_clean.merge(sarimax_df, on=['CompanyId','TimeFrame'], how='left')\n",
    "final_df = final_df.merge(pycaret_df, on=['CompanyId','TimeFrame'], how='left')\n",
    "\n",
    "# -------------------------\n",
    "# Select Top N per timeframe\n",
    "# -------------------------\n",
    "top_n = 5\n",
    "\n",
    "def select_top_n(df, pred_col, n=5):\n",
    "    top_list = []\n",
    "    for tf in df['TimeFrame'].unique():\n",
    "        tf_df = df[df['TimeFrame'] == tf].copy()\n",
    "        tf_df = tf_df.sort_values(pred_col, ascending=False)\n",
    "        top_list.append(tf_df.head(n))\n",
    "    return pd.concat(top_list, ignore_index=True)\n",
    "\n",
    "# Example: select top 5 by SARIMAX\n",
    "top_sarimax_df = select_top_n(final_df, 'Pred_SARIMAX', top_n)\n",
    "\n",
    "# Example: select top 5 by Ridge\n",
    "def select_top_n(df, pred_col, n=5):\n",
    "    if pred_col not in df.columns:\n",
    "        print(f\"⚠️ Column {pred_col} not found in DataFrame\")\n",
    "        return pd.DataFrame()  # return empty\n",
    "    top_list = []\n",
    "    for tf in df['TimeFrame'].unique():\n",
    "        tf_df = df[df['TimeFrame'] == tf].copy()\n",
    "        tf_df = tf_df.sort_values(pred_col, ascending=False)\n",
    "        top_list.append(tf_df.head(n))\n",
    "    return pd.concat(top_list, ignore_index=True) if top_list else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Merge for comparison\n",
    "top_combined_df = top_sarimax_df.merge(\n",
    "    pycaret_df,\n",
    "    on=['CompanyId','TimeFrame'],\n",
    "    how='outer',\n",
    "    suffixes=('_SARIMAX','_PyCaret')\n",
    ")\n",
    "\n",
    "#print(\"Top N companies per timeframe (combined SARIMAX + PyCaret):\")\n",
    "#print(top_combined_df)\n",
    "\n",
    "# pdf_short = ranked_rows.filter(F.col(\"TimeFrame\") == \"Short\").toPandas()\n",
    "#top_combined_df.to_csv(f\"cvs/final_top_combined_df.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_top_n_final(df, pred_col=\"PredictedTomorrowClose\", n=5):\n",
    "    \"\"\"\n",
    "    Select top-N rows per TimeFrame by prediction column.\n",
    "    \"\"\"\n",
    "    top_list = []\n",
    "    for tf in df['TimeFrame'].unique():\n",
    "        tf_df = df[df['TimeFrame'] == tf].copy()\n",
    "        tf_df = tf_df.sort_values(pred_col, ascending=False)\n",
    "        top_list.append(tf_df.head(n))\n",
    "    return pd.concat(top_list, ignore_index=True)\n",
    "\n",
    "# Pick top-N by PredictedTomorrowClose\n",
    "top_candidates = select_top_n_final(final_df, pred_col=\"PredictedTomorrowClose\", n=5)\n",
    "\n",
    "# Reduce to just what you need for DB write\n",
    "top_out = top_candidates[[\n",
    "    \"CompanyId\",\n",
    "    \"TimeFrame\",\n",
    "    \"PredictedTomorrowClose\"\n",
    "]]\n",
    "\n",
    "print(top_out)\n",
    "from pyspark.sql.functions import lit, max as spark_max\n",
    "\n",
    "# --- Step 1: Reduce to needed columns ---\n",
    "top_out = top_candidates[[\"CompanyId\", \"TimeFrame\", \"PredictedTomorrowClose\"]]\n",
    "\n",
    "# --- Step 2: Create managed Delta table if not exists ---\n",
    "table_name = \"bsf.final_top_candidates\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    CompanyId STRING,\n",
    "    TimeFrame STRING,\n",
    "    PredictedTomorrowClose DOUBLE,\n",
    "    run_id INT\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# --- Step 3: Determine next run_id ---\n",
    "if spark._jsparkSession.catalog().tableExists(table_name):\n",
    "    latest_run_id = (\n",
    "        spark.read.table(table_name)\n",
    "        .agg(spark_max(\"run_id\"))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    run_id = (latest_run_id or 0) + 1\n",
    "else:\n",
    "    run_id = 1\n",
    "\n",
    "# --- Step 4: Add run_id and write ---\n",
    "top_out_df = top_out.withColumn(\"run_id\", lit(run_id))\n",
    "top_out_df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"✅ Run {run_id} written to {table_name}\")\n",
    "print(top_out_df.show())\n",
    "# pdf_short = ranked_rows.filter(F.col(\"TimeFrame\") == \"Short\").toPandas()\n",
    "#top_combined_df.to_csv(f\"cvs/final_top_combined_1_df.csv\", index=False)\n",
    "from pyspark.sql.functions import lit, col, max as spark_max\n",
    "\n",
    "table_name = \"bsf.final_top_combined\"\n",
    "\n",
    "# --- Step 1: Create managed Delta table if not exists ---\n",
    "# (I’ll include all your columns explicitly)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    CompanyId STRING,\n",
    "    StockDate DATE,\n",
    "    Open DOUBLE,\n",
    "    High DOUBLE,\n",
    "    Low DOUBLE,\n",
    "    Close DOUBLE,\n",
    "    TomorrowClose DOUBLE,\n",
    "    Return DOUBLE,\n",
    "    TomorrowReturn DOUBLE,\n",
    "    Doji BOOLEAN,\n",
    "    Hammer BOOLEAN,\n",
    "    InvertedHammer BOOLEAN,\n",
    "    ShootingStar BOOLEAN,\n",
    "    BullishEngulfing BOOLEAN,\n",
    "    BearishEngulfing BOOLEAN,\n",
    "    PiercingLine BOOLEAN,\n",
    "    DarkCloudCover BOOLEAN,\n",
    "    MorningStar BOOLEAN,\n",
    "    EveningStar BOOLEAN,\n",
    "    ThreeWhiteSoldiers BOOLEAN,\n",
    "    ThreeBlackCrows BOOLEAN,\n",
    "    TweezerTop BOOLEAN,\n",
    "    TweezerBottom BOOLEAN,\n",
    "    InsideBar BOOLEAN,\n",
    "    OutsideBar BOOLEAN,\n",
    "    MA DOUBLE,\n",
    "    MA_slope DOUBLE,\n",
    "    UpTrend_MA BOOLEAN,\n",
    "    DownTrend_MA BOOLEAN,\n",
    "    MomentumUp BOOLEAN,\n",
    "    MomentumDown BOOLEAN,\n",
    "    ConfirmedUpTrend BOOLEAN,\n",
    "    ConfirmedDownTrend BOOLEAN,\n",
    "    RecentReturn DOUBLE,\n",
    "    UpTrend_Return BOOLEAN,\n",
    "    DownTrend_Return BOOLEAN,\n",
    "    Volatility DOUBLE,\n",
    "    LowVolatility BOOLEAN,\n",
    "    HighVolatility BOOLEAN,\n",
    "    ROC DOUBLE,\n",
    "    MomentumZ DOUBLE,\n",
    "    SignalStrength INT,\n",
    "    SignalStrengthHybrid DOUBLE,\n",
    "    ActionConfidence DOUBLE,\n",
    "    BullishStrengthHybrid DOUBLE,\n",
    "    BearishStrengthHybrid DOUBLE,\n",
    "    SignalDuration DOUBLE,\n",
    "    ValidAction BOOLEAN,\n",
    "    HasValidSignal BOOLEAN,\n",
    "    MomentumAction STRING,\n",
    "    PatternAction STRING,\n",
    "    CandleAction STRING,\n",
    "    CandidateAction STRING,\n",
    "    Action STRING,\n",
    "    TomorrowAction STRING,\n",
    "    TomorrowActionSource STRING,\n",
    "    BatchId STRING,\n",
    "    IngestedAt STRING,\n",
    "    TimeFrame STRING,\n",
    "    log_Open DOUBLE,\n",
    "    log_High DOUBLE,\n",
    "    log_Low DOUBLE,\n",
    "    log_Close DOUBLE,\n",
    "    Pred_Linear DOUBLE,\n",
    "    Pred_Lasso DOUBLE,\n",
    "    Pred_Ridge DOUBLE,\n",
    "    PredictedTomorrowClose DOUBLE,\n",
    "    Pred_SARIMAX DOUBLE,\n",
    "    order STRING,\n",
    "    seasonal_order STRING,\n",
    "    Pred_lr_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_lasso_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_ridge_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_en_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_lr_PyCaret_PyCaret DOUBLE,\n",
    "    Pred_lasso_PyCaret_PyCaret DOUBLE,\n",
    "    Pred_ridge_PyCaret_PyCaret DOUBLE,\n",
    "    Pred_en_PyCaret_PyCaret DOUBLE,\n",
    "    run_id INT\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# --- Step 2: Get next run_id ---\n",
    "if spark._jsparkSession.catalog().tableExists(table_name):\n",
    "    latest_run_id = (\n",
    "        spark.read.table(table_name)\n",
    "        .agg(spark_max(\"run_id\"))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    run_id = (latest_run_id or 0) + 1\n",
    "else:\n",
    "    run_id = 1\n",
    "\n",
    "# --- Step 3: Add run_id to DataFrame and save ---\n",
    "df_with_id = top_combined_df.withColumn(\"run_id\", lit(run_id))\n",
    "\n",
    "df_with_id.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"✅ Run {run_id} written to {table_name}\")\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d443b73-e0c7-45d2-8d23-b50f716434f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
