{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ca6ff0-6b82-447e-bb52-a58d0f87e055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/jupyter/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyter/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyter/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9658b9c4-0eed-46e7-8d14-765de0dfa0da;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0rc1 in spark-list\n",
      "\tfound io.delta#delta-storage;3.0.0rc1 in spark-list\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in spark-list\n",
      ":: resolution report :: resolve 605ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0rc1 from spark-list in [default]\n",
      "\tio.delta#delta-storage;3.0.0rc1 from spark-list in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from spark-list in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9658b9c4-0eed-46e7-8d14-765de0dfa0da\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/12ms)\n",
      "25/09/13 20:56:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/13 20:56:46 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_candidates_analysis' log_level=WARN (effective=WARN), progress=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/13 20:57:11 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/13 20:57:11 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/13 20:57:11 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/13 20:57:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/13 20:57:12 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/13 20:57:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/13 20:57:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/13 20:57:15 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/13 20:57:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/13 20:57:19 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-----------+\n",
      "|namespace|tableName                |isTemporary|\n",
      "+---------+-------------------------+-----------+\n",
      "|bsf      |company                  |false      |\n",
      "|bsf      |companystockhistory      |false      |\n",
      "|bsf      |daily_signals            |false      |\n",
      "|bsf      |daily_signals_allcol     |false      |\n",
      "|bsf      |daily_signals_last       |false      |\n",
      "|bsf      |daily_signals_last_allcol|false      |\n",
      "+---------+-------------------------+-----------+\n",
      "\n",
      "Table: bsf.company | Rows: 30949\n",
      "Table: bsf.companystockhistory | Rows: 459399\n",
      "Table: bsf.daily_signals | Rows: 1377555\n",
      "Table: bsf.daily_signals_allcol | Rows: 1377555\n",
      "Table: bsf.daily_signals_last | Rows: 5478\n",
      "Table: bsf.daily_signals_last_allcol | Rows: 5478\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "spark = init_spark(\"bsf_candidates_analysis\", log_level=\"WARN\", show_progress=False, enable_ui=True, priority=True)\n",
    "engine = init_mariadb_engine()\n",
    "\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", 200)         # Adjust width for readability\n",
    "pd.set_option(\"display.max_rows\", 20)       # Show only top 20 rows by default\n",
    "\n",
    "# Show tables\n",
    "tables_df = spark.sql(\"SHOW TABLES IN bsf\")\n",
    "tables_df.show(truncate=False)\n",
    "\n",
    "# Add row count for each table\n",
    "for row in tables_df.collect():\n",
    "    table_name = row['tableName']\n",
    "    full_name = f\"bsf.{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        count = spark.table(full_name).count()\n",
    "    except Exception as e:\n",
    "        count = f\"Error: {e}\"\n",
    "    \n",
    "    print(f\"Table: {full_name} | Rows: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea65e572-292b-4152-bc44-cadae76b45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡️ spark.sql.warehouse.dir     : file:/srv/lakehouse/tables\n",
      "⚡️ spark.delta.basePath        : /srv/lakehouse/delta\n",
      "⚡️ spark.sql.filesource.path   : /srv/lakehouse/files\n",
      "⚡️ spark.nond2rd.defaultpath   : /srv/lakehouse/files\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|bsf      |\n",
      "|default  |\n",
      "+---------+\n",
      "\n",
      "ℹ️ Database Name: bsf, Location: file:/srv/lakehouse/tables/bsf.db\n",
      "ℹ️ Database Name: default, Location: file:/srv/lakehouse/tables/default.db\n"
     ]
    }
   ],
   "source": [
    "# Get and display default locations with description\n",
    "warehouse_dir = spark.conf.get(\"spark.sql.warehouse.dir\", \"Not set\")\n",
    "delta_base_path = spark.conf.get(\"spark.delta.basePath\", \"Not set\")\n",
    "filesource_path = spark.conf.get(\"spark.sql.filesource.path\", \"Not set\")\n",
    "nond2rd_path = spark.conf.get(\"spark.nond2rd.defaultpath\", \"Not set\")\n",
    "\n",
    "# Print configuration values\n",
    "print(f\"⚡️ spark.sql.warehouse.dir     : {warehouse_dir}\")\n",
    "print(f\"⚡️ spark.delta.basePath        : {delta_base_path}\")\n",
    "print(f\"⚡️ spark.sql.filesource.path   : {filesource_path}\")\n",
    "print(f\"⚡️ spark.nond2rd.defaultpath   : {nond2rd_path}\")\n",
    "\n",
    "# Show all databases in Hive - Using Spark SQL\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
    "\n",
    "# List all databases using Spark catalog - Using Catalog API\n",
    "db_list = spark.catalog.listDatabases()\n",
    "         \n",
    "# Display databases\n",
    "\n",
    "for db in db_list:\n",
    "    print(f\"ℹ️ Database Name: {db.name}, Location: {db.locationUri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2227b717-5452-4763-ab2a-b5ba309cbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Approximate size on disk\n",
    "def get_size(path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b771f83-c9f2-4922-ab5d-662354f97b71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-----------+\n",
      "|namespace|tableName                |isTemporary|\n",
      "+---------+-------------------------+-----------+\n",
      "|bsf      |company                  |false      |\n",
      "|bsf      |companystockhistory      |false      |\n",
      "|bsf      |daily_signals            |false      |\n",
      "|bsf      |daily_signals_allcol     |false      |\n",
      "|bsf      |daily_signals_last       |false      |\n",
      "|bsf      |daily_signals_last_allcol|false      |\n",
      "+---------+-------------------------+-----------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|default  |stores   |false      |\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/13 21:10:16 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/09/13 21:10:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN bsf\").show(truncate=False)\n",
    "spark.sql(\"SHOW TABLES IN default\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8c8bcb-8bc5-44e0-9797-bf50d4a582a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 30949, size_mb: 0.0, location: file:/srv/lakehouse/tables/bsf.db/company\n"
     ]
    }
   ],
   "source": [
    "table_name=\"bsf.company\"\n",
    "\n",
    "num_rows = spark.sql(f\"SELECT COUNT(*) AS cnt FROM {table_name}\").collect()[0]['cnt']\n",
    "\n",
    "# 2️⃣ Table location\n",
    "desc = spark.sql(f\"DESCRIBE FORMATTED {table_name}\").collect()\n",
    "location_row = next(row for row in desc if row['col_name'].strip() == 'Location')\n",
    "table_location = location_row['data_type'].strip()\n",
    "\n",
    "\n",
    "\n",
    "size_bytes = get_size(table_location)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "\n",
    "print (f\"rows: {num_rows}, size_mb: {size_mb}, location: {table_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a787f-888e-4120-9861-1d0e82db3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
