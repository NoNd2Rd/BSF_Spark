{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b906b38d-f9ff-445b-b0ad-0105d57a153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/jupyter/.venv/python3.9_bsf/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyter/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyter/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-34a77889-0542-41fd-ae90-1a339cd51520;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0rc1 in spark-list\n",
      "\tfound io.delta#delta-storage;3.0.0rc1 in spark-list\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in spark-list\n",
      ":: resolution report :: resolve 395ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0rc1 from spark-list in [default]\n",
      "\tio.delta#delta-storage;3.0.0rc1 from spark-list in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from spark-list in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-34a77889-0542-41fd-ae90-1a339cd51520\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/12ms)\n",
      "25/09/20 22:50:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/20 22:50:49 WARN SparkConf: Total executor cores: 3 is not divisible by cores per executor: 2, the left cores: 1 will not be allocated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Started 'bsf_candidates_analysis' log_level=WARN (effective=WARN), progress=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 22:51:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/20 22:51:05 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/20 22:51:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/20 22:51:06 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/20 22:51:06 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/20 22:51:06 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/20 22:51:08 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/20 22:51:08 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/20 22:51:08 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/20 22:51:11 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "spark = init_spark(\"bsf_candidates_analysis\", log_level=\"WARN\", show_progress=False, enable_ui=True, process_option=\"wide\")\n",
    "engine = init_mariadb_engine()\n",
    "\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", 200)         # Adjust width for readability\n",
    "pd.set_option(\"display.max_rows\", 20)       # Show only top 20 rows by default\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# -----------------------------\n",
    "# Load tables\n",
    "# -----------------------------\n",
    "df_last = spark.table(\"bsf.history_signals_allcol_last\")\n",
    "df_all = spark.table(\"bsf.history_signals\")\n",
    "\n",
    "# -----------------------------\n",
    "# Drop some BS columns\n",
    "# -----------------------------\n",
    "cols_to_drop = [\"BatchId\", \"IngestedAt\"]\n",
    "df_last = df_last.drop(*cols_to_drop)\n",
    "df_all = df_all.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4708986e-ed73-4b3e-baee-35dea231f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def check_future_rows(timeframe_dict, dict_name=\"timeframe\"):\n",
    "    for tf, df in timeframe_dict.items():\n",
    "        print(f\"\\n--- Checking {dict_name} '{tf}' ---\")\n",
    "        \n",
    "        # Add row number per company sorted by date descending\n",
    "        window_spec = Window.partitionBy(\"CompanyId\").orderBy(F.col(\"StockDate\").desc())\n",
    "        df_with_rn = df.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "        \n",
    "        # Filter rows where TomorrowClose is NaN\n",
    "        df_nulls = df_with_rn.filter(F.isnan(F.col(\"TomorrowClose\")))\n",
    "        \n",
    "        # Show the rows\n",
    "        df_nulls.select(\"CompanyId\", \"StockDate\", \"rn\").show(truncate=False)\n",
    "        \n",
    "        # Sanity check: count per company\n",
    "        df_nulls.groupBy(\"CompanyId\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b93743-3435-4b94-a889-c087e2541230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage 1 completed: Top 60 candidates selected per timeframe\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Filter only Buy actions from last-row DF\n",
    "# -----------------------------\n",
    "df_last_buys = df_last.filter(F.col(\"Action\") == \"Buy\").cache()  # cache because we reuse it\n",
    "\n",
    "# -----------------------------\n",
    "# Define ranking window per timeframe\n",
    "# -----------------------------\n",
    "w_tf = Window.partitionBy(\"TimeFrame\").orderBy(\n",
    "    F.desc(\"ActionConfidence\"),\n",
    "    F.desc(\"Return\")\n",
    ")\n",
    "\n",
    "top_n = 60\n",
    "\n",
    "# -----------------------------\n",
    "# Rank and select top N Buy companies per timeframe\n",
    "# -----------------------------\n",
    "df_ranked_last_top = (\n",
    "    df_last_buys\n",
    "    .withColumn(\"BuyRank\", F.row_number().over(w_tf))  # use rank() if you want ties\n",
    "    .filter(F.col(\"BuyRank\") <= top_n)\n",
    "    .orderBy(F.col(\"ActionConfidence\").desc(), F.col(\"BuyRank\").asc())\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Extract top companies per timeframe\n",
    "# -----------------------------\n",
    "top_companies = df_ranked_last_top.select(\"CompanyId\", \"TimeFrame\").distinct().cache()\n",
    "# -----------------------------\n",
    "# Filter original last-row DF and full historical DF to include only top Buy companies\n",
    "# Cache large DataFrames once\n",
    "# -----------------------------\n",
    "df_ranked_last_topN = df_last.join(\n",
    "    broadcast(df_ranked_last_top.select(\"CompanyId\",\"TimeFrame\",\"BuyRank\")),\n",
    "    on=[\"CompanyId\",\"TimeFrame\"],\n",
    "    how=\"inner\"\n",
    ").cache()\n",
    "\n",
    "df_ranked_all_topN = df_all.join(\n",
    "    broadcast(df_ranked_last_top.select(\"CompanyId\",\"TimeFrame\",\"BuyRank\")),\n",
    "    on=[\"CompanyId\",\"TimeFrame\"],\n",
    "    how=\"inner\"\n",
    ").cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# -----------------------------\n",
    "# List of timeframes\n",
    "# -----------------------------\n",
    "timeframes = [\"Short\", \"Swing\", \"Long\", \"Daily\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Dictionaries to store per-timeframe DataFrames\n",
    "# -----------------------------\n",
    "timeframe_dfs = {}\n",
    "timeframe_dfs_all = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Efficient per-timeframe splitting\n",
    "# -----------------------------\n",
    "for tf in timeframes:\n",
    "    # Last-row top N for this timeframe\n",
    "    timeframe_dfs[tf] = df_ranked_last_topN.filter(F.col(\"TimeFrame\") == tf)\n",
    "    \n",
    "    # Full historical top N for this timeframe\n",
    "    timeframe_dfs_all[tf] = df_ranked_all_topN.filter(F.col(\"TimeFrame\") == tf)\n",
    "\n",
    "\n",
    "print(f\"✅ Stage 1 completed: Top {top_n} candidates selected per timeframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce54f984-8682-433a-a57d-536a3000b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def debug_spark_df(df: DataFrame, col_check=None, n=5, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Helper to inspect a PySpark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "        col_check: optional column name to count nulls\n",
    "        n: number of rows to show\n",
    "        name: optional label for printing\n",
    "    \"\"\"\n",
    "    n_rows = df.count()\n",
    "    n_cols = len(df.columns)\n",
    "    print(f\"--- Debug {name} ---\")\n",
    "    print(f\"Shape: ({n_rows}, {n_cols})\")\n",
    "    \n",
    "    if col_check and col_check in df.columns:\n",
    "        null_count = df.filter(F.col(col_check).isNull()).count()\n",
    "        print(f\"Column '{col_check}' nulls: {null_count}\")\n",
    "    \n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    print(f\"Sample rows:\")\n",
    "    df.show(n)\n",
    "    print(\"--- End Debug ---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db05beb4-5c8c-4e8c-a552-a74e536f775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helper: select best model\n",
    "# -----------------------------\n",
    "def select_best_model_simple(metrics_dict, strategy=\"hybrid\"):\n",
    "    if strategy == \"rmse\":\n",
    "        return min(metrics_dict, key=lambda m: metrics_dict[m][\"RMSE\"])\n",
    "    elif strategy == \"mae\":\n",
    "        return min(metrics_dict, key=lambda m: metrics_dict[m][\"MAE\"])\n",
    "    elif strategy == \"direction\":\n",
    "        return max(metrics_dict, key=lambda m: metrics_dict[m][\"DirectionAcc\"])\n",
    "    elif strategy == \"hybrid\":\n",
    "        # DirectionAcc first, RMSE tie-breaker\n",
    "        return max(metrics_dict, key=lambda m: (metrics_dict[m][\"DirectionAcc\"], -metrics_dict[m][\"RMSE\"]))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e327ac3-3134-41e5-9b64-9a014125a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Hybrid scoring function\n",
    "# -----------------------------\n",
    "def hybrid_score(metrics, w_dir=1.0, w_rmse=0.5, w_mape=0.5):\n",
    "    \"\"\"\n",
    "    Combine multiple metrics: higher DirectionAcc, lower RMSE & MAPE\n",
    "    \"\"\"\n",
    "    return w_dir*metrics[\"DirectionAcc\"] - w_rmse*metrics[\"RMSE\"] - w_mape*metrics[\"MAPE\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Pick best model using hybrid score\n",
    "# -----------------------------\n",
    "def select_best_model(metrics_dict):\n",
    "    scores = {name: hybrid_score(metrics_dict[name]) for name in metrics_dict}\n",
    "    best_name = max(scores, key=scores.get)\n",
    "    return best_name, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01785ed-147c-4b2c-a20a-390a73558003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 2 - Timeframe: Short ===\n",
      "=== Phase 2 - Timeframe: Swing ===\n",
      "=== Phase 2 - Timeframe: Long ===\n",
      "=== Phase 2 - Timeframe: Daily ===\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "target_stage2 = \"TomorrowClose\"\n",
    "epsilon = 1e-8\n",
    "all_stage2_predictions = []\n",
    "\n",
    "# -----------------------------\n",
    "# Loop over timeframes (Pandas)\n",
    "# -----------------------------\n",
    "for tf, sdf_tf in timeframe_dfs_all.items():  \n",
    "    # Convert Spark DF to Pandas once\n",
    "    df_tf = sdf_tf.toPandas()\n",
    "    \n",
    "    companies = df_tf[\"CompanyId\"].unique()\n",
    "    print(f\"=== Phase 2 - Timeframe: {tf} ===\")\n",
    "    \n",
    "    for cid in companies:\n",
    "        # Filter by company\n",
    "        df_c = df_tf[df_tf[\"CompanyId\"] == cid].copy()\n",
    "        if df_c.empty:\n",
    "            continue\n",
    "        \n",
    "        # 1️⃣ Identify training rows and future rows\n",
    "        train_df = df_c[df_c[target_stage2].notna()].copy()\n",
    "        future_df = df_c[df_c[target_stage2].isna()].copy()\n",
    "        if train_df.empty or future_df.empty:\n",
    "            continue\n",
    "        \n",
    "        # 2️⃣ Log-transform OHLC columns to stabilize variance\n",
    "        for col in [\"Open\", \"High\", \"Low\", \"Close\"]:\n",
    "            df_c[f\"log_{col}\"] = np.log(df_c[col].replace(0, epsilon))\n",
    "        \n",
    "        # 3️⃣ Select numeric & boolean features (excluding target)\n",
    "        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.difference([target_stage2]).tolist()\n",
    "        bool_cols = train_df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "        all_features = numeric_cols + bool_cols\n",
    "        \n",
    "        # 4️⃣ Feature correlation with target\n",
    "        corr = train_df[all_features + [target_stage2]].corr()[target_stage2].abs()\n",
    "        threshold = 0.03\n",
    "        good_features = corr[corr >= threshold].drop(target_stage2).index.tolist()\n",
    "        if not good_features:\n",
    "            continue\n",
    "        \n",
    "        # 5️⃣ Prepare train dataset\n",
    "        X_train = train_df[good_features].fillna(0)\n",
    "        y_train = train_df[target_stage2]\n",
    "        X_future = future_df[good_features].fillna(0)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # 6️⃣ Train models\n",
    "        # -----------------------------\n",
    "        models = {\n",
    "            \"Linear\": LinearRegression(),\n",
    "            \"Lasso\": Lasso(alpha=0.01),\n",
    "            \"Ridge\": Ridge(alpha=1.0, solver=\"svd\"),\n",
    "            \"XGBoost\": XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, verbosity=0)\n",
    "        }\n",
    "        metrics_dict = {}\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            pred_train = model.predict(X_train)\n",
    "            rmse = mean_squared_error(y_train, pred_train, squared=False)\n",
    "            mae = mean_absolute_error(y_train, pred_train)\n",
    "            mape = np.mean(np.abs((y_train - pred_train) / (y_train + epsilon)))\n",
    "            direction = np.mean(np.sign(pred_train[1:] - pred_train[:-1]) == np.sign(y_train.values[1:] - y_train.values[:-1]))\n",
    "            r2 = r2_score(y_train, pred_train)\n",
    "            k = X_train.shape[1]\n",
    "            n = len(y_train)\n",
    "            adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1)) if n - k - 1 != 0 else 0\n",
    "            metrics_dict[name] = {\"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"DirectionAcc\": direction, \"R2\": r2, \"AdjR2\": adj_r2}\n",
    "        \n",
    "        # 7️⃣ Pick best model\n",
    "        #best_name = max(metrics_dict, key=lambda m: hybrid_score(metrics_dict[m]))\n",
    "        #best_model = models[best_name]\n",
    "        \n",
    "        # Pick best model with hybrid score\n",
    "        best_name, scores = select_best_model(metrics_dict)\n",
    "        best_model = models[best_name]\n",
    "\n",
    "        \n",
    "        # 8️⃣ Predict future rows\n",
    "        for name, model in models.items():\n",
    "            future_df[f\"Pred_{name}\"] = model.predict(X_future)\n",
    "        \n",
    "        # Weighted ensemble (inverse RMSE)\n",
    "        total_inv = sum(1 / metrics_dict[m][\"RMSE\"] for m in metrics_dict)\n",
    "        weights = {m: (1 / metrics_dict[m][\"RMSE\"]) / total_inv for m in metrics_dict}\n",
    "        future_df[\"Pred_Sklearn\"] = sum(future_df[f\"Pred_{m}\"] * w for m, w in weights.items())\n",
    "        \n",
    "        # Predicted return\n",
    "        if \"Close\" in future_df.columns:\n",
    "            future_df[\"PredictedReturn_Sklearn\"] = (future_df[\"Pred_Sklearn\"] - future_df[\"Close\"]) / future_df[\"Close\"]\n",
    "        \n",
    "        # Add identifiers & best model info\n",
    "        future_df[\"TimeFrame\"] = tf\n",
    "        future_df[\"CompanyId\"] = cid\n",
    "        future_df[\"BestModel\"] = best_name\n",
    "        future_df[\"BestModel_RMSE\"] = metrics_dict[best_name][\"RMSE\"]\n",
    "        future_df[\"BestModel_MAPE\"] = metrics_dict[best_name][\"MAPE\"]\n",
    "        future_df[\"BestModel_DirAcc\"] = metrics_dict[best_name][\"DirectionAcc\"]\n",
    "        # Raise error if best_name or metrics are missing/null/blank\n",
    "        if (not best_name) or future_df[[\"BestModel_RMSE\",\"BestModel_MAPE\",\"BestModel_DirAcc\"]].isnull().any().any():\n",
    "            raise ValueError(\n",
    "                f\"Missing or null metrics for company {cid}, timeframe {tf}, best_name={best_name} | \"\n",
    "                f\"Metrics: {metrics}\"\n",
    "            )\n",
    "        if future_df.empty or len(good_features) == 0:\n",
    "            print(f\"Skipping {cid}-{tf} | future rows: {len(future_df)}, good features: {len(good_features)}\")\n",
    "            breakpoint()  # or raise ValueError to stop\n",
    "\n",
    "        all_stage2_predictions.append(future_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff593d0-5005-4d2a-8af7-ae0a3a7caaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "top_n_phase2 = 30  # number of top candidates per timeframe\n",
    "# -----------------------------\n",
    "# Combine all Stage 2 predictions into a single Pandas DF\n",
    "# -----------------------------\n",
    "if all_stage2_predictions:\n",
    "    stage2_df = pd.concat(all_stage2_predictions, ignore_index=True)\n",
    "else:\n",
    "    raise ValueError(\"No Stage 2 predictions generated!\")\n",
    "# -----------------------------\n",
    "# Convert Stage 2 Pandas DF → Spark DF\n",
    "# -----------------------------\n",
    "spark_stage2_all = (\n",
    "    spark.createDataFrame(stage2_df)\n",
    "    .withColumn(\"CompanyId\", F.col(\"CompanyId\").cast(\"bigint\"))\n",
    "    .withColumn(\"TimeFrame\", F.trim(F.col(\"TimeFrame\")))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Pick best prediction per CompanyId + TimeFrame\n",
    "# -----------------------------\n",
    "window_comp = Window.partitionBy(\"CompanyId\", \"TimeFrame\").orderBy(F.desc(\"PredictedReturn_Sklearn\"))\n",
    "\n",
    "spark_stage2_best = (\n",
    "    spark_stage2_all\n",
    "    .withColumn(\"row_num\", F.row_number().over(window_comp))\n",
    "    .filter(F.col(\"row_num\") == 1)\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Assign Phase2 rank per timeframe for all rows\n",
    "# -----------------------------\n",
    "window_tf = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"PredictedReturn_Sklearn\"))\n",
    "\n",
    "spark_stage2_ranked = (\n",
    "    spark_stage2_best\n",
    "    .withColumn(\"Phase2_Rank\", F.row_number().over(window_tf))\n",
    ")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "phase2_top_dfs = {}\n",
    "\n",
    "# Columns from Stage2 DF we care about\n",
    "pred_cols = [\n",
    "    \"Pred_Linear\", \"Pred_Lasso\", \"Pred_Ridge\", \"Pred_XGBoost\",\n",
    "    \"Pred_Sklearn\", \"PredictedReturn_Sklearn\",\n",
    "    \"BestModel\", \"BestModel_RMSE\", \"BestModel_MAPE\",\n",
    "    \"BestModel_DirAcc\", \"Phase2_Rank\"\n",
    "]\n",
    "\n",
    "for tf, sdf_tf in timeframe_dfs_all.items():\n",
    "    # Clean historical DF keys\n",
    "    sdf_tf_clean = (\n",
    "        sdf_tf\n",
    "        .withColumn(\"CompanyId\", F.col(\"CompanyId\").cast(\"bigint\"))\n",
    "        .withColumn(\"TimeFrame\", F.trim(F.col(\"TimeFrame\")))\n",
    "    )\n",
    "\n",
    "    # Select only the columns from Stage2 we want\n",
    "    sdf_stage2 = spark_stage2_ranked.select(\n",
    "        [\"CompanyId\", \"TimeFrame\"] + pred_cols\n",
    "    )\n",
    "\n",
    "    # Left join\n",
    "    sdf_enriched = sdf_tf_clean.join(\n",
    "        F.broadcast(sdf_stage2),\n",
    "        on=[\"CompanyId\", \"TimeFrame\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    phase2_top_dfs[tf] = sdf_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6354ace-160f-4cd7-8d47-47d6c6fd0391",
   "metadata": {},
   "source": [
    "spark_stage2_ranked.filter(F.col(\"CompanyId\") == 77864).toPandas().head(20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a675d-1778-4cf4-983d-ed40bce49867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-N filter per timeframe\n",
    "phase2_topN_dfs = {}\n",
    "for tf, sdf in phase2_top_dfs.items():\n",
    "    sdf_topN = sdf.filter(F.col(\"Phase2_Rank\") <= top_n_phase2)\n",
    "    phase2_topN_dfs[tf] = sdf_topN\n",
    "\n",
    "\n",
    "print(f\"✅ Stage 2 completed: Top {top_n_phase2} candidates selected per timeframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f784d3-4955-4402-be12-c52164b8b8cf",
   "metadata": {},
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "for tf, sdf_topN in phase2_topN_dfs.items():\n",
    "    print(f\"TimeFrame = {tf}\")\n",
    "\n",
    "    # Count rows per rank\n",
    "    sdf_topN.groupBy(\"Phase2_Rank\") \\\n",
    "        .agg(F.count(\"*\").alias(\"RowCount\")) \\\n",
    "        .orderBy(\"Phase2_Rank\") \\\n",
    "        .show(50)\n",
    "\n",
    "    # Total rows and distinct companies\n",
    "    total_rows = sdf_topN.count()\n",
    "    distinct_companies = sdf_topN.select(\"CompanyId\").distinct().count()\n",
    "    print(f\"Total rows: {total_rows}, Distinct companies: {distinct_companies}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45784ca4-06c8-4d9b-b885-9c8987f4a192",
   "metadata": {},
   "source": [
    "# Original Stage 2 ranked\n",
    "print(\"spark_stage2_ranked row count:\", spark_stage2_ranked.count())\n",
    "\n",
    "# Each timeframe after Top-N filter\n",
    "for tf, sdf_topN in phase2_topN_dfs.items():\n",
    "    print(f\"TimeFrame={tf} TopN row count: {sdf_topN.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e500d44-26c2-4b9d-93f4-ccf8153381af",
   "metadata": {},
   "source": [
    "total_topN = sum(sdf_topN.count() for sdf_topN in phase2_topN_dfs.values())\n",
    "print(\"Total TopN rows across all timeframes:\", total_topN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650b55f-236c-4458-984f-71808a4ca1a0",
   "metadata": {},
   "source": [
    "tf = \"Daily\"\n",
    "company_id = 77864\n",
    "\n",
    "phase2_topN_dfs[tf].filter(F.col(\"CompanyId\") == company_id).select(\n",
    "    \"CompanyId\", \"TimeFrame\", \"BestModel\",\n",
    "    \"BestModel_RMSE\", \"BestModel_MAPE\", \"BestModel_DirAcc\",\n",
    "    \"PredictedReturn_Sklearn\", \"Phase2_Rank\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3490b-c67b-417f-ae37-3f1119e41fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters\n",
    "# -----------------------------\n",
    "top_n_final = 10\n",
    "sarimax_order = (1,0,0)\n",
    "sarimax_seasonal_order = (0,0,0,0)\n",
    "epsilon = 1e-6\n",
    "ml_weight = 0.6\n",
    "sarimax_weight = 0.4\n",
    "\n",
    "forecast_steps_map = {\n",
    "    \"Daily\": 1,\n",
    "    \"Short\": 3,\n",
    "    \"Swing\": 5,\n",
    "    \"Long\": 10\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def infer_season_length(ts, max_lag=30, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Infer seasonal period `m` from autocorrelation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts : pandas.Series\n",
    "        Time series values\n",
    "    max_lag : int\n",
    "        Maximum lag to inspect for autocorrelation\n",
    "    threshold : float\n",
    "        Minimum autocorrelation to consider a peak as seasonal\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    m : int\n",
    "        Estimated seasonal period\n",
    "    \"\"\"\n",
    "    ts = ts.dropna()\n",
    "    if len(ts) < 2:\n",
    "        return 1  # not enough data to infer\n",
    "    \n",
    "    acf_vals = acf(ts, nlags=max_lag, fft=True)\n",
    "    \n",
    "    # Ignore lag 0\n",
    "    acf_vals[0] = 0\n",
    "    \n",
    "    # Find first lag where autocorrelation exceeds threshold\n",
    "    peaks = np.where(acf_vals > threshold)[0]\n",
    "    \n",
    "    if len(peaks) == 0:\n",
    "        return 1  # no strong seasonality detected\n",
    "    \n",
    "    # Choose the first peak as seasonal period\n",
    "    m = int(peaks[0])\n",
    "    return m\n",
    "\n",
    "\n",
    "import pmdarima as pm\n",
    "\n",
    "# -----------------------------\n",
    "# Phase 3: Loop over companies per timeframe\n",
    "# -----------------------------\n",
    "phase3_results = []\n",
    "\n",
    "for tf, sdf_tf in phase2_topN_dfs.items():\n",
    "    print(f\"=== Phase 3 - Timeframe: {tf} ===\")\n",
    "\n",
    "    # Collect companies\n",
    "    companies = sdf_tf.select(\"CompanyId\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    forecast_horizon = forecast_steps_map.get(tf, 1)\n",
    "    for cid in companies:\n",
    "        # Filter Spark DF once, convert to Pandas\n",
    "        df_c = sdf_tf.filter(F.col(\"CompanyId\") == cid).orderBy(\"StockDate\").toPandas()\n",
    "        if df_c.empty:\n",
    "            continue\n",
    "        \n",
    "        # -----------------------------\n",
    "        # SARIMAX Forecast\n",
    "        # -----------------------------\n",
    "        y = df_c[\"Close\"].replace(0, epsilon)\n",
    "\n",
    "        last_close = y.iloc[-1]\n",
    "\n",
    "        try:\n",
    "            #auto_model = fit_auto_arima(y, seasonal=True, m=7)  # m=7 for weekly seasonality on daily data\n",
    "            m = infer_season_length(y, max_lag=30, threshold=0.3)\n",
    "            '''\n",
    "            auto_model = pm.auto_arima(\n",
    "                y,\n",
    "                start_p=0, start_q=0,  \n",
    "                max_p=2, max_q=2,      # keep small since your system has 6GB RAM (3,3)\n",
    "                d=None,                # let auto_arima decide\n",
    "                start_P=0, start_Q=0,  \n",
    "                max_P=1, max_Q=1,      # was (2,2)\n",
    "                D=None,\n",
    "                m=m,                   # season length (e.g. 7 = weekly seasonality for daily data)\n",
    "                seasonal=True,\n",
    "                stepwise=True,         # faster stepwise search\n",
    "                suppress_warnings=True,\n",
    "                error_action=\"ignore\", # continue even if a model fails\n",
    "                trace=False             # show models it tries (True)\n",
    "            )\n",
    "            '''\n",
    "\n",
    "\n",
    "            \n",
    "            auto_model = pm.auto_arima(\n",
    "                y,\n",
    "                start_p=0, start_q=0,\n",
    "                max_p=2, max_q=2,      # smaller max orders → fewer models\n",
    "                d=None,                # let auto_arima decide\n",
    "                start_P=0, start_Q=0,\n",
    "                max_P=1, max_Q=1,      # smaller seasonal orders\n",
    "                D=None,\n",
    "                m=m,                   # seasonal length\n",
    "                seasonal=True,\n",
    "                stepwise=True,         # enable stepwise search (faster than full search)\n",
    "                max_order=3,           # sum of p+q+P+Q ≤ 3 → reduces combinations\n",
    "                max_d=2,               # restrict differencing search\n",
    "                max_D=1,               # restrict seasonal differencing\n",
    "                n_jobs=1,              # parallel jobs if supported\n",
    "                suppress_warnings=True,\n",
    "                error_action=\"ignore\",\n",
    "                trace=False            # disable verbose output\n",
    "            )\n",
    "          \n",
    "            if auto_model is not None:\n",
    "                # Extract best orders found by auto_arima\n",
    "                sarimax_order = auto_model.order\n",
    "                sarimax_seasonal_order = auto_model.seasonal_order\n",
    "                # Log the AIC\n",
    "                #print(f\"Best auto_arima model: {auto_model.summary()}\")\n",
    "                #print(f\"AIC: {auto_model.aic()}\")\n",
    "                aic=auto_model.aic()\n",
    "                mltype=\"automl\"\n",
    "            else:\n",
    "                sarimax_order = (1,0,0)\n",
    "                sarimax_seasonal_order = (0,0,0,0)\n",
    "                aic=0\n",
    "                mltype=\"sarimax\"\n",
    "\n",
    "                \n",
    "            model = SARIMAX(y,\n",
    "                            order=sarimax_order,\n",
    "                            seasonal_order=sarimax_seasonal_order,\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False)\n",
    "            sarimax_res = model.fit(disp=False)\n",
    "            forecast = sarimax_res.get_forecast(steps=forecast_horizon)\n",
    "            pred_price = forecast.predicted_mean.iloc[-1]\n",
    "            last_close = y.iloc[-1]\n",
    "            sarimax_return = (pred_price - last_close) / last_close\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ SARIMAX failed for {cid}-{tf}: {e}\")\n",
    "            pred_price = last_close\n",
    "            sarimax_return = 0.0\n",
    "        \n",
    "        # -----------------------------\n",
    "        # ML Prediction from existing Phase 2 columns\n",
    "        # -----------------------------\n",
    "        ml_return = df_c[\"MaxPredictedReturn\"].iloc[0] if \"MaxPredictedReturn\" in df_c.columns else 0.0\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Weighted score\n",
    "        # -----------------------------\n",
    "        weighted_score = ml_weight * ml_return + sarimax_weight * sarimax_return\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Store enriched data\n",
    "        # -----------------------------\n",
    "        df_c[\"SARIMAX_PredictedClose\"] = pred_price\n",
    "        df_c[\"SARIMAX_PredictedReturn\"] = sarimax_return\n",
    "        df_c[\"WeightedScore\"] = weighted_score\n",
    "        df_c[\"AIC\"] = aic\n",
    "        df_c[\"MlType\"] = mltype\n",
    "        phase3_results.append(df_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d05b67-0199-4f30-8904-9423e63958d5",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "phase3_results = []\n",
    "\n",
    "for tf, sdf_tf in phase2_topN_dfs.items():  # Use top-N Phase2\n",
    "    print(f\"=== Phase 3 - Timeframe: {tf} ===\")\n",
    "\n",
    "    # Collect distinct companies\n",
    "    companies = sdf_tf.select(\"CompanyId\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    forecast_horizon = forecast_steps_map.get(tf, 1)\n",
    "\n",
    "    for cid in companies:\n",
    "        # Convert company data to Pandas\n",
    "        df_c = sdf_tf.filter(F.col(\"CompanyId\") == cid).orderBy(\"StockDate\").toPandas()\n",
    "        if df_c.empty:\n",
    "            continue\n",
    "\n",
    "        y = df_c[\"Close\"]\n",
    "\n",
    "        try:\n",
    "            # -----------------------------\n",
    "            # SARIMAX forecast\n",
    "            # -----------------------------\n",
    "            model = SARIMAX(\n",
    "                y,\n",
    "                order=sarimax_order,\n",
    "                seasonal_order=sarimax_seasonal_order,\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False\n",
    "            )\n",
    "            sarimax_res = model.fit(disp=False)\n",
    "            forecast = sarimax_res.get_forecast(steps=forecast_horizon)\n",
    "            pred_prices = forecast.predicted_mean\n",
    "\n",
    "            # Forecast dates\n",
    "            last_date = df_c[\"StockDate\"].max()\n",
    "            forecast_dates = [last_date + pd.Timedelta(days=i) for i in range(1, forecast_horizon + 1)]\n",
    "\n",
    "            # Forecasted returns\n",
    "            sarimax_returns = pred_prices.pct_change().fillna((pred_prices.iloc[0] - y.iloc[-1]) / y.iloc[-1])\n",
    "\n",
    "            # Create forecast DataFrame\n",
    "            df_forecast = pd.DataFrame({\n",
    "                \"StockDate\": forecast_dates,\n",
    "                \"SARIMAX_PredictedClose\": pred_prices,\n",
    "                \"SARIMAX_PredictedReturn\": sarimax_returns,\n",
    "                \"CompanyId\": cid,\n",
    "                \"TimeFrame\": tf\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ SARIMAX failed for {cid}-{tf}: {e}\")\n",
    "            df_forecast = pd.DataFrame({\n",
    "                \"StockDate\": [df_c[\"StockDate\"].max() + pd.Timedelta(days=i) for i in range(1, forecast_horizon + 1)],\n",
    "                \"SARIMAX_PredictedClose\": [y.iloc[-1]] * forecast_horizon,\n",
    "                \"SARIMAX_PredictedReturn\": [0.0] * forecast_horizon,\n",
    "                \"CompanyId\": cid,\n",
    "                \"TimeFrame\": tf\n",
    "            })\n",
    "\n",
    "        # -----------------------------\n",
    "        # ML Prediction (from Phase2 top-N)\n",
    "        # -----------------------------\n",
    "        ml_return = df_c[\"MaxPredictedReturn\"].iloc[0] if \"MaxPredictedReturn\" in df_c.columns else 0.0\n",
    "\n",
    "        # Weighted score\n",
    "        weighted_score = ml_weight * ml_return + sarimax_weight * df_forecast[\"SARIMAX_PredictedReturn\"]\n",
    "\n",
    "        df_forecast[\"WeightedScore\"] = weighted_score\n",
    "        df_forecast[\"MlType\"] = mltype  # Optional metadata\n",
    "\n",
    "        # Append historical + forecast\n",
    "        df_c[\"SARIMAX_PredictedClose\"] = df_c[\"Close\"]  # Historical SARIMAX = actual\n",
    "        df_c[\"SARIMAX_PredictedReturn\"] = 0.0\n",
    "        df_c[\"WeightedScore\"] = ml_weight * ml_return  # Historical weighted score\n",
    "        df_c[\"MlType\"] = mltype\n",
    "\n",
    "        df_combined = pd.concat([df_c, df_forecast], ignore_index=True)\n",
    "        phase3_results.append(df_combined)\n",
    "\n",
    "# Combine all companies/timeframes\n",
    "df_phase3_enriched = pd.concat(phase3_results, ignore_index=True)\n",
    "print(df_phase3_enriched.tail(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e79a5-72bb-4f08-b456-ea211c3e137f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dbbc4d3-2a30-4099-aa82-4ea7e6e3b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 23:18:12 WARN TaskSetManager: Stage 2512 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/20 23:18:26 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`bsf`.`final_phase3_enriched` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/09/20 23:18:26 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/09/20 23:18:27 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/09/20 23:18:27 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/09/20 23:18:27 WARN HiveConf: HiveConf of name hive.metastore.client.connect.timeout does not exist\n",
      "25/09/20 23:18:27 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/09/20 23:18:32 WARN TaskSetManager: Stage 2525 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/20 23:18:40 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`bsf`.`final_topn_selected` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/09/20 23:18:43 WARN TaskSetManager: Stage 2541 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/20 23:18:45 WARN TaskSetManager: Stage 2547 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/09/20 23:18:49 WARN TaskSetManager: Stage 2553 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short: Final top N = 14831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 23:18:53 WARN TaskSetManager: Stage 2559 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swing: Final top N = 15873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 23:18:57 WARN TaskSetManager: Stage 2565 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long: Final top N = 15210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/20 23:19:01 WARN TaskSetManager: Stage 2571 contains a task of very large size (1231 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily: Final top N = 14909\n",
      "✅ Stage 3 completed: Latest rows per company + top 10 candidates selected per timeframe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Combine all companies/timeframes\n",
    "# -----------------------------\n",
    "df_phase3_full = pd.concat(phase3_results, ignore_index=True)\n",
    "df_phase3_spark = spark.createDataFrame(df_phase3_full)\n",
    "\n",
    "# -----------------------------\n",
    "# Enrich with company info\n",
    "# -----------------------------\n",
    "sdf_company = spark.table(\"bsf.company\").select(\"CompanyId\",\"TradingSymbol\",\"Name\")\n",
    "df_phase3_enriched = df_phase3_spark.join(F.broadcast(sdf_company), on=\"CompanyId\", how=\"left\")\n",
    "\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Generate a timestamp string\n",
    "# -----------------------------\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "df_phase3_enriched = df_phase3_enriched.withColumn(\"RunTimestamp\", F.lit(timestamp))\n",
    "cols = [\"RunTimestamp\"] + [c for c in df_phase3_enriched.columns if c != \"RunTimestamp\"]\n",
    "df_phase3_enriched = df_phase3_enriched.select(cols)\n",
    "\n",
    "\n",
    "# 3️⃣ Save to Delta\n",
    "db='bsf'\n",
    "table='final_phase3_enriched'\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {db}.{table}\")\n",
    "# Correct save\n",
    "df_phase3_enriched.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{db}.{table}\")\n",
    "\n",
    "# 1️⃣ Keep only the latest row per company + timeframe\n",
    "window_last = Window.partitionBy(\"CompanyId\", \"TimeFrame\").orderBy(F.desc(\"StockDate\"))\n",
    "df_latest_per_company = (\n",
    "    df_phase3_enriched\n",
    "    .withColumn(\"rn\", F.row_number().over(window_last))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# 2️⃣ Rank companies by WeightedScore per timeframe\n",
    "window_tf = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"WeightedScore\"))\n",
    "df_topN_companies = (\n",
    "    df_latest_per_company\n",
    "    .withColumn(\"Phase3_Rank\", F.row_number().over(window_tf))\n",
    "    .filter(F.col(\"Phase3_Rank\") <= top_n_final)\n",
    "    .drop(\"Phase3_Rank\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3️⃣ Save to Delta\n",
    "db='bsf'\n",
    "table='final_topn_selected'\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {db}.{table}\")\n",
    "\n",
    "df_topN_companies.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{db}.{table}\")\n",
    "\n",
    "# csv for review\n",
    "output_path = spark.conf.get(\"spark.sql.filesource.path\", \"/srv/lakehouse/nond2rd\")\n",
    "df_topN_companies.toPandas().to_csv(\n",
    "    f\"{output_path}/{table}.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Create dict by timeframe for plotting\n",
    "# -----------------------------\n",
    "phase3_top_dfs = {\n",
    "    tf: df_phase3_enriched.filter(F.col(\"TimeFrame\") == tf)\n",
    "    for tf in df_phase3_enriched.select(\"TimeFrame\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "}\n",
    "# -----------------------------\n",
    "# Optional: show counts\n",
    "# -----------------------------\n",
    "for tf in timeframes:\n",
    "    print(f\"{tf}: Final top N = {phase3_top_dfs[tf].count()}\")\n",
    "\n",
    "print(f\"✅ Stage 3 completed: Latest rows per company + top {top_n_final} candidates selected per timeframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cc18ae7-0c4f-4cce-8242-62ef11b89196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stage 4 completed: Confirm write to delta table!!\n"
     ]
    }
   ],
   "source": [
    "# Read Delta table\n",
    "df_latest_spark = spark.table(f\"{db}.{table}\")\n",
    "\n",
    "# Convert to Pandas\n",
    "df_latest_pdf = df_latest_spark.toPandas()\n",
    "\n",
    "# Option 1: Show all records (careful if large)\n",
    "#print(df_latest_pdf)\n",
    "\n",
    "# Option 2: Show grouped by timeframe\n",
    "#for tf, group in df_latest_pdf.groupby(\"TimeFrame\"):\n",
    "#    print(f\"\\n=== TimeFrame: {tf} ===\")\n",
    "#    display(group.sort_values(\"WeightedScore\", ascending=False))\n",
    "print(f\"✅ Stage 4 completed: Confirm write to delta table!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce048994-4866-431a-b35f-9ac40e60b63c",
   "metadata": {},
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9c85a-d5e4-4dc3-a0be-cccf40607ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
