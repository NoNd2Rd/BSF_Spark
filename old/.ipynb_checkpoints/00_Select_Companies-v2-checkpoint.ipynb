{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9994d80-88c1-4800-be8f-0d32386259ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsf_env import init_spark, init_mariadb_engine,set_spark_verbosity\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "spark = init_spark(\"bsf_candidates_analysis\", log_level=\"WARN\", show_progress=False, enable_ui=True, priority=False)\n",
    "engine = init_mariadb_engine()\n",
    "\n",
    "ingest_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", 200)         # Adjust width for readability\n",
    "pd.set_option(\"display.max_rows\", 20)       # Show only top 20 rows by default\n",
    "\n",
    "df_last = spark.table(\"bsf.daily_signals_last_allcol \")\n",
    "df_all = spark.table(\"bsf.daily_signals\")\n",
    "\n",
    "#df_all.groupBy(\"Action\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "#print(df_all.groupBy(\"TimeFrame\", \"Action\") \\\n",
    "#  .count() \\\n",
    "#  .orderBy(\"TimeFrame\", F.desc(\"count\")) \\\n",
    "#  .show(truncate=False))\n",
    "#from pyspark import StorageLevel\n",
    "\n",
    "#df = df_last.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "\n",
    "#df = df_last.cache()\n",
    "#df.count()   # forces Spark to actually materialize & cache\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregate Buy/Sell/Hold counts per company per timeframe\n",
    "# -----------------------------\n",
    "df_counts = df_last.groupBy(\"CompanyId\", \"TimeFrame\").agg(\n",
    "    F.sum(F.when(F.col(\"Action\") == \"Buy\", 1).otherwise(0)).alias(\"BuyCount\"),\n",
    "    F.sum(F.when(F.col(\"Action\") == \"Sell\", 1).otherwise(0)).alias(\"SellCount\"),\n",
    "    F.sum(F.when(F.col(\"Action\") == \"Hold\", 1).otherwise(0)).alias(\"HoldCount\"),\n",
    "    F.sum(\"Return\").alias(\"Return\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Define window partitioned by timeframe\n",
    "# -----------------------------\n",
    "w_buy = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"BuyCount\"))\n",
    "w_sell = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"SellCount\"))\n",
    "w_hold = Window.partitionBy(\"TimeFrame\").orderBy(F.desc(\"HoldCount\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Add separate rank columns\n",
    "# -----------------------------\n",
    "df_ranked = (\n",
    "    df_counts\n",
    "    .withColumn(\"BuyRank\", F.row_number().over(w_buy))\n",
    "    .withColumn(\"SellRank\", F.row_number().over(w_sell))\n",
    "    .withColumn(\"HoldRank\", F.row_number().over(w_hold))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Select what you want\n",
    "# -----------------------------\n",
    "ranked_companies = df_ranked.select(\n",
    "    \"CompanyId\", \"TimeFrame\", \"BuyCount\", \"SellCount\", \"HoldCount\", \n",
    "    \"Return\", \"BuyRank\", \"SellRank\", \"HoldRank\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Join back to the original df to get full rows with rank by last return\n",
    "# -----------------------------\n",
    "df_ranked_last = df_last.join(ranked_companies, on=[\"CompanyId\", \"TimeFrame\"], how=\"inner\") # adds the sellcount etc from the ranked_company df\n",
    "df_ranked_all = df_all.join(\n",
    "    ranked_companies.select(\"CompanyId\", \"TimeFrame\"), \n",
    "    on=[\"CompanyId\", \"TimeFrame\"], \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "# --- Filter last-ranked DF to top 20 ---\n",
    "df_ranked_last_top = df_ranked_last.filter(F.col(\"BuyRank\") <= top_n)\n",
    "\n",
    "# --- Get the top 20 companies per timeframe ---\n",
    "top_companies = df_ranked_last_top.select(\"CompanyId\", \"TimeFrame\").distinct()\n",
    "\n",
    "# --- Filter all historical DF to only include top 20 companies ---\n",
    "df_ranked_last_topN = df_ranked_last.join(top_companies, on=[\"CompanyId\", \"TimeFrame\"], how=\"inner\")\n",
    "df_ranked_all_topN = df_ranked_all.join(top_companies, on=[\"CompanyId\", \"TimeFrame\"], how=\"inner\")\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbf8f8-48eb-46ea-a592-49950d860fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_ranked_last_top.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db818d4-e9f8-4868-ae9c-f3c074b9bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53ee05-c3d1-43f2-933d-e25b68370e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of timeframes\n",
    "timeframes = [\"Short\", \"Swing\", \"Long\", \"Daily\"]\n",
    "top_n = 20  # number of top-ranked companies per timeframe\n",
    "\n",
    "# --- Filter top N per timeframe for last-ranked DF ---\n",
    "timeframe_dfs = {}\n",
    "timeframe_dfs_all = {}\n",
    "\n",
    "for tf in timeframes:\n",
    "    # All rows for this timeframe\n",
    "    sdf_all = df_ranked_all.filter((F.col(\"TimeFrame\") == tf) & (F.col(\"BuyRank\") <= top_n))\n",
    "    timeframe_dfs_all[tf] = sdf_all\n",
    "    \n",
    "    # Top N rows for this timeframe (using BuyRank)\n",
    "    sdf_top = df_ranked_last.filter((F.col(\"TimeFrame\") == tf) & (F.col(\"BuyRank\") <= top_n))\n",
    "    timeframe_dfs[tf] = sdf_top\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf4912-6641-4ee6-aa6a-554bb39d6ba3",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Convert to Pandas and save as csv\n",
    "# -----------------------------\n",
    "#List of timeframes\n",
    "timeframes = [\"Short\", \"Swing\", \"Long\", \"Daily\"]\n",
    "\n",
    "for tf in timeframes:\n",
    "    df_name = f\"pdf_{tf.lower()}\"  # e.g., \"pdf_short\"\n",
    "    df_name_all = f\"pdf_{tf.lower()}_all\"  # e.g., \"pdf_short_all\"\n",
    "    globals()[df_name] = df_ranked_last.filter(F.col(\"TimeFrame\") == tf)\n",
    "    globals()[df_name_all] = df_ranked_all.filter(F.col(\"TimeFrame\") == tf)\n",
    "    #ranked_rows.filter(F.col(\"TimeFrame\") == tf).toPandas().to_csv(f\"cvs/{tf.lower()}_output.csv\", index=False)\n",
    "\n",
    "# pdf_short = ranked_rows.filter(F.col(\"TimeFrame\") == \"Short\").toPandas()\n",
    "# pdf_short.to_csv(f\"cvs/short_output.csv\", index=False)\n",
    "\n",
    "timeframe_dfs = {\n",
    "    \"Short\": pdf_short,\n",
    "    \"Swing\": pdf_swing,\n",
    "    \"Long\": pdf_long,\n",
    "    \"Daily\": pdf_daily\n",
    "}\n",
    "timeframe_dfs_all = {\n",
    "    \"Short\": pdf_short_all,\n",
    "    \"Swing\": pdf_swing_all,\n",
    "    \"Long\": pdf_long_all,\n",
    "    \"Daily\": pdf_daily_all\n",
    "}\n",
    "\n",
    "\n",
    "# Store Spark DFs only (no toPandas here)\n",
    "timeframe_dfs = {tf: df_ranked_last.filter(F.col(\"TimeFrame\") == tf) for tf in timeframes}\n",
    "timeframe_dfs_all = {tf: df_ranked_all.filter(F.col(\"TimeFrame\") == tf) for tf in timeframes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e75a5-ef03-4e5c-a3b5-1fdab94bdc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_ranked_last.filter(F.col(\"BuyRank\") <= 20).orderBy(\"TimeFrame\", \"BuyRank\").toPandas())\n",
    "print(\"✅ Stage 1 completed: Top 20 candidates selected per timeframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08253949-c321-4d73-9b8e-39766384188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeframe_dfs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a7298-8d97-4ced-b9c6-9d1fac7fcb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pycaret.regression import setup, compare_models, predict_model, finalize_model\n",
    "from pycaret.regression import setup, create_model, tune_model, finalize_model, predict_model\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#full\n",
    "# -------------------------\n",
    "# Full Stage 2 → Stage 3 Pipeline\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pycaret.regression import setup, compare_models, predict_model, finalize_model\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# -------------------------\n",
    "# Stage 2: Predict TomorrowClose Regression\n",
    "# -------------------------\n",
    "target_stage2 = \"TomorrowClose\"\n",
    "epsilon = 1e-6\n",
    "all_stage2_predictions = []\n",
    "top_n=5\n",
    "# forecast steps per timeframe for Stage 2\n",
    "forecast_steps_map = {\n",
    "    \"Daily\": 1,\n",
    "    \"Short\": 3,\n",
    "    \"Swing\": 5,\n",
    "    \"Long\": 10\n",
    "}\n",
    "# with memory issues may want to limit\n",
    "pycaret_models = [\"lr\", \"lasso\", \"ridge\", \"en\"]\n",
    "pycaret_models = [\"lasso\"]\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loop over timeframes with tqdm\n",
    "for tf, sdf_tf in tqdm(timeframe_dfs_all.items(), desc=\"TimeFrames\"):\n",
    "    pdf_tf = sdf_tf.toPandas()\n",
    "    companies = pdf_tf['CompanyId'].unique()\n",
    "    \n",
    "    # Inner loop over companies with tqdm\n",
    "    for cid in tqdm(companies, desc=f\"Companies ({tf})\", leave=False):\n",
    "        df_c = pdf_tf[pdf_tf['CompanyId'] == cid].copy()\n",
    "\n",
    "\n",
    "# Loop over timeframes\n",
    "for tf, sdf_tf in timeframe_dfs_all.items():\n",
    "    pdf_tf = sdf_tf.toPandas()\n",
    "    companies = pdf_tf['CompanyId'].unique()\n",
    "    print(f\"\\n=== Phase 2 - Processing timeframe: {tf} ===\")\n",
    "    # Loop over companies\n",
    "    for cid in companies:\n",
    "        df_c = pdf_tf[pdf_tf['CompanyId'] == cid].copy()\n",
    "'''   \n",
    "for tf, sdf_tf in timeframe_dfs_all.items():\n",
    "    pdf_tf = sdf_tf.toPandas()\n",
    "    print(f\"\\n=== Phase 2 - Processing top 20 for timeframe: {tf} ===\")\n",
    "    companies = pdf_tf['CompanyId'].unique()\n",
    "    # Loop over companies\n",
    "    for cid in companies:\n",
    "        df_c = pdf_tf[pdf_tf['CompanyId'] == cid].copy()\n",
    "    \n",
    "        # -------------------------------\n",
    "        # Log-transform OHLC to normalize scale\n",
    "        # -------------------------------\n",
    "        for col in [\"Open\",\"High\",\"Low\",\"Close\"]:\n",
    "            df_c[f\"log_{col}\"] = np.log(df_c[col].replace(0, epsilon))\n",
    "\n",
    "        # -------------------------------\n",
    "        # Training data: rows where target is known\n",
    "        # -------------------------------\n",
    "        train_df = df_c[df_c[target_stage2].notna()].copy()\n",
    "        if train_df.empty:\n",
    "            print(f\"skipped company {cid} no train_df\")\n",
    "            continue\n",
    "\n",
    "        # -------------------------------\n",
    "        # Feature selection: numeric columns correlated with target\n",
    "        # -------------------------------\n",
    "        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if target_stage2 in numeric_cols:\n",
    "            numeric_cols.remove(target_stage2)\n",
    "        # Automatically pick numeric and boolean columns\n",
    "        bool_cols = train_df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "        \n",
    "        all_features = numeric_cols + bool_cols\n",
    "\n",
    "        corr = train_df[all_features + [target_stage2]].corr()[target_stage2].abs()\n",
    "        threshold = 0.03  # minimal correlation\n",
    "        good_features = corr[corr >= threshold].index.tolist()\n",
    "\n",
    "        X_train = train_df[good_features].fillna(0)\n",
    "        y_train = train_df[target_stage2]\n",
    "\n",
    "        # -------------------------------\n",
    "        # Stage 1 models\n",
    "        # -------------------------------\n",
    "        lr_model = LinearRegression().fit(X_train, y_train)\n",
    "        lasso_model = Lasso(alpha=0.01).fit(X_train, y_train)\n",
    "        ridge_model = Ridge(alpha=1.0, solver=\"svd\").fit(X_train, y_train)\n",
    "        \n",
    "        lr_rmse = mean_squared_error(y_train, lr_model.predict(X_train), squared=False)\n",
    "        lasso_rmse = mean_squared_error(y_train, lasso_model.predict(X_train), squared=False)\n",
    "        ridge_rmse = mean_squared_error(y_train, ridge_model.predict(X_train), squared=False)\n",
    "        \n",
    "        total_inv = 1/lr_rmse + 1/lasso_rmse + 1/ridge_rmse\n",
    "        weights = {\n",
    "            \"Linear\": (1/lr_rmse)/total_inv,\n",
    "            \"Lasso\": (1/lasso_rmse)/total_inv,\n",
    "            \"Ridge\": (1/ridge_rmse)/total_inv\n",
    "        }\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Predict future rows (target is NaN)\n",
    "        # -------------------------------\n",
    "        future_df = df_c[df_c[target_stage2].isna()].copy()  ##Predicts just tomorrowclose=nan which is the last records in the df so it should be 1 day\n",
    "        if not future_df.empty:\n",
    "            X_future = future_df[good_features].fillna(0)\n",
    "            \n",
    "            future_df[\"Pred_Linear\"] = lr_model.predict(X_future)\n",
    "            future_df[\"Pred_Lasso\"] = lasso_model.predict(X_future)\n",
    "            future_df[\"Pred_Ridge\"] = ridge_model.predict(X_future)\n",
    "            # Weighted prediction\n",
    "            future_df[\"Pred_Sklearn\"] = (\n",
    "                future_df[\"Pred_Linear\"] * weights[\"Linear\"] +\n",
    "                future_df[\"Pred_Lasso\"] * weights[\"Lasso\"] +\n",
    "                future_df[\"Pred_Ridge\"] * weights[\"Ridge\"]\n",
    "            )\n",
    "\n",
    "            future_df[\"PredictedReturn_Sklearn\"] = (future_df[\"Pred_Sklearn\"] - future_df[\"Close\"]) / future_df[\"Close\"]\n",
    "            # -------------------------------\n",
    "            # PyCaret regression embedded\n",
    "            # -------------------------------\n",
    "            '''\n",
    "            try:\n",
    "                # PyCaret setup (on train_df only)\n",
    "                train_df_clean = train_df.dropna(subset=[target_stage2])\n",
    "\n",
    "                pycaret_exp = setup(\n",
    "                    data=train_df_clean[all_features + [target_stage2]],\n",
    "                    target=target_stage2,\n",
    "                    session_id=42,\n",
    "                    log_experiment=False,   # ✅ manual MLflow control\n",
    "                    #html=False,\n",
    "                    #verbose=False\n",
    "                )\n",
    "                \n",
    "                best_model = compare_models(verbose=False)\n",
    "                final_model = finalize_model(best_model)\n",
    "                \n",
    "                pycaret_preds = predict_model(final_model, data=future_df[all_features])\n",
    "                # Add PyCaret prediction to future_df\n",
    "                future_df[\"Pred_PyCaret\"] = pycaret_preds[\"prediction_label\"]\n",
    "                future_df[\"PredictedReturn_PyCaret\"] = (future_df[\"Pred_PyCaret\"] - future_df[\"Close\"]) / future_df[\"Close\"]\n",
    "                  \n",
    "            except Exception as e:\n",
    "                print(f\"PyCaret failed for {cid} {tf}: {e}\")\n",
    "                future_df[\"Pred_PyCaret\"] = float(\"nan\") \n",
    "                future_df[\"PredictedReturn_PyCaret\"] = float(\"nan\") \n",
    "            '''\n",
    "            train_df_clean = train_df.dropna(subset=[target_stage2])\n",
    "            s = setup(\n",
    "                data=train_df_clean[all_features + [target_stage2]],\n",
    "                target=target_stage2,\n",
    "                session_id=42,\n",
    "                log_experiment=False,   # ✅ manual MLflow control\n",
    "                html=False,\n",
    "                verbose=False\n",
    "            )\n",
    "            for model_name in pycaret_models:\n",
    "                    try:\n",
    "                        model = create_model(model_name, fold=2)\n",
    "                        tuned = tune_model(model, fold=2, optimize=\"MAE\", n_iter=3)\n",
    "                        final = finalize_model(tuned)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Tuning failed for {model_name} {cid}-{tf}: {e}\")\n",
    "                        # fallback to untuned\n",
    "                        final = finalize_model(model)\n",
    "\n",
    "                    pycaret_preds = predict_model(final, data=future_df[all_features])\n",
    "                    #pred_col = next(\n",
    "                        #(c for c in [\"Label\", \"prediction_label\", \"prediction\"] if c in pycaret_preds.columns),\n",
    "                        #None\n",
    "                    #)\n",
    "                    #mean_pred = preds[pred_col].mean()     \n",
    "                    future_df[\"Pred_PyCaret\"] = pycaret_preds[\"prediction_label\"]\n",
    "                    future_df[\"PredictedReturn_PyCaret\"] = (future_df[\"Pred_PyCaret\"] - future_df[\"Close\"]) / future_df[\"Close\"]\n",
    "                    \n",
    "    \n",
    "            #future_df[\"Pred_PyCaret\"] = float(\"nan\") \n",
    "            #future_df[\"PredictedReturn_PyCaret\"] = float(\"nan\") \n",
    "\n",
    "            \n",
    "            future_df[\"TimeFrame\"] = tf\n",
    "            future_df[\"CompanyId\"] = cid\n",
    "            all_stage2_predictions.append(future_df)\n",
    "\n",
    "# -------------------------------\n",
    "# Combine Stage 1 predictions\n",
    "# -------------------------------\n",
    "if all_stage2_predictions:\n",
    "    stage2_df = pd.concat(all_stage2_predictions, ignore_index=True)\n",
    "else:\n",
    "    stage2_df = pd.DataFrame()\n",
    "    print(\"No predictions generated.\")\n",
    "    \n",
    "top_list = []\n",
    "\n",
    "for tf in stage2_df[\"TimeFrame\"].unique():\n",
    "    tf_df = stage2_df[stage2_df[\"TimeFrame\"] == tf].copy()\n",
    "    \n",
    "    # Take the higher of the two predicted returns\n",
    "    tf_df[\"MaxPredictedReturn\"] = tf_df[[\"PredictedReturn_Sklearn\", \"PredictedReturn_PyCaret\"]].max(axis=1)\n",
    "    \n",
    "    # Sort by this max predicted return\n",
    "    tf_df = tf_df.sort_values(\"MaxPredictedReturn\", ascending=False)\n",
    "    \n",
    "    # Take top N\n",
    "    top_list.append(tf_df.head(top_n))\n",
    "\n",
    "# Concatenate top stocks across all timeframes\n",
    "stage2_top_df = pd.concat(top_list, ignore_index=True)\n",
    "\n",
    "# Optional: keep only relevant columns\n",
    "cols_to_keep = [\"TimeFrame\", \"CompanyId\", \"Close\",\n",
    "                \"Pred_Sklearn\", \"PredictedReturn_Sklearn\",\n",
    "                \"Pred_PyCaret\", \"PredictedReturn_PyCaret\", \"MaxPredictedReturn\"]\n",
    "stage2_top_df = stage2_top_df[cols_to_keep]\n",
    "\n",
    "print(\"\\n=== Stage 2 Top Predictions per Timeframe ===\")\n",
    "print(stage2_top_df[[\"TimeFrame\", \"CompanyId\", \"Close\", \"Pred_Sklearn\", \"Pred_PyCaret\", \"MaxPredictedReturn\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc74091-8a08-431d-a117-0224fd983d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5e0dd-9c2a-494f-a0b5-d514c5c1149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# -------------------------------\n",
    "# Top-N selection per timeframe (using average of Linear/Lasso/Ridge)\n",
    "# -------------------------------\n",
    "if not stage2_df.empty:\n",
    "    top_list = []\n",
    "    for tf in stage2_df[\"TimeFrame\"].unique():\n",
    "        tf_df = stage2_df[stage2_df[\"TimeFrame\"] == tf].copy()  \n",
    "        tf_df = tf_df.sort_values(\"PredictedTomorrowClose\", ascending=False)\n",
    "        top_list.append(tf_df.head(top_n))\n",
    "    \n",
    "    stage2_top_df = pd.concat(top_list, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n=== Stage 2 Top Predictions per Timeframe ===\")\n",
    "    print(stage2_top_df[[\"TimeFrame\", \"CompanyId\", \"Close\", \"PredictedTomorrowClose\", \"PredictedReturn\"]])\n",
    "\n",
    "else:\n",
    "    stage2_top_df = pd.DataFrame()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83348c5-6d9f-4cf9-8bbd-da0d6a20b271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbcc44-5d5a-4375-894c-bfb80c80c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Phase 2: SARIMAX + PyCaret (optimized)\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pycaret.regression import setup, create_model, tune_model, finalize_model, predict_model\n",
    "\n",
    "# Stage 2 targets\n",
    "target_stage1 = \"PredictedTomorrowClose\"\n",
    "target_stage2 = \"TomorrowReturn\"\n",
    "\n",
    "# Forecast steps per timeframe\n",
    "forecast_steps_map = {\n",
    "    \"Daily\": 1,\n",
    "    \"Short\": 3,\n",
    "    \"Swing\": 5,\n",
    "    \"Long\": 10\n",
    "}\n",
    "\n",
    "# Option 1: Use all Stage 1 predictions\n",
    "combined_top_df_clean = stage1_df.fillna(0)\n",
    "\n",
    "# Option 2: Use only the top-N per timeframe\n",
    "combined_top_df_clean = stage1_top_df.fillna(0)\n",
    "\n",
    "# Numeric features (exclude targets)\n",
    "stage2_features = [c for c in combined_top_df_clean.select_dtypes(include=[np.number]).columns \n",
    "                   if c not in [target_stage1, target_stage2]]\n",
    "\n",
    "# -------------------------\n",
    "# Phase 2: SARIMAX + PyCaret (optimized)\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pycaret.regression import setup, create_model, tune_model, finalize_model, predict_model\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8001\")\n",
    "mlflow.set_experiment(\"Stage2_SARIMAX_PyCaret\")\n",
    "\n",
    "sarimax_results = []\n",
    "pycaret_results = []\n",
    "\n",
    "# with memory issues may want to limit\n",
    "pycaret_models = [\"lr\", \"lasso\", \"ridge\", \"en\"]\n",
    "pycaret_models = [\"lasso\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for tf, steps in forecast_steps_map.items():\n",
    "    df_tf = timeframe_dfs_all[tf].toPandas().copy()\n",
    "\n",
    "    top_companies = combined_top_df_clean.loc[\n",
    "        combined_top_df_clean[\"TimeFrame\"] == tf, \"CompanyId\"\n",
    "    ].unique()\n",
    "\n",
    "    for cid in top_companies:\n",
    "        df_c = df_tf[df_tf[\"CompanyId\"] == cid].copy().dropna(subset=[target_stage2])\n",
    "\n",
    "        if df_c.empty or len(df_c) < 120:\n",
    "            print(f\"⏭️ Skipping {cid}-{tf} (not enough data)\")\n",
    "            continue\n",
    "\n",
    "        ts = df_c[target_stage2]\n",
    "\n",
    "        # -------------------------\n",
    "        # SARIMAX\n",
    "        # -------------------------\n",
    "        try:\n",
    "            sarimax_entry = {\"CompanyId\": cid, \"TimeFrame\": tf}\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"SARIMAX_{cid}_{tf}\"):\n",
    "                # Example fixed order (replace with auto_arima search results if you have them)\n",
    "                order = (1, 1, 1)\n",
    "                seasonal_order = (0, 1, 1, 7)\n",
    "\n",
    "                sarimax_model = SARIMAX(\n",
    "                    ts, order=order, seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False\n",
    "                )\n",
    "                sarimax_fit = sarimax_model.fit(disp=False)\n",
    "\n",
    "                forecast = sarimax_fit.get_forecast(steps=steps)\n",
    "                mean_pred = forecast.predicted_mean.mean()\n",
    "                '''\n",
    "                sarimax_entry[\"Pred_SARIMAX\"] = mean_pred\n",
    "                mlflow.log_metric(\"MeanPred_SARIMAX\", mean_pred)\n",
    "\n",
    "                # Log params\n",
    "                mlflow.log_params({\"order\": order, \"seasonal_order\": seasonal_order})\n",
    "\n",
    "            sarimax_results.append(sarimax_entry)\n",
    "            '''\n",
    "                sarimax_entry = {\n",
    "                    \"CompanyId\": cid,\n",
    "                    \"TimeFrame\": tf,\n",
    "                    \"Pred_SARIMAX\": float(mean_pred),  # ensure scalar\n",
    "                    \"order\": str(order),\n",
    "                    \"seasonal_order\": str(seasonal_order)\n",
    "                }\n",
    "                mlflow.log_metric(\"MeanPred_SARIMAX\", mean_pred)\n",
    "                mlflow.log_params({\"order\": order, \"seasonal_order\": seasonal_order})\n",
    "            \n",
    "                # save the model object as an artifact\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    model_path = os.path.join(tmpdir, \"sarimax_model.pkl\")\n",
    "                    joblib.dump(sarimax_fit, model_path)\n",
    "                    mlflow.log_artifact(model_path, name=\"SARIMAX_model\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ SARIMAX failed for {cid}-{tf}: {e}\")\n",
    "\"\"\"\n",
    "for tf, steps in forecast_steps_map.items():\n",
    "    df_tf = timeframe_dfs_all[tf].toPandas().copy()\n",
    "\n",
    "    top_companies = combined_top_df_clean.loc[\n",
    "        combined_top_df_clean[\"TimeFrame\"] == tf, \"CompanyId\"\n",
    "    ].unique()\n",
    "\n",
    "    for cid in top_companies:\n",
    "        df_c = df_tf[df_tf[\"CompanyId\"] == cid].copy().dropna(subset=[target_stage2])\n",
    "\n",
    "        if df_c.empty or len(df_c) < 120:\n",
    "            print(f\"⏭️ Skipping {cid}-{tf} (not enough data)\")\n",
    "            continue\n",
    "\n",
    "        ts = df_c[target_stage2]\n",
    "\n",
    "        # -------------------------\n",
    "        # SARIMAX\n",
    "        # -------------------------\n",
    "        try:\n",
    "            sarimax_entry = {\"CompanyId\": cid, \"TimeFrame\": tf}\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"SARIMAX_{cid}_{tf}\"):\n",
    "                # Example fixed order (replace with auto_arima search results if you have them)\n",
    "                order = (1, 1, 1)\n",
    "                seasonal_order = (0, 1, 1, 7)\n",
    "\n",
    "                sarimax_model = SARIMAX(\n",
    "                    ts, order=order, seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False\n",
    "                )\n",
    "                sarimax_fit = sarimax_model.fit(disp=False)\n",
    "\n",
    "                forecast = sarimax_fit.get_forecast(steps=steps)\n",
    "                mean_pred = forecast.predicted_mean.mean()\n",
    "\n",
    "                # Store results\n",
    "                sarimax_entry.update({\n",
    "                    \"Pred_SARIMAX\": float(mean_pred),\n",
    "                    \"order\": str(order),\n",
    "                    \"seasonal_order\": str(seasonal_order)\n",
    "                })\n",
    "\n",
    "                # Log metric and params\n",
    "                mlflow.log_metric(\"MeanPred_SARIMAX\", mean_pred)\n",
    "                mlflow.log_params({\"order\": order, \"seasonal_order\": seasonal_order})\n",
    "\n",
    "                # save model to temp file\n",
    "                model_path = f\"sarimax_{cid}_{tf}.pkl\"\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    pickle.dump(sarimax_fit, f)\n",
    "                mlflow.log_artifact(model_path)\n",
    "                os.remove(model_path)\n",
    "                \n",
    "                sarimax_results.append(sarimax_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ SARIMAX failed for {cid}-{tf}: {e}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # PyCaret\n",
    "        # -------------------------\n",
    "        try:\n",
    "            stage2_features_c = [\n",
    "                c for c in df_c.select_dtypes(include=[np.number]).columns\n",
    "                if c not in [target_stage1, target_stage2]\n",
    "            ]\n",
    "\n",
    "            pycaret_entry = {\"CompanyId\": cid, \"TimeFrame\": tf}\n",
    "\n",
    "            s = setup(\n",
    "                data=df_c,\n",
    "                target=target_stage2,\n",
    "                numeric_features=stage2_features_c,\n",
    "                session_id=42,\n",
    "                log_experiment=False,   # ✅ manual MLflow control\n",
    "                html=False\n",
    "            )\n",
    "\n",
    "            for model_name in pycaret_models:\n",
    "                with mlflow.start_run(run_name=f\"PyCaret_{model_name}_{cid}_{tf}\"):\n",
    "                    '''\n",
    "                    Memory issues: \n",
    "                    Model list is above\n",
    "                    options\n",
    "                    ------------\n",
    "                    df_c_small = df_c.sample(frac=0.3, random_state=42)\n",
    "                    tuned = tune_model(model, fold=2, optimize=\"MAE\", n_iter=10)\n",
    "                    \n",
    "                    model = create_model(model_name, fold=2)  # or even 1\n",
    "                    tuned = tune_model(model, fold=2, optimize=\"MAE\")\n",
    "\n",
    "                    suggested:\n",
    "                    model = create_modelmodel_name, fold=2)\n",
    "                    final = finalize_model(model)\n",
    "                    preds = predict_model(final, data=df_c)\n",
    "                    ------------\n",
    "                    '''\n",
    "                    try:\n",
    "                        model = create_model(model_name, fold=3)\n",
    "                        tuned = tune_model(model, fold=3, optimize=\"MAE\")\n",
    "                        final = finalize_model(tuned)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Tuning failed for {model_name} {cid}-{tf}: {e}\")\n",
    "                        # fallback to untuned\n",
    "                        final = finalize_model(model)\n",
    "\n",
    "                    preds = predict_model(final, data=df_c)\n",
    "                    pred_col = next(\n",
    "                        (c for c in [\"Label\", \"prediction_label\", \"prediction\"] if c in preds.columns),\n",
    "                        None\n",
    "                    )\n",
    "                    mean_pred = preds[pred_col].mean()\n",
    "                    pycaret_entry[f\"Pred_{model_name}_PyCaret\"] = mean_pred\n",
    "\n",
    "                    mlflow.log_metric(f\"MeanPred_{model_name}\", mean_pred)\n",
    "\n",
    "                    # Log sklearn model directly\n",
    "                    mlflow.pycaret.log_model(\n",
    "                        model=final_model,\n",
    "                        name=f\"{model_name}_model\",\n",
    "                        input_example=df_c.head(1)  # just one row is enough\n",
    "                    )\n",
    "\n",
    "            pycaret_results.append(pycaret_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ PyCaret failed for {cid}-{tf}: {e}\")\n",
    "\n",
    "\n",
    "for entry in sarimax_results:\n",
    "    print(entry.keys())\n",
    "\n",
    "\n",
    "# Convert results\n",
    "sarimax_df = pd.DataFrame(sarimax_results)\n",
    "pycaret_df = pd.DataFrame(pycaret_results)\n",
    "\n",
    "# Merge SARIMAX + PyCaret outputs\n",
    "final_df = combined_top_df_clean.merge(sarimax_df, on=['CompanyId','TimeFrame'], how='left')\n",
    "final_df = final_df.merge(pycaret_df, on=['CompanyId','TimeFrame'], how='left')\n",
    "\n",
    "# -------------------------\n",
    "# Select Top N per timeframe\n",
    "# -------------------------\n",
    "top_n = 5\n",
    "\n",
    "def select_top_n(df, pred_col, n=5):\n",
    "    top_list = []\n",
    "    for tf in df['TimeFrame'].unique():\n",
    "        tf_df = df[df['TimeFrame'] == tf].copy()\n",
    "        tf_df = tf_df.sort_values(pred_col, ascending=False)\n",
    "        top_list.append(tf_df.head(n))\n",
    "    return pd.concat(top_list, ignore_index=True)\n",
    "\n",
    "# Example: select top 5 by SARIMAX\n",
    "top_sarimax_df = select_top_n(final_df, 'Pred_SARIMAX', top_n)\n",
    "\n",
    "# Example: select top 5 by Ridge\n",
    "def select_top_n(df, pred_col, n=5):\n",
    "    if pred_col not in df.columns:\n",
    "        print(f\"⚠️ Column {pred_col} not found in DataFrame\")\n",
    "        return pd.DataFrame()  # return empty\n",
    "    top_list = []\n",
    "    for tf in df['TimeFrame'].unique():\n",
    "        tf_df = df[df['TimeFrame'] == tf].copy()\n",
    "        tf_df = tf_df.sort_values(pred_col, ascending=False)\n",
    "        top_list.append(tf_df.head(n))\n",
    "    return pd.concat(top_list, ignore_index=True) if top_list else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Merge for comparison\n",
    "top_combined_df = top_sarimax_df.merge(\n",
    "    pycaret_df,\n",
    "    on=['CompanyId','TimeFrame'],\n",
    "    how='outer',\n",
    "    suffixes=('_SARIMAX','_PyCaret')\n",
    ")\n",
    "\n",
    "#print(\"Top N companies per timeframe (combined SARIMAX + PyCaret):\")\n",
    "#print(top_combined_df)\n",
    "\n",
    "# pdf_short = ranked_rows.filter(F.col(\"TimeFrame\") == \"Short\").toPandas()\n",
    "#top_combined_df.to_csv(f\"cvs/final_top_combined_df.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_top_n_final(df, pred_col=\"PredictedTomorrowClose\", n=5):\n",
    "    \"\"\"\n",
    "    Select top-N rows per TimeFrame by prediction column.\n",
    "    \"\"\"\n",
    "    top_list = []\n",
    "    for tf in df['TimeFrame'].unique():\n",
    "        tf_df = df[df['TimeFrame'] == tf].copy()\n",
    "        tf_df = tf_df.sort_values(pred_col, ascending=False)\n",
    "        top_list.append(tf_df.head(n))\n",
    "    return pd.concat(top_list, ignore_index=True)\n",
    "\n",
    "# Pick top-N by PredictedTomorrowClose\n",
    "top_candidates = select_top_n_final(final_df, pred_col=\"PredictedTomorrowClose\", n=5)\n",
    "\n",
    "# Reduce to just what you need for DB write\n",
    "top_out = top_candidates[[\n",
    "    \"CompanyId\",\n",
    "    \"TimeFrame\",\n",
    "    \"PredictedTomorrowClose\"\n",
    "]]\n",
    "\n",
    "print(top_out)\n",
    "from pyspark.sql.functions import lit, max as spark_max\n",
    "\n",
    "# --- Step 1: Reduce to needed columns ---\n",
    "top_out = top_candidates[[\"CompanyId\", \"TimeFrame\", \"PredictedTomorrowClose\"]]\n",
    "\n",
    "# --- Step 2: Create managed Delta table if not exists ---\n",
    "table_name = \"bsf.final_top_candidates\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    CompanyId STRING,\n",
    "    TimeFrame STRING,\n",
    "    PredictedTomorrowClose DOUBLE,\n",
    "    run_id INT\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# --- Step 3: Determine next run_id ---\n",
    "if spark._jsparkSession.catalog().tableExists(table_name):\n",
    "    latest_run_id = (\n",
    "        spark.read.table(table_name)\n",
    "        .agg(spark_max(\"run_id\"))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    run_id = (latest_run_id or 0) + 1\n",
    "else:\n",
    "    run_id = 1\n",
    "\n",
    "# --- Step 4: Add run_id and write ---\n",
    "top_out_df = top_out.withColumn(\"run_id\", lit(run_id))\n",
    "top_out_df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"✅ Run {run_id} written to {table_name}\")\n",
    "print(top_out_df.show())\n",
    "# pdf_short = ranked_rows.filter(F.col(\"TimeFrame\") == \"Short\").toPandas()\n",
    "#top_combined_df.to_csv(f\"cvs/final_top_combined_1_df.csv\", index=False)\n",
    "from pyspark.sql.functions import lit, col, max as spark_max\n",
    "\n",
    "table_name = \"bsf.final_top_combined\"\n",
    "\n",
    "# --- Step 1: Create managed Delta table if not exists ---\n",
    "# (I’ll include all your columns explicitly)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    CompanyId STRING,\n",
    "    StockDate DATE,\n",
    "    Open DOUBLE,\n",
    "    High DOUBLE,\n",
    "    Low DOUBLE,\n",
    "    Close DOUBLE,\n",
    "    TomorrowClose DOUBLE,\n",
    "    Return DOUBLE,\n",
    "    TomorrowReturn DOUBLE,\n",
    "    Doji BOOLEAN,\n",
    "    Hammer BOOLEAN,\n",
    "    InvertedHammer BOOLEAN,\n",
    "    ShootingStar BOOLEAN,\n",
    "    BullishEngulfing BOOLEAN,\n",
    "    BearishEngulfing BOOLEAN,\n",
    "    PiercingLine BOOLEAN,\n",
    "    DarkCloudCover BOOLEAN,\n",
    "    MorningStar BOOLEAN,\n",
    "    EveningStar BOOLEAN,\n",
    "    ThreeWhiteSoldiers BOOLEAN,\n",
    "    ThreeBlackCrows BOOLEAN,\n",
    "    TweezerTop BOOLEAN,\n",
    "    TweezerBottom BOOLEAN,\n",
    "    InsideBar BOOLEAN,\n",
    "    OutsideBar BOOLEAN,\n",
    "    MA DOUBLE,\n",
    "    MA_slope DOUBLE,\n",
    "    UpTrend_MA BOOLEAN,\n",
    "    DownTrend_MA BOOLEAN,\n",
    "    MomentumUp BOOLEAN,\n",
    "    MomentumDown BOOLEAN,\n",
    "    ConfirmedUpTrend BOOLEAN,\n",
    "    ConfirmedDownTrend BOOLEAN,\n",
    "    RecentReturn DOUBLE,\n",
    "    UpTrend_Return BOOLEAN,\n",
    "    DownTrend_Return BOOLEAN,\n",
    "    Volatility DOUBLE,\n",
    "    LowVolatility BOOLEAN,\n",
    "    HighVolatility BOOLEAN,\n",
    "    ROC DOUBLE,\n",
    "    MomentumZ DOUBLE,\n",
    "    SignalStrength INT,\n",
    "    SignalStrengthHybrid DOUBLE,\n",
    "    ActionConfidence DOUBLE,\n",
    "    BullishStrengthHybrid DOUBLE,\n",
    "    BearishStrengthHybrid DOUBLE,\n",
    "    SignalDuration DOUBLE,\n",
    "    ValidAction BOOLEAN,\n",
    "    HasValidSignal BOOLEAN,\n",
    "    MomentumAction STRING,\n",
    "    PatternAction STRING,\n",
    "    CandleAction STRING,\n",
    "    CandidateAction STRING,\n",
    "    Action STRING,\n",
    "    TomorrowAction STRING,\n",
    "    TomorrowActionSource STRING,\n",
    "    BatchId STRING,\n",
    "    IngestedAt STRING,\n",
    "    TimeFrame STRING,\n",
    "    log_Open DOUBLE,\n",
    "    log_High DOUBLE,\n",
    "    log_Low DOUBLE,\n",
    "    log_Close DOUBLE,\n",
    "    Pred_Linear DOUBLE,\n",
    "    Pred_Lasso DOUBLE,\n",
    "    Pred_Ridge DOUBLE,\n",
    "    PredictedTomorrowClose DOUBLE,\n",
    "    Pred_SARIMAX DOUBLE,\n",
    "    order STRING,\n",
    "    seasonal_order STRING,\n",
    "    Pred_lr_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_lasso_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_ridge_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_en_PyCaret_SARIMAX DOUBLE,\n",
    "    Pred_lr_PyCaret_PyCaret DOUBLE,\n",
    "    Pred_lasso_PyCaret_PyCaret DOUBLE,\n",
    "    Pred_ridge_PyCaret_PyCaret DOUBLE,\n",
    "    Pred_en_PyCaret_PyCaret DOUBLE,\n",
    "    run_id INT\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# --- Step 2: Get next run_id ---\n",
    "if spark._jsparkSession.catalog().tableExists(table_name):\n",
    "    latest_run_id = (\n",
    "        spark.read.table(table_name)\n",
    "        .agg(spark_max(\"run_id\"))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    run_id = (latest_run_id or 0) + 1\n",
    "else:\n",
    "    run_id = 1\n",
    "\n",
    "# --- Step 3: Add run_id to DataFrame and save ---\n",
    "df_with_id = top_combined_df.withColumn(\"run_id\", lit(run_id))\n",
    "\n",
    "df_with_id.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"✅ Run {run_id} written to {table_name}\")\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d443b73-e0c7-45d2-8d23-b50f716434f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSF (3.9)",
   "language": "python",
   "name": "python3.9_bsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
